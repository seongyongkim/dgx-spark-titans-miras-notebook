{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98527b27",
   "metadata": {},
   "source": [
    "# üß† Hybrid Titans Memory: Augmenting LLMs with Learning-Based Memory\n",
    "\n",
    "## Introduction\n",
    "Standard Large Language Models (LLMs) like GPT-4 or Mistral suffer from a \"Fixed Context Window.\" Once text scrolls off the top, it is gone forever. RAG (Retrieval Augmented Generation) helps, but it is static database retrieval, not true \"learning.\"\n",
    "\n",
    "**Titans**, a new architecture from Google Research, proposes a \"Neural Memory\" that *learns* context in real-time.\n",
    "\n",
    "In this notebook, we will implement a slightly simplified **Hybrid Architecture**:\n",
    "1.  **The Frozen Brain**: A standard pre-trained LLM (GPT-2 for speed, scalable to Llama-3).\n",
    "2.  **The Learning Sidecar**: A tiny Neural Network that \"watches\" the LLM's thoughts and updates its own weights using **Test-Time Training (TTT)**.\n",
    "\n",
    "### The Objective\n",
    "We will teach the model 3 random facts. Then, we will **delete** the conversation history (Input Context). Finally, we will ask the model to answer a question. If it answers correctly, it means the information didn't come from the prompt‚Äîit came from the **Neural Memory weights**.\n",
    "\n",
    "Let's build it! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3a2e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Running on device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0xef70daf7fa90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Environment Setup\n",
    "# We need PyTorch for the memory module and Transformers for the LLM.\n",
    "# 'accelerate' and 'bitsandbytes' are highly recommended for loading larger models efficiently.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import copy\n",
    "\n",
    "# Determine if we have a GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Running on device: {device}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If you need to install packages, uncomment below:\n",
    "# !pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db3568d",
   "metadata": {},
   "source": [
    "## üß† 2. The Neural Memory Module (Sidecar)\n",
    "\n",
    "This is the core innovation. Instead of just \"storing\" vectors (like a vector DB), we train a neural network to memorize them.\n",
    "\n",
    "### \"Surprise\" Metric\n",
    "We define learning as minimizing **Surprise**.\n",
    "If the LLM sees a new concept (like \"The sky is green\"), its internal state will be \"surprised\" (Novelty).\n",
    "Our Memory Module attempts to **predict** or **reconstruct** this hidden state.\n",
    "-   If reconstruction is good -> Low Surprise (No learning needed).\n",
    "-   If reconstruction is bad -> High Surprise (Update weights!).\n",
    "\n",
    "We will implement **Test-Time Training (TTT)**. The `memorize()` function runs a backward pass *during inference*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7991775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    A Neural Memory Module (The \"Sidecar\").\n",
    "    \n",
    "    This acts as a dynamic, learnable memory that runs alongside the frozen LLM.\n",
    "    Functionally, it is a simple Autoencoder or MLP that maps an input states (query)\n",
    "    to a memory context.\n",
    "    \n",
    "    Key Feature:\n",
    "    It contains its own optimizer. This allows it to update its weights \n",
    "    on-the-fly (Test-Time Training) based on the context of the current conversation,\n",
    "    effectively \"memorizing\" new information in its weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, memory_dim=None, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        \n",
    "        # If no specific memory dimension is given, we keep it same as input\n",
    "        # flexible for compression or expansion\n",
    "        if memory_dim is None:\n",
    "            memory_dim = input_dim \n",
    "            \n",
    "        # The Architecture: A simple Encoder-Decoder style network\n",
    "        # 1. Compress/Transform input to memory space\n",
    "        self.encoder = nn.Linear(input_dim, memory_dim)\n",
    "        # 2. Non-linearity to capture complex relationships\n",
    "        self.activation = nn.GELU() \n",
    "        # 3. Project back to input space (or context space)\n",
    "        self.decoder = nn.Linear(memory_dim, input_dim)\n",
    "        \n",
    "        # Internal Optimizer:\n",
    "        # Standard PyTorch models don't usually hold their own optimizer.\n",
    "        # We do this here to encapsulate the \"Learning\" capability within the module itself.\n",
    "        # SGD is used here for simplicity and stability in small batch updates.\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: Generates a 'Memory Context' from the input query.\n",
    "        \"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        activated = self.activation(encoded)\n",
    "        reconstruction = self.decoder(activated)\n",
    "        return reconstruction\n",
    "\n",
    "    def memorize(self, target_state):\n",
    "        \"\"\"\n",
    "        The core learning mechanism: Test-Time Training (TTT).\n",
    "        \n",
    "        This function performs a single gradient descent step to minimize \n",
    "        reconstruction error (Surprise) on a given target state.\n",
    "        \n",
    "        Args:\n",
    "            target_state: The vector we want the memory to 'remember'.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Detach target to ensure we don't backpropagate into the entity generating the target\n",
    "        target = target_state.detach()\n",
    "        \n",
    "        # Try to predict/reconstruct the target\n",
    "        reconstruction = self.forward(target)\n",
    "        \n",
    "        # Calculate 'Surprise' (Loss): How different is our memory's prediction from reality?\n",
    "        loss = F.mse_loss(reconstruction, target)\n",
    "        \n",
    "        # Update weights to reduce surprise next time\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674bd4e",
   "metadata": {},
   "source": [
    "## ü§ñ 3. Initialize the Frozen LLM\n",
    "\n",
    "We will use **GPT-2** (Small) for this demonstration. It is fast, lightweight, and perfect for testing concepts.\n",
    "*   **Frozen**: We will set `requires_grad = False` for the LLM. It will *not* change.\n",
    "*   **Tokenizer**: Standard GPT-2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665f0b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è Loading TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syk/miniconda3/envs/rapids-25.12/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùÑÔ∏è LLM parameters frozen.\n",
      "üìè Hidden Dimension: 2048\n",
      "üß† Neural Memory initialized.\n"
     ]
    }
   ],
   "source": [
    "# Load Model & Tokenizer\n",
    "#model_name = \"gpt2\" # Can swap with \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" if GPU permits\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "print(f\"‚¨áÔ∏è Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Freeze the LLM\n",
    "for param in llm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"‚ùÑÔ∏è LLM parameters frozen.\")\n",
    "    \n",
    "# Get Hidden Dimension Size (e.g., 768 for GPT-2, 2048 for TinyLlama)\n",
    "# We check for 'hidden_size' (Llama/Mistral) and fall back to 'n_embd' (GPT-2)\n",
    "hidden_dim = getattr(llm.config, \"hidden_size\", getattr(llm.config, \"n_embd\", None))\n",
    "if hidden_dim is None:\n",
    "    raise ValueError(\"Model config must have 'hidden_size' or 'n_embd'\")\n",
    "\n",
    "print(f\"üìè Hidden Dimension: {hidden_dim}\")\n",
    "\n",
    "# Initialize our Trainable Memory Sidecar\n",
    "memory_module = NeuralMemory(input_dim=hidden_dim).to(device)\n",
    "print(\"üß† Neural Memory initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b032dc71",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 4. The Hybrid Inference Engine\n",
    "\n",
    "This is the glue that binds them. We need a custom generation loop.\n",
    "Unlike `model.generate()`, we need to inspect the internals step-by-step.\n",
    "\n",
    "**The Loop:**\n",
    "1.  **Embed**: Term text -> Vectors.\n",
    "2.  **Recall**: Pass Vectors -> Memory -> `memory_context`.\n",
    "3.  **Mix**: Combine `[Embeddings + memory_context]` (Soft Prompting).\n",
    "4.  **Forward**: Run LLM.\n",
    "5.  **Learn**: Take the output hidden state, calculate surprise, and call `memory.memorize()`.\n",
    "\n",
    "*Note: For simplicity in this demo, we will perform the memory update on the INPUT embeddings to \"remember input facts\", a common simplification for these demos.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f210a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Optimized Engine Ready.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class HybridTitansEngine:\n",
    "    def __init__(self, llm, memory, tokenizer):\n",
    "        self.llm = llm\n",
    "        self.memory = memory\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # We retrieve the embedding layer generically to support both GPT-2 and Llama\n",
    "        self.embedding_layer = self.llm.get_input_embeddings()\n",
    "        \n",
    "    def process_and_learn(self, text, steps=20):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        input_ids = inputs.input_ids\n",
    "        seq_len = input_ids.shape[1]\n",
    "        \n",
    "        self.memory.train()\n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(steps):\n",
    "             self.memory.optimizer.zero_grad()\n",
    "             \n",
    "             with torch.no_grad():\n",
    "                 # Generic embedding lookup\n",
    "                 embeds = self.embedding_layer(input_ids)\n",
    "            \n",
    "             min_len = 3 \n",
    "             if seq_len > min_len:\n",
    "                 cut_point = random.randint(min_len, seq_len)\n",
    "                 query_embeds = embeds[:, :cut_point, :]\n",
    "             else:\n",
    "                 query_embeds = embeds\n",
    "                 \n",
    "             query = query_embeds.mean(dim=1).detach()\n",
    "             \n",
    "             soft_memory = self.memory(query)\n",
    "             \n",
    "             # Inject\n",
    "             soft_memory = soft_memory.unsqueeze(1) \n",
    "             combined_embeds = torch.cat([soft_memory, embeds], dim=1)\n",
    "             \n",
    "             # Labels\n",
    "             ignore_token = torch.full((1, 1), -100, dtype=torch.long, device=device)\n",
    "             combined_labels = torch.cat([ignore_token, input_ids], dim=1)\n",
    "             \n",
    "             outputs = self.llm(inputs_embeds=combined_embeds, labels=combined_labels)\n",
    "             \n",
    "             loss = outputs.loss\n",
    "             loss.backward()\n",
    "             self.memory.optimizer.step()\n",
    "             losses.append(loss.item())\n",
    "             \n",
    "        return sum(losses) / len(losses)\n",
    "\n",
    "    def generate(self, prompt, max_new_tokens=20):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        input_ids = inputs.input_ids \n",
    "        \n",
    "        # Ensure we don't exceed model context\n",
    "        input_ids = input_ids[:, -1020:] \n",
    "        \n",
    "        # Generic embedding lookup\n",
    "        embeds = self.embedding_layer(input_ids)\n",
    "        query = embeds.mean(dim=1)\n",
    "        \n",
    "        self.memory.eval()\n",
    "        with torch.no_grad():\n",
    "            memory_context = self.memory(query)\n",
    "        \n",
    "        memory_context = memory_context.unsqueeze(1)\n",
    "        current_embeds = torch.cat([memory_context, embeds], dim=1)\n",
    "        \n",
    "        generated_ids = []\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = self.llm(inputs_embeds=current_embeds)\n",
    "            logits = outputs.logits[:, -1, :] \n",
    "            \n",
    "            # Greedy search\n",
    "            next_token_id = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "            \n",
    "            generated_ids.append(next_token_id.item())\n",
    "            if next_token_id.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "            # Generic embedding lookup for single token\n",
    "            next_embed = self.embedding_layer(next_token_id)\n",
    "            current_embeds = torch.cat([current_embeds, next_embed], dim=1)\n",
    "            \n",
    "        return self.tokenizer.decode(generated_ids)\n",
    "\n",
    "# Robust parameters: High LR for one-shot/few-shot learning\n",
    "memory_module = NeuralMemory(input_dim=hidden_dim, learning_rate=0.04).to(device)\n",
    "engine = HybridTitansEngine(llm, memory_module, tokenizer)\n",
    "print(\"‚öôÔ∏è Optimized Engine Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b7e0d4",
   "metadata": {},
   "source": [
    "## üß™ 5. Demo: The \"Total Recall\" Experiment\n",
    "\n",
    "We will now perform the experiment.\n",
    "\n",
    "**Phase 1: Learning**\n",
    "We will feed the engine 3 random facts.\n",
    "It will use `process_and_learn` to update `NeuralMemory` weights.\n",
    "\n",
    "**Phase 2: Context Clearance**\n",
    "We will NOT pass these facts into the generation prompt. The context window is effectively empty.\n",
    "\n",
    "**Phase 3: Testing**\n",
    "We ask the model questions. If it answers, it retrieved the concept from the Neural Network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb656e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¶ Phase 1: Learning...\n",
      "   Epoch 0: Avg Surprise 4.9707\n",
      "   Epoch 10: Avg Surprise 0.3891\n",
      "   Epoch 20: Avg Surprise 0.2403\n",
      "   Epoch 30: Avg Surprise 0.2420\n",
      "   Epoch 40: Avg Surprise 0.2470\n",
      "   Epoch 50: Avg Surprise 0.2325\n",
      "   (Learning Complete)\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "üü• Phase 2: Clearing Context...\n",
      "   (Brain Wiped. Only Neural Sidecar retains weights.)\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "üü© Phase 3: Testing Recall...\n",
      "\n",
      "Question: The secret project code is\n",
      "Titans Answer: Omega-99. The meeting is at 4:32\n",
      "\n",
      "Question: The CEO's favorite fruit is\n",
      "Titans Answer: a apple. The CEO's favorite fruit is a apple. The\n",
      "\n",
      "Question: My shopping list is\n",
      "Titans Answer: eggs, milk, bread, milk, bread, fruit CEO's\n",
      "\n",
      "Question: The meeting is at\n",
      "Titans Answer: 4:32 PM exactly. The CEO's favorite fruit\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "üéâ Experiment Complete. Check if the answers match the facts!\n"
     ]
    }
   ],
   "source": [
    "# The Facts\n",
    "facts = [\n",
    "    \"The secret project code is Omega-99.\",\n",
    "    \"The CEO's favorite fruit is a apple.\", \n",
    "    \"My shopping list is eggs, milk, bread\", \n",
    "    \"The meeting is at 4:32 PM exactly.\"\n",
    "]\n",
    "\n",
    "print(\"üü¶ Phase 1: Learning...\")\n",
    "# We use Interleaved Training to prevent Catastrophic Forgetting.\n",
    "# We increase epochs significantly to force 'Overfitting' on these specific facts.\n",
    "\n",
    "n_epochs = 60 \n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    random.shuffle(facts) # Semantic Mixing\n",
    "    for fact in facts:\n",
    "        # We train with fewer steps per 'visit' but more visits overall\n",
    "        loss = engine.process_and_learn(fact, steps=10) \n",
    "        epoch_loss += loss\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"   Epoch {epoch}: Avg Surprise {epoch_loss/3:.4f}\")\n",
    "\n",
    "print(\"   (Learning Complete)\")\n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "\n",
    "print(\"üü• Phase 2: Clearing Context...\")\n",
    "# We do nothing here. The 'facts' variable is just a python list. \n",
    "print(\"   (Brain Wiped. Only Neural Sidecar retains weights.)\")\n",
    "\n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "\n",
    "print(\"üü© Phase 3: Testing Recall...\")\n",
    "\n",
    "questions = [\n",
    "    \"The secret project code is\",\n",
    "    \"The CEO's favorite fruit is\",\n",
    "    \"My shopping list is\",\n",
    "    \"The meeting is at\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQuestion: {q}\")\n",
    "    answer = engine.generate(q, max_new_tokens=15)\n",
    "    print(f\"Titans Answer: {answer}\")\n",
    "    \n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "print(\"üéâ Experiment Complete. Check if the answers match the facts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea81841",
   "metadata": {},
   "source": [
    "## üë• 6. Advanced: Multi-User Sessions\n",
    "\n",
    "One of the most powerful features of this architecture is **Efficiency**.\n",
    "\n",
    "*   **The LLM (Weights)**: Very large (GBs of VRAM). Shared across all users.\n",
    "*   **The Memory (Sidecar)**: Very small (KBs). Unique to each user.\n",
    "\n",
    "This means we can host **one** frozen GPT-2/Llama-3 model and serve **thousands** of users, each having their own private, persistent learning memory, without re-training the base model or managing massive KV caches.\n",
    "\n",
    "Let's simulate two users, **Alice** and **Bob**, sharing the same brain but keeping their secrets private."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522259f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë• Multi-User Sessions Initialized (Shared LLM, Private Memory).\n",
      "\n",
      "üîπ Alice is learning: 'The secret code is Blue-Sky.'\n",
      "   Alice Loss: 5.0707\n",
      "   Alice Loss: 0.2378\n",
      "   Alice Loss: 0.0598\n",
      "   Alice Loss: 0.0315\n",
      "   Alice Loss: 0.0216\n",
      "üî∏ Bob is learning:   'The secret code is Red-Storm.'\n",
      "   Bob Loss:   7.3027\n",
      "   Bob Loss:   0.3043\n",
      "   Bob Loss:   0.0941\n",
      "   Bob Loss:   0.0477\n",
      "   Bob Loss:   0.0316\n",
      "(Learning Complete)\n",
      "\n",
      "--- Testing Alice's Session ---\n",
      "Alice's View: Blue-Sky.</s>\n",
      "\n",
      "--- Testing Bob's Session ---\n",
      "Bob's View:   Red-Storm. The\n",
      "\n",
      "‚úÖ Verification: The Shared LLM produced different outputs based on Private Memory.\n"
     ]
    }
   ],
   "source": [
    "# 1. Instantiate separate memories for Alice and Bob\n",
    "# They are independent instances of the NeuralMemory class.\n",
    "memory_alice = NeuralMemory(input_dim=hidden_dim, learning_rate=0.04).to(device)\n",
    "memory_bob   = NeuralMemory(input_dim=hidden_dim, learning_rate=0.04).to(device)\n",
    "\n",
    "# 2. Create separate engines (Sessions) \n",
    "# Crucially, they share the SAME frozen 'llm' and 'tokenizer' objects.\n",
    "session_alice = HybridTitansEngine(llm, memory_alice, tokenizer)\n",
    "session_bob   = HybridTitansEngine(llm, memory_bob, tokenizer)\n",
    "\n",
    "print(\"üë• Multi-User Sessions Initialized (Shared LLM, Private Memory).\\n\")\n",
    "\n",
    "# 3. Learn conflicting private facts\n",
    "fact_alice = \"The secret code is Blue-Sky.\"\n",
    "fact_bob   = \"The secret code is Red-Storm.\"\n",
    "\n",
    "print(f\"üîπ Alice is learning: '{fact_alice}'\")\n",
    "# We increase training iterations to ensure the memory 'locks in' the specific fact\n",
    "# Since we are overwriting the generic 'secret code' concept, we need strong gradients.\n",
    "for i in range(50): \n",
    "    loss_a = session_alice.process_and_learn(fact_alice, steps=5)\n",
    "    if i % 10 == 0: print(f\"   Alice Loss: {loss_a:.4f}\")\n",
    "\n",
    "print(f\"üî∏ Bob is learning:   '{fact_bob}'\")\n",
    "for i in range(50):\n",
    "    loss_b = session_bob.process_and_learn(fact_bob, steps=5)\n",
    "    if i % 10 == 0: print(f\"   Bob Loss:   {loss_b:.4f}\")\n",
    "\n",
    "print(\"(Learning Complete)\\n\")\n",
    "\n",
    "# 4. Test Recall\n",
    "# The prompt is identical for both users.\n",
    "question = \"The secret code is\"\n",
    "\n",
    "print(\"--- Testing Alice's Session ---\")\n",
    "# Alice's engine uses Alice's memory weights\n",
    "ans_a = session_alice.generate(question, max_new_tokens=6)\n",
    "print(f\"Alice's View: {ans_a}\")\n",
    "\n",
    "print(\"\\n--- Testing Bob's Session ---\")\n",
    "# Bob's engine uses Bob's memory weights\n",
    "ans_b = session_bob.generate(question, max_new_tokens=6)\n",
    "print(f\"Bob's View:   {ans_b}\")\n",
    "\n",
    "print(\"\\n‚úÖ Verification: The Shared LLM produced different outputs based on Private Memory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
