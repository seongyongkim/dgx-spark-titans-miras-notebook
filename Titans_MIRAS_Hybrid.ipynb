{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faad5306",
   "metadata": {},
   "source": [
    "# üß† Titans-MIRAS: Teaching AI to Remember\n",
    "\n",
    "## A Beginner's Guide to Neural Memory Systems\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this tutorial, you'll discover how to give an AI \"long-term memory\" ‚Äî the ability to learn and remember facts **during a conversation**, not just during training.\n",
    "\n",
    "By the end, you'll understand:\n",
    "- ü§î **The Problem**: Why ChatGPT \"forgets\" things\n",
    "- üí° **The Solution**: Neural Memory modules that learn in real-time\n",
    "- üîß **The Code**: How to build your own memory-augmented AI\n",
    "- üß™ **The Proof**: A demo where AI remembers facts without being told twice\n",
    "\n",
    "---\n",
    "\n",
    "### The Problem with Today's AI\n",
    "\n",
    "Imagine talking to ChatGPT:\n",
    "\n",
    "```\n",
    "You: My name is Alice and I live in Tokyo.\n",
    "AI:  Nice to meet you, Alice!\n",
    "\n",
    "... (1000 messages later) ...\n",
    "\n",
    "You: Where do I live?\n",
    "AI:  I don't have access to that information.\n",
    "```\n",
    "\n",
    "**Why does this happen?**\n",
    "\n",
    "LLMs (Large Language Models) like GPT-4 have a **fixed context window** ‚Äî a limited \"short-term memory\" of recent messages. Once your conversation exceeds this limit (typically 4K-128K tokens), older information is simply **deleted**.\n",
    "\n",
    "The AI's **weights** (the billions of numbers that define its knowledge) were set during training and **never change** during conversation. It can't truly \"learn\" new facts about you.\n",
    "\n",
    "---\n",
    "\n",
    "### The Titans Solution\n",
    "\n",
    "In 2024, Google Research introduced **Titans** ‚Äî an architecture where a small neural network called **Neural Memory** runs alongside the LLM and **updates its weights in real-time**.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "| Component | Analogy | What it does |\n",
    "|-----------|---------|-------------|\n",
    "| **LLM (Frozen)** | üìö Library | Has vast knowledge but can't add new books |\n",
    "| **Neural Memory** | üìù Notebook | Small, personal, constantly updated |\n",
    "\n",
    "The Neural Memory learns by measuring **\"Surprise\"** ‚Äî if something is unexpected (hard to predict), it's worth remembering!\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This tutorial assumes you know:\n",
    "- Basic Python (variables, functions, loops)\n",
    "- What a neural network is (layers, weights, forward pass)\n",
    "- Basic PyTorch syntax (tensors, `torch.nn`)\n",
    "\n",
    "Don't worry if you're rusty ‚Äî we'll explain everything step by step! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6ce72",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Environment Setup\n",
    "\n",
    "Before we start coding, let's make sure your computer is ready.\n",
    "\n",
    "### What We Need\n",
    "- **Python 3.10+** ‚Äî The programming language\n",
    "- **PyTorch** ‚Äî The deep learning framework\n",
    "- **Transformers** ‚Äî Hugging Face's library for pre-trained models\n",
    "- **A GPU** (optional but recommended) ‚Äî Makes everything 10-100x faster\n",
    "\n",
    "Let's check what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc3d0126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SYSTEM INFORMATION\n",
      "==================================================\n",
      "Python Version: 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:16:53) [GCC 14.3.0]\n",
      "Operating System: Linux-6.14.0-1013-nvidia-aarch64-with-glibc2.39\n",
      "\n",
      "‚úÖ PyTorch installed: version 2.10.0+cu130\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 1: Check Python and System Information\n",
    "# ============================================\n",
    "# This cell verifies that Python is installed correctly\n",
    "# and shows information about your operating system.\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Operating System: {platform.platform()}\")\n",
    "print()\n",
    "\n",
    "# Check if PyTorch is installed\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch installed: version {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not found. Install with: pip install torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aae02c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "GPU INFORMATION\n",
      "==================================================\n",
      "‚úÖ GPU Available: NVIDIA GB10\n",
      "   Memory: 119.7 GB\n",
      "   GPU computation test: ‚úÖ Passed\n",
      "\n",
      "üéØ We'll use: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 2: Check GPU Availability\n",
    "# ============================================\n",
    "# GPUs (Graphics Processing Units) can run neural networks\n",
    "# much faster than CPUs. This cell checks if you have one.\n",
    "#\n",
    "# Don't worry if you don't have a GPU ‚Äî everything will\n",
    "# still work, just a bit slower!\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress CUDA capability warnings (e.g., for newer GPUs)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*cuda capability.*\", category=UserWarning)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # CUDA is NVIDIA's GPU computing platform\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu_index = torch.cuda.current_device()\n",
    "    gpu_name = torch.cuda.get_device_name(gpu_index)\n",
    "    gpu_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "    gpu_memory_gb = round(gpu_memory / (1024**3), 2)\n",
    "    \n",
    "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory_gb} GB\")\n",
    "    \n",
    "    # Quick test: can we do math on the GPU?\n",
    "    x = torch.randn(1000, 1000, device=device)\n",
    "    y = torch.matmul(x, x)  # Matrix multiplication\n",
    "    print(f\"   GPU computation test: ‚úÖ Passed\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è  No GPU found. Using CPU (slower but works fine!)\")\n",
    "\n",
    "print(f\"\\nüéØ We'll use: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb19729",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "- **GPU Memory**: More is better. 8GB+ is great for this tutorial.\n",
    "- **CUDA**: This is NVIDIA's framework for GPU computing. If you see it, you're good!\n",
    "- **CPU fallback**: Everything works on CPU too, just 5-10x slower.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd4a65",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Understanding Neural Memory\n",
    "\n",
    "Before we write code, let's understand the **theory** behind Neural Memory.\n",
    "\n",
    "## The Core Idea: Learning Through Surprise\n",
    "\n",
    "Imagine you're reading a book:\n",
    "- \"The sky is blue\" ‚Üí üò¥ Boring, you already knew that\n",
    "- \"The password is X-8-DELTA-9\" ‚Üí üò≤ Surprising! Worth remembering!\n",
    "\n",
    "Neural Memory works the same way. It measures **Surprise** (also called \"prediction error\"):\n",
    "\n",
    "```\n",
    "Surprise = How wrong was my prediction?\n",
    "         = ||What I expected - What actually happened||¬≤\n",
    "```\n",
    "\n",
    "### The Mathematical Formula\n",
    "\n",
    "$$\\text{Surprise}(x, y) = ||f(x) - y||^2$$\n",
    "\n",
    "Where:\n",
    "- $x$ = Input (what the AI is currently \"thinking about\")\n",
    "- $y$ = Target (what we want it to remember)\n",
    "- $f(x)$ = Memory's prediction\n",
    "- $|| \\cdot ||^2$ = Mean Squared Error (MSE)\n",
    "\n",
    "### The Learning Loop\n",
    "\n",
    "```\n",
    "1. üìñ READ:     Get the current context from the LLM\n",
    "2. üò≤ SURPRISE: Calculate how unexpected this is\n",
    "3. üìù LEARN:    If surprising, update memory weights\n",
    "4. üîÆ RECALL:   Use memory to help with the next step\n",
    "```\n",
    "\n",
    "This is called **Test-Time Training (TTT)** ‚Äî the memory learns *during* inference, not just during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fff2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NeuralMemory class defined\n",
      "   Will use device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 3: Build the Neural Memory Module\n",
    "# ============================================\n",
    "# This is the heart of our system: a small neural network\n",
    "# that can update its own weights during inference.\n",
    "#\n",
    "# Architecture:\n",
    "#   Input ‚Üí Linear ‚Üí GELU ‚Üí Linear ‚Üí Output\n",
    "#\n",
    "# Key Features:\n",
    "#   - Uses float32 for numerical stability (avoids NaN errors)\n",
    "#   - Has its own optimizer for real-time learning\n",
    "#   - memorize() updates weights, recall() just reads\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    A Neural Memory module that learns in real-time.\n",
    "    \n",
    "    Think of it as a tiny brain that watches the LLM and takes notes.\n",
    "    When something surprising happens, it updates its \"notes\" (weights)\n",
    "    so it can remember better next time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Size of input vectors (must match LLM hidden dimension)\n",
    "    hidden_dim : int\n",
    "        Size of the internal \"compression\" layer\n",
    "    output_dim : int\n",
    "        Size of output vectors (usually same as input_dim)\n",
    "    lr : float\n",
    "        Learning rate - how fast to update weights\n",
    "        Higher = faster learning but less stable\n",
    "        Lower = slower learning but more stable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, \n",
    "                 lr: float = 1e-3, device_str: str = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store dimensions for later reference\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Determine device (GPU or CPU)\n",
    "        if device_str:\n",
    "            self.device = torch.device(device_str)\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Build the neural network\n",
    "        # This is a simple 2-layer network:\n",
    "        #   1. Linear: Compress input to hidden dimension\n",
    "        #   2. GELU: Non-linear activation (lets us learn complex patterns)\n",
    "        #   3. Linear: Expand back to output dimension\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),   # Encoder\n",
    "            nn.GELU(),                          # Activation\n",
    "            nn.Linear(hidden_dim, output_dim),  # Decoder\n",
    "        )\n",
    "        \n",
    "        # Move to device and use float32 for stability\n",
    "        # (float16 can cause NaN errors during training)\n",
    "        self.to(self.device, torch.float32)\n",
    "        \n",
    "        # Create optimizer for real-time learning\n",
    "        # AdamW is a modern optimizer that works well for most cases\n",
    "        self.optim = torch.optim.AdamW(self.parameters(), lr=lr)\n",
    "        \n",
    "        # Loss function measures \"surprise\"\n",
    "        # MSE = Mean Squared Error = average of (prediction - target)¬≤\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    @torch.no_grad()  # This decorator means \"don't track gradients\"\n",
    "    def recall(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Retrieve information from memory WITHOUT learning.\n",
    "        \n",
    "        This is like reading your notes without adding new ones.\n",
    "        Fast and doesn't change the memory.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Query vector (what are we trying to remember?)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Memory's response to the query\n",
    "        \"\"\"\n",
    "        # Convert input to correct device and dtype\n",
    "        x = x.to(self.device, torch.float32)\n",
    "        \n",
    "        # Forward pass through the network\n",
    "        output = self.net(x)\n",
    "        \n",
    "        # .detach() disconnects from computation graph\n",
    "        # (we don't need gradients for recall)\n",
    "        return output.detach()\n",
    "    \n",
    "    def memorize(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Learn something new by updating memory weights.\n",
    "        \n",
    "        This is like studying: we look at what we should have known (y),\n",
    "        compare it to what we predicted (f(x)), and adjust our\n",
    "        understanding (weights) to do better next time.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input/query vector\n",
    "        y : torch.Tensor  \n",
    "            Target - what we want to remember\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            The \"surprise\" (loss) value - lower is better\n",
    "        \"\"\"\n",
    "        # Convert inputs to correct device and dtype\n",
    "        x = x.to(self.device, torch.float32)\n",
    "        y = y.to(self.device, torch.float32)\n",
    "        \n",
    "        # Forward pass: what does memory predict?\n",
    "        prediction = self.net(x)\n",
    "        \n",
    "        # Calculate surprise (how wrong were we?)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "        \n",
    "        # Backward pass: figure out how to improve\n",
    "        self.optim.zero_grad()  # Clear old gradients\n",
    "        loss.backward()          # Calculate new gradients\n",
    "        self.optim.step()        # Update weights\n",
    "        \n",
    "        # Return the surprise value as a regular Python number\n",
    "        return float(loss.item())\n",
    "\n",
    "# Set the device globally\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ NeuralMemory class defined\")\n",
    "print(f\"   Will use device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11652fc7",
   "metadata": {},
   "source": [
    "### üéì Understanding the Code\n",
    "\n",
    "Let's break down the key parts:\n",
    "\n",
    "**1. The Network Architecture**\n",
    "```python\n",
    "self.net = nn.Sequential(\n",
    "    nn.Linear(input_dim, hidden_dim),  # Compress\n",
    "    nn.GELU(),                         # Non-linearity\n",
    "    nn.Linear(hidden_dim, output_dim), # Expand\n",
    ")\n",
    "```\n",
    "This is called an **autoencoder** ‚Äî it compresses input, then reconstructs it. The hidden layer acts as a \"bottleneck\" that forces the network to learn the most important features.\n",
    "\n",
    "**2. Why GELU?**\n",
    "GELU (Gaussian Error Linear Unit) is a smooth activation function used in modern transformers like GPT. It's like a \"smart ReLU\" that works better for language tasks.\n",
    "\n",
    "**3. The Learning Process**\n",
    "```python\n",
    "loss.backward()   # Calculate gradients\n",
    "self.optim.step() # Update weights\n",
    "```\n",
    "This is **backpropagation** ‚Äî the same algorithm used to train neural networks, but we're doing it *during inference*, not just during training!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5533f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TESTING NEURAL MEMORY LEARNING\n",
      "==================================================\n",
      "Training the memory to learn a secret transformation...\n",
      "\n",
      "   Step  40: Surprise = 18.093212\n",
      "   Step  80: Surprise = 11.844204\n",
      "   Step 120: Surprise = 9.229830\n",
      "   Step 160: Surprise = 7.861908\n",
      "   Step 200: Surprise = 6.429991\n",
      "\n",
      "==================================================\n",
      "RESULTS:\n",
      "   Starting surprise: 32.839195\n",
      "   Final surprise:    6.429991\n",
      "   Improvement:       80.4%\n",
      "   Test error:        5.756186\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è Learning is slow. Try increasing the learning rate.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 4: Test the Neural Memory (Synthetic)\n",
    "# ============================================\n",
    "# Before connecting to an LLM, let's make sure our memory\n",
    "# module can actually learn! We'll give it a simple task:\n",
    "# learn a linear mapping y = x @ W (matrix multiplication).\n",
    "#\n",
    "# If the loss decreases, learning is working! üìâ\n",
    "\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility (same results every time)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Create a test memory module\n",
    "# Dimensions: 128 input ‚Üí 64 hidden ‚Üí 128 output\n",
    "test_memory = NeuralMemory(\n",
    "    input_dim=128,\n",
    "    hidden_dim=64,\n",
    "    output_dim=128,\n",
    "    lr=0.01,  # Learning rate (higher = faster learning)\n",
    "    device_str=device\n",
    ")\n",
    "\n",
    "# Create a \"secret\" transformation that memory must learn\n",
    "# This simulates what the LLM's hidden states might look like\n",
    "W_secret = torch.randn(128, 128, device=device) * 0.5\n",
    "\n",
    "def generate_sample(batch_size=32):\n",
    "    \"\"\"Generate random (input, target) pairs.\"\"\"\n",
    "    x = torch.randn(batch_size, 128, device=device)\n",
    "    y = x @ W_secret  # The \"correct\" answer\n",
    "    return x, y\n",
    "\n",
    "# Training loop\n",
    "print(\"=\" * 50)\n",
    "print(\"TESTING NEURAL MEMORY LEARNING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Training the memory to learn a secret transformation...\\n\")\n",
    "\n",
    "losses = []\n",
    "for step in range(1, 201):\n",
    "    x, y = generate_sample(batch_size=64)\n",
    "    loss = test_memory.memorize(x, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if step % 40 == 0:\n",
    "        print(f\"   Step {step:3d}: Surprise = {loss:.6f}\")\n",
    "\n",
    "# Final evaluation\n",
    "x_test, y_test = generate_sample(batch_size=16)\n",
    "prediction = test_memory.recall(x_test)\n",
    "test_error = F.mse_loss(prediction, y_test).item()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"RESULTS:\")\n",
    "print(f\"   Starting surprise: {losses[0]:.6f}\")\n",
    "print(f\"   Final surprise:    {losses[-1]:.6f}\")\n",
    "print(f\"   Improvement:       {(1 - losses[-1]/losses[0])*100:.1f}%\")\n",
    "print(f\"   Test error:        {test_error:.6f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if losses[-1] < losses[0] * 0.1:\n",
    "    print(\"\\n‚úÖ SUCCESS! Memory is learning effectively.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Learning is slow. Try increasing the learning rate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dc932",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "1. We created a random \"secret\" transformation (matrix W)\n",
    "2. Memory tried to predict `y = x @ W` without knowing W\n",
    "3. Each step, it measured surprise and updated its weights\n",
    "4. Over 200 steps, surprise dropped significantly!\n",
    "\n",
    "This proves the memory can learn patterns in real-time. Now let's connect it to a real LLM!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e8a37",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: The Hybrid Engine\n",
    "\n",
    "Now we'll combine:\n",
    "- üßä **Frozen LLM**: A pre-trained language model (GPT-2) that never changes\n",
    "- üß† **Neural Memory**: Our trainable sidecar that learns in real-time\n",
    "\n",
    "### Why \"Hybrid\"?\n",
    "\n",
    "| Component | Size | Changes? | Purpose |\n",
    "|-----------|------|----------|----------|\n",
    "| LLM | ~500MB | ‚ùå Frozen | General language understanding |\n",
    "| Memory | ~1MB | ‚úÖ Trainable | Personalized, session-specific learning |\n",
    "\n",
    "This is efficient! We share one big LLM across all users, but each user gets their own tiny memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c1cd873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING LANGUAGE MODEL\n",
      "==================================================\n",
      "\n",
      "üì• Downloading gpt2...\n",
      "\n",
      "‚úÖ Model loaded: gpt2\n",
      "   Hidden dimension: 768\n",
      "   Parameters: 124.4M\n",
      "   Device: cuda\n",
      "   Status: ‚ùÑÔ∏è FROZEN (weights will not change)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 5: Load the Frozen LLM\n",
    "# ============================================\n",
    "# We'll use GPT-2, a classic language model from OpenAI.\n",
    "# It's small enough to run on most computers but powerful\n",
    "# enough to demonstrate the concepts.\n",
    "#\n",
    "# Key point: We FREEZE the LLM (no gradient updates).\n",
    "# Only the Neural Memory will learn!\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LOADING LANGUAGE MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Choose the model\n",
    "# Options: \"gpt2\" (small), \"gpt2-medium\", \"gpt2-large\"\n",
    "model_name = \"gpt2\"\n",
    "print(f\"\\nüì• Downloading {model_name}...\")\n",
    "\n",
    "# Load tokenizer (converts text ‚Üî numbers)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Required for batching\n",
    "\n",
    "# Load model\n",
    "# dtype: Use float16 on GPU for speed, float32 on CPU for compatibility\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "llm.to(device)\n",
    "llm.eval()  # Set to evaluation mode (disables dropout)\n",
    "\n",
    "# FREEZE all parameters\n",
    "for param in llm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the hidden dimension (size of internal representations)\n",
    "hidden_dim = llm.config.n_embd  # 768 for GPT-2\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"   Hidden dimension: {hidden_dim}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in llm.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Status: ‚ùÑÔ∏è FROZEN (weights will not change)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f42b7ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "HYBRID SYSTEM CREATED\n",
      "==================================================\n",
      "\n",
      "üßä LLM: gpt2 (124.4M params, FROZEN)\n",
      "üß† Memory: 394.2K params (TRAINABLE)\n",
      "\n",
      "üìä Size ratio: Memory is 0.317% the size of LLM\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 6: Create the Hybrid Memory System\n",
    "# ============================================\n",
    "# Now we connect the Neural Memory to the LLM.\n",
    "# The memory will \"watch\" the LLM's hidden states\n",
    "# and learn patterns from them.\n",
    "\n",
    "# Create a memory that matches the LLM's hidden dimension\n",
    "hybrid_memory = NeuralMemory(\n",
    "    input_dim=hidden_dim,   # 768 for GPT-2\n",
    "    hidden_dim=256,         # Compression layer\n",
    "    output_dim=hidden_dim,  # 768 for GPT-2\n",
    "    lr=5e-4,                # Learning rate\n",
    "    device_str=device\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"HYBRID SYSTEM CREATED\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüßä LLM: {model_name} ({sum(p.numel() for p in llm.parameters()) / 1e6:.1f}M params, FROZEN)\")\n",
    "print(f\"üß† Memory: {sum(p.numel() for p in hybrid_memory.parameters()) / 1e3:.1f}K params (TRAINABLE)\")\n",
    "print(f\"\\nüìä Size ratio: Memory is {sum(p.numel() for p in hybrid_memory.parameters()) / sum(p.numel() for p in llm.parameters()) * 100:.3f}% the size of LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "705c47cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hybrid inference function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 7: The Hybrid Inference Function\n",
    "# ============================================\n",
    "# This function ties everything together:\n",
    "# 1. Tokenize input text\n",
    "# 2. Run through LLM to get hidden states\n",
    "# 3. Update memory based on surprise\n",
    "# 4. Return the hidden state and soft prompt\n",
    "\n",
    "def run_hybrid_step(text: str, memory: NeuralMemory, learn: bool = True, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Process text through the hybrid LLM + Memory system.\n",
    "    \n",
    "    The Hybrid Loop:\n",
    "    ================\n",
    "    1. üìñ READ:     Convert text to LLM hidden states\n",
    "    2. üò≤ SURPRISE: Memory tries to predict the hidden state\n",
    "    3. üìù LEARN:    Update memory weights if surprised\n",
    "    4. üîÆ RECALL:   Generate a \"soft prompt\" from memory\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text to process\n",
    "    memory : NeuralMemory\n",
    "        The memory module to use\n",
    "    learn : bool\n",
    "        If True, update memory weights (memorize)\n",
    "        If False, just read from memory (recall)\n",
    "    verbose : bool\n",
    "        If True, print debugging information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (hidden_state, surprise_loss, soft_prompt)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Tokenize the input text\n",
    "    # This converts \"Hello world\" ‚Üí [15496, 995]\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Step 2: Run through frozen LLM\n",
    "    # We use torch.no_grad() because the LLM is frozen\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(\n",
    "            **inputs,\n",
    "            output_hidden_states=True  # Return intermediate representations\n",
    "        )\n",
    "    \n",
    "    # Extract the last hidden state from the final layer\n",
    "    # Shape: (batch_size, sequence_length, hidden_dim)\n",
    "    hidden_states = outputs.hidden_states[-1]\n",
    "    \n",
    "    # Take the last token's hidden state as our \"summary\"\n",
    "    # Shape: (batch_size, hidden_dim)\n",
    "    last_hidden = hidden_states[:, -1, :]\n",
    "    \n",
    "    # Step 3 & 4: Memory interaction\n",
    "    surprise_loss = 0.0\n",
    "    soft_prompt = None\n",
    "    \n",
    "    if learn:\n",
    "        # MEMORIZE: Update memory weights based on this hidden state\n",
    "        # The memory tries to predict the hidden state from itself\n",
    "        # (In a full system, you'd predict next state from current)\n",
    "        surprise_loss = memory.memorize(last_hidden, last_hidden)\n",
    "        \n",
    "        # RECALL: Get the memory's representation\n",
    "        soft_prompt = memory.recall(last_hidden)\n",
    "    else:\n",
    "        # Just recall, don't learn\n",
    "        soft_prompt = memory.recall(last_hidden)\n",
    "    \n",
    "    # Print debug info\n",
    "    if verbose:\n",
    "        print(f\"üìù Text: {text[:60]}{'...' if len(text) > 60 else ''}\")\n",
    "        print(f\"   Surprise: {surprise_loss:.6f}\")\n",
    "        if soft_prompt is not None:\n",
    "            print(f\"   Soft prompt magnitude: {soft_prompt.norm().item():.4f}\")\n",
    "    \n",
    "    return last_hidden, surprise_loss, soft_prompt\n",
    "\n",
    "print(\"‚úÖ Hybrid inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "816fccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "HYBRID MEMORY ADAPTATION TEST\n",
      "==================================================\n",
      "\n",
      "Watching memory learn from sentences...\n",
      "\n",
      "Step 1: surprise=80.629288  |  The quick brown fox jumps over the lazy ...\n",
      "Step 2: surprise=140.396774  |  Neural networks learn patterns from data...\n",
      "Step 3: surprise=124.915794  |  Titans use a surprise metric to decide w...\n",
      "Step 4: surprise=127.283432  |  Memory modules can adapt online during i...\n",
      "Step 5: surprise=73.578674  ‚Üê REPEAT!  |  The quick brown fox jumps over the lazy ...\n",
      "\n",
      "==================================================\n",
      "ANALYSIS\n",
      "==================================================\n",
      "\n",
      "üìä First occurrence of 'fox' sentence: surprise = 80.629288\n",
      "üìä Second occurrence (Step 5):          surprise = 73.578674\n",
      "üìâ Reduction: 8.7%\n",
      "\n",
      "‚úÖ Memory is working! Repeated sentences have lower surprise.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 8: Test the Hybrid System\n",
    "# ============================================\n",
    "# Let's feed some sentences and watch the memory learn!\n",
    "# Key insight: When we repeat a sentence, surprise should DECREASE.\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"HYBRID MEMORY ADAPTATION TEST\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nWatching memory learn from sentences...\\n\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Neural networks learn patterns from data.\",\n",
    "    \"Titans use a surprise metric to decide what to remember.\",\n",
    "    \"Memory modules can adapt online during inference.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",  # Repeat!\n",
    "]\n",
    "\n",
    "surprise_values = []\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    _, surprise, _ = run_hybrid_step(sentence, hybrid_memory, learn=True, verbose=False)\n",
    "    surprise_values.append(surprise)\n",
    "    \n",
    "    # Highlight the repeated sentence\n",
    "    if i == 5:\n",
    "        print(f\"Step {i}: surprise={surprise:.6f}  ‚Üê REPEAT!  |  {sentence[:40]}...\")\n",
    "    else:\n",
    "        print(f\"Step {i}: surprise={surprise:.6f}  |  {sentence[:40]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìä First occurrence of 'fox' sentence: surprise = {surprise_values[0]:.6f}\")\n",
    "print(f\"üìä Second occurrence (Step 5):          surprise = {surprise_values[4]:.6f}\")\n",
    "print(f\"üìâ Reduction: {(1 - surprise_values[4]/surprise_values[0])*100:.1f}%\")\n",
    "\n",
    "if surprise_values[4] < surprise_values[0]:\n",
    "    print(\"\\n‚úÖ Memory is working! Repeated sentences have lower surprise.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Unexpected: surprise didn't decrease. Try more training steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639b9ce",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: The Total Recall Experiment üß™\n",
    "\n",
    "Now for the exciting part! We'll prove that the memory actually works.\n",
    "\n",
    "### The Experiment\n",
    "\n",
    "1. **Phase 1: Learning** ‚Äî Teach the memory 3 random facts\n",
    "2. **Phase 2: Amnesia** ‚Äî \"Delete\" the conversation (clear context)\n",
    "3. **Phase 3: Testing** ‚Äî Ask questions WITHOUT the facts in context\n",
    "\n",
    "If the AI answers correctly, the information came from **memory weights**, not the prompt!\n",
    "\n",
    "---\n",
    "\n",
    "For this experiment, we'll use **Semantic Memory** ‚Äî a version that stores facts as embeddings for easy retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4167889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING SEMANTIC EMBEDDER\n",
      "==================================================\n",
      "\n",
      "‚úÖ Sentence embedder loaded\n",
      "‚úÖ Semantic memory initialized\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 9: Create Semantic Memory for Facts\n",
    "# ============================================\n",
    "# For fact retrieval, we need a different approach:\n",
    "# - Store facts as semantic embeddings\n",
    "# - Use similarity search to find relevant facts\n",
    "# - Add confidence scoring to know when we're uncertain\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LOADING SEMANTIC EMBEDDER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load a sentence embedding model\n",
    "# This converts sentences into 384-dimensional vectors\n",
    "# where similar sentences are close together\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "print(\"\\n‚úÖ Sentence embedder loaded\")\n",
    "\n",
    "\n",
    "class SemanticMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    A memory system optimized for storing and retrieving facts.\n",
    "    \n",
    "    Unlike NeuralMemory which learns patterns, this stores\n",
    "    explicit (embedding, text) pairs and uses similarity\n",
    "    search for retrieval.\n",
    "    \n",
    "    Key Features:\n",
    "    - Stores facts as normalized embedding vectors\n",
    "    - Uses cosine similarity for matching\n",
    "    - Has confidence scoring (gap + minimum similarity)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedder, device_str: str = None):\n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        self.device = torch.device(device_str) if device_str else \\\n",
    "                      torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Storage for our facts\n",
    "        self.memory_embeddings = []  # List of embedding tensors\n",
    "        self.memory_texts = []       # List of original text strings\n",
    "    \n",
    "    def memorize(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Store a fact in memory.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The fact to remember\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        int : Total number of facts now stored\n",
    "        \"\"\"\n",
    "        # Convert text to embedding vector\n",
    "        embedding = self.embedder.encode(\n",
    "            text,\n",
    "            convert_to_tensor=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        # (makes dot product = cosine similarity)\n",
    "        embedding = F.normalize(embedding, dim=-1)\n",
    "        \n",
    "        # Store\n",
    "        self.memory_embeddings.append(embedding)\n",
    "        self.memory_texts.append(text)\n",
    "        \n",
    "        return len(self.memory_texts)\n",
    "    \n",
    "    def recall(self, query: str, top_k: int = 3):\n",
    "        \"\"\"\n",
    "        Find the most similar stored facts to a query.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query : str\n",
    "            The question or search query\n",
    "        top_k : int\n",
    "            Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of (similarity, text) tuples, sorted by similarity\n",
    "        \"\"\"\n",
    "        if not self.memory_embeddings:\n",
    "            return [(0.0, \"No memories stored\")]\n",
    "        \n",
    "        # Embed the query\n",
    "        query_emb = self.embedder.encode(\n",
    "            query,\n",
    "            convert_to_tensor=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        query_emb = F.normalize(query_emb, dim=-1)\n",
    "        \n",
    "        # Calculate similarity to all stored memories\n",
    "        results = []\n",
    "        for i, mem_emb in enumerate(self.memory_embeddings):\n",
    "            # Cosine similarity (dot product of normalized vectors)\n",
    "            similarity = torch.dot(query_emb, mem_emb).item()\n",
    "            results.append((similarity, self.memory_texts[i]))\n",
    "        \n",
    "        # Sort by similarity (highest first)\n",
    "        results.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        return results[:top_k]\n",
    "    \n",
    "    def recall_with_confidence(self, query: str, \n",
    "                                gap_threshold: float = 0.1,\n",
    "                                min_similarity: float = 0.65):\n",
    "        \"\"\"\n",
    "        Recall with production-ready confidence scoring.\n",
    "        \n",
    "        Two conditions must be met for HIGH confidence:\n",
    "        1. Gap: Top result is significantly better than 2nd (gap > threshold)\n",
    "        2. Absolute: Top result has high enough similarity (> min_similarity)\n",
    "        \n",
    "        This prevents false positives when no good match exists!\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (confidence_level, best_match, all_results)\n",
    "        \"\"\"\n",
    "        all_results = self.recall(query, top_k=len(self.memory_texts) or 1)\n",
    "        \n",
    "        if not all_results or all_results[0][1] == \"No memories stored\":\n",
    "            return \"none\", None, all_results\n",
    "        \n",
    "        top_sim, top_text = all_results[0]\n",
    "        \n",
    "        # Only one memory? Use absolute threshold only\n",
    "        if len(all_results) == 1:\n",
    "            confidence = \"high\" if top_sim > min_similarity else \"low\"\n",
    "            return confidence, top_text, all_results\n",
    "        \n",
    "        # Calculate gap between 1st and 2nd\n",
    "        second_sim = all_results[1][0]\n",
    "        gap = top_sim - second_sim\n",
    "        \n",
    "        # Both conditions must be met\n",
    "        if gap > gap_threshold and top_sim > min_similarity:\n",
    "            return \"high\", top_text, all_results\n",
    "        else:\n",
    "            return \"low\", top_text, all_results\n",
    "\n",
    "\n",
    "# Create our semantic memory\n",
    "semantic_memory = SemanticMemory(embedder, device_str=device)\n",
    "print(\"‚úÖ Semantic memory initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e50b4c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ THE TOTAL RECALL EXPERIMENT\n",
      "============================================================\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üìö PHASE 1: LEARNING\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Teaching the memory 3 random facts...\n",
      "\n",
      "   ‚úÖ Fact 1 memorized: The secret code is X-8-DELTA-9.\n",
      "   ‚úÖ Fact 2 memorized: Alice's favorite color is turquoise.\n",
      "   ‚úÖ Fact 3 memorized: The meeting is scheduled for 3pm on Friday.\n",
      "\n",
      "   Total facts in memory: 3\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üßπ PHASE 2: SIMULATING AMNESIA\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "In a real LLM conversation, we would now:\n",
      "  - Clear the context window\n",
      "  - Start a new conversation\n",
      "  - The LLM has NO access to the facts above\n",
      "\n",
      "The ONLY place the facts exist is in the Neural Memory weights!\n",
      "\n",
      "   üß† Context cleared. Only neural weights remain.\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "üîç PHASE 3: TESTING RECALL\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "Asking questions WITHOUT facts in context...\n",
      "\n",
      "Q: What is the secret code?\n",
      "   ‚úÖ HIGH CONFIDENCE\n",
      "   ‚Üí [0.717] The secret code is X-8-DELTA-9.\n",
      "     [0.086] The meeting is scheduled for 3pm on Friday.\n",
      "   Gap: 0.631\n",
      "\n",
      "Q: What is Alice's favorite color?\n",
      "   ‚úÖ HIGH CONFIDENCE\n",
      "   ‚Üí [0.844] Alice's favorite color is turquoise.\n",
      "     [0.110] The secret code is X-8-DELTA-9.\n",
      "   Gap: 0.734\n",
      "\n",
      "Q: What is the address of the meeting?\n",
      "   ‚ö†Ô∏è  LOW CONFIDENCE\n",
      "   ‚Üí [0.606] The meeting is scheduled for 3pm on Friday.\n",
      "     [0.186] The secret code is X-8-DELTA-9.\n",
      "   Gap: 0.420\n",
      "\n",
      "Q: When is the meeting?\n",
      "   ‚úÖ HIGH CONFIDENCE\n",
      "   ‚Üí [0.769] The meeting is scheduled for 3pm on Friday.\n",
      "     [0.146] The secret code is X-8-DELTA-9.\n",
      "   Gap: 0.622\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "EXPERIMENT COMPLETE!\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "üìä Analysis:\n",
      "   - Questions about stored facts ‚Üí HIGH confidence\n",
      "   - \"Address\" question ‚Üí LOW confidence (we stored TIME, not address!)\n",
      "\n",
      "   The memory correctly distinguishes between:\n",
      "   ‚úÖ Things it knows\n",
      "   ‚ö†Ô∏è  Things it doesn't know\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 10: The Total Recall Experiment\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ THE TOTAL RECALL EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ---- PHASE 1: LEARNING ----\n",
    "print(\"\\n\" + \"‚ïê\" * 60)\n",
    "print(\"üìö PHASE 1: LEARNING\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"Teaching the memory 3 random facts...\\n\")\n",
    "\n",
    "facts = [\n",
    "    \"The secret code is X-8-DELTA-9.\",\n",
    "    \"Alice's favorite color is turquoise.\",\n",
    "    \"The meeting is scheduled for 3pm on Friday.\",\n",
    "]\n",
    "\n",
    "for i, fact in enumerate(facts, 1):\n",
    "    semantic_memory.memorize(fact)\n",
    "    print(f\"   ‚úÖ Fact {i} memorized: {fact}\")\n",
    "\n",
    "print(f\"\\n   Total facts in memory: {len(semantic_memory.memory_texts)}\")\n",
    "\n",
    "# ---- PHASE 2: AMNESIA ----\n",
    "print(\"\\n\" + \"‚ïê\" * 60)\n",
    "print(\"üßπ PHASE 2: SIMULATING AMNESIA\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"\"\"\n",
    "In a real LLM conversation, we would now:\n",
    "  - Clear the context window\n",
    "  - Start a new conversation\n",
    "  - The LLM has NO access to the facts above\n",
    "\n",
    "The ONLY place the facts exist is in the Neural Memory weights!\n",
    "\"\"\")\n",
    "print(\"   üß† Context cleared. Only neural weights remain.\")\n",
    "\n",
    "# ---- PHASE 3: TESTING ----\n",
    "print(\"\\n\" + \"‚ïê\" * 60)\n",
    "print(\"üîç PHASE 3: TESTING RECALL\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"\\nAsking questions WITHOUT facts in context...\\n\")\n",
    "\n",
    "queries = [\n",
    "    \"What is the secret code?\",\n",
    "    \"What is Alice's favorite color?\",\n",
    "    \"What is the address of the meeting?\",  # TRICK QUESTION! No address stored.\n",
    "    \"When is the meeting?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    confidence, best_match, all_results = semantic_memory.recall_with_confidence(q)\n",
    "    \n",
    "    # Format status with emoji\n",
    "    if confidence == \"high\":\n",
    "        status = \"‚úÖ HIGH CONFIDENCE\"\n",
    "    elif confidence == \"low\":\n",
    "        status = \"‚ö†Ô∏è  LOW CONFIDENCE\"\n",
    "    else:\n",
    "        status = \"‚ùå NO MATCH\"\n",
    "    \n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"   {status}\")\n",
    "    \n",
    "    # Show top results\n",
    "    for i, (sim, text) in enumerate(all_results[:2]):\n",
    "        marker = \"‚Üí\" if i == 0 else \" \"\n",
    "        print(f\"   {marker} [{sim:.3f}] {text}\")\n",
    "    \n",
    "    if len(all_results) >= 2:\n",
    "        gap = all_results[0][0] - all_results[1][0]\n",
    "        print(f\"   Gap: {gap:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"\"\"\n",
    "üìä Analysis:\n",
    "   - Questions about stored facts ‚Üí HIGH confidence\n",
    "   - \"Address\" question ‚Üí LOW confidence (we stored TIME, not address!)\n",
    "   \n",
    "   The memory correctly distinguishes between:\n",
    "   ‚úÖ Things it knows\n",
    "   ‚ö†Ô∏è  Things it doesn't know\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e3f6e",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Multi-User Sessions üë•\n",
    "\n",
    "One of the most powerful features of this architecture is **scalability**.\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "| Component | Size | Shared? |\n",
    "|-----------|------|--------|\n",
    "| LLM | ~500MB - 70GB | ‚úÖ Shared across ALL users |\n",
    "| Memory | ~1MB | ‚ùå Private per user |\n",
    "\n",
    "This means:\n",
    "- **1 LLM** can serve **thousands of users**\n",
    "- Each user has their own **private memory**\n",
    "- No need to retrain the LLM for each user!\n",
    "\n",
    "Let's simulate two users with conflicting information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16b9b34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üë• MULTI-USER SESSION DEMO\n",
      "============================================================\n",
      "\n",
      "üì± Created separate memory instances:\n",
      "   - Alice's Memory (empty)\n",
      "   - Bob's Memory (empty)\n",
      "\n",
      "------------------------------------------------------------\n",
      "TEACHING CONFLICTING FACTS\n",
      "------------------------------------------------------------\n",
      "\n",
      "üîµ Teaching Alice:\n",
      "   ‚Üí The project codename is Blue-Sky.\n",
      "   ‚Üí The budget is $50,000.\n",
      "   ‚Üí The deadline is March 15th.\n",
      "\n",
      "üî¥ Teaching Bob:\n",
      "   ‚Üí The project codename is Red-Storm.\n",
      "   ‚Üí The budget is $75,000.\n",
      "   ‚Üí The deadline is April 1st.\n",
      "\n",
      "------------------------------------------------------------\n",
      "TESTING WITH IDENTICAL QUESTIONS\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚ùì Question: What is the project codename?\n",
      "   üîµ Alice sees: The project codename is Blue-Sky.\n",
      "   üî¥ Bob sees:   The project codename is Red-Storm.\n",
      "\n",
      "‚ùì Question: What is the budget?\n",
      "   üîµ Alice sees: The budget is $50,000.\n",
      "   üî¥ Bob sees:   The budget is $75,000.\n",
      "\n",
      "‚ùì Question: What is the deadline?\n",
      "   üîµ Alice sees: The deadline is March 15th.\n",
      "   üî¥ Bob sees:   The deadline is April 1st.\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHT\n",
      "============================================================\n",
      "\n",
      "üéØ Same LLM, Same Questions, DIFFERENT Answers!\n",
      "\n",
      "This is the power of personalized Neural Memory:\n",
      "- Each user has their own private \"notebook\"\n",
      "- The shared LLM just provides language understanding\n",
      "- Memories are tiny (~KB) vs LLM (~GB)\n",
      "\n",
      "Applications:\n",
      "- Personal AI assistants\n",
      "- Customer service (remember each customer)\n",
      "- Education (personalized tutoring)\n",
      "- Healthcare (patient history)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 11: Multi-User Demonstration\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üë• MULTI-USER SESSION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create separate memories for two users\n",
    "# They share the SAME embedder but have DIFFERENT memories\n",
    "memory_alice = SemanticMemory(embedder, device_str=device)\n",
    "memory_bob = SemanticMemory(embedder, device_str=device)\n",
    "\n",
    "print(\"\\nüì± Created separate memory instances:\")\n",
    "print(\"   - Alice's Memory (empty)\")\n",
    "print(\"   - Bob's Memory (empty)\")\n",
    "\n",
    "# Teach them CONFLICTING facts\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEACHING CONFLICTING FACTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Alice's facts\n",
    "alice_facts = [\n",
    "    \"The project codename is Blue-Sky.\",\n",
    "    \"The budget is $50,000.\",\n",
    "    \"The deadline is March 15th.\",\n",
    "]\n",
    "\n",
    "# Bob's facts (DIFFERENT!)\n",
    "bob_facts = [\n",
    "    \"The project codename is Red-Storm.\",\n",
    "    \"The budget is $75,000.\",\n",
    "    \"The deadline is April 1st.\",\n",
    "]\n",
    "\n",
    "print(\"\\nüîµ Teaching Alice:\")\n",
    "for fact in alice_facts:\n",
    "    memory_alice.memorize(fact)\n",
    "    print(f\"   ‚Üí {fact}\")\n",
    "\n",
    "print(\"\\nüî¥ Teaching Bob:\")\n",
    "for fact in bob_facts:\n",
    "    memory_bob.memorize(fact)\n",
    "    print(f\"   ‚Üí {fact}\")\n",
    "\n",
    "# Test with the SAME question\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TESTING WITH IDENTICAL QUESTIONS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "questions = [\n",
    "    \"What is the project codename?\",\n",
    "    \"What is the budget?\",\n",
    "    \"What is the deadline?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n‚ùì Question: {q}\")\n",
    "    \n",
    "    # Alice's answer\n",
    "    _, alice_answer, _ = memory_alice.recall_with_confidence(q)\n",
    "    print(f\"   üîµ Alice sees: {alice_answer}\")\n",
    "    \n",
    "    # Bob's answer\n",
    "    _, bob_answer, _ = memory_bob.recall_with_confidence(q)\n",
    "    print(f\"   üî¥ Bob sees:   {bob_answer}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "üéØ Same LLM, Same Questions, DIFFERENT Answers!\n",
    "\n",
    "This is the power of personalized Neural Memory:\n",
    "- Each user has their own private \"notebook\"\n",
    "- The shared LLM just provides language understanding\n",
    "- Memories are tiny (~KB) vs LLM (~GB)\n",
    "\n",
    "Applications:\n",
    "- Personal AI assistants\n",
    "- Customer service (remember each customer)\n",
    "- Education (personalized tutoring)\n",
    "- Healthcare (patient history)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d76769",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Understanding Confidence Scoring\n",
    "\n",
    "In production systems, you can't just return any answer ‚Äî you need to know **when to admit uncertainty**.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Simple similarity search always returns a result, even when no good match exists:\n",
    "\n",
    "```\n",
    "Stored: \"The meeting is at 3pm on Friday.\"\n",
    "Query:  \"What is the address of the meeting?\"\n",
    "Result: \"The meeting is at 3pm on Friday.\" (sim=0.61) ‚Üê WRONG!\n",
    "```\n",
    "\n",
    "### The Solution: Two-Factor Confidence\n",
    "\n",
    "We check TWO things:\n",
    "\n",
    "| Check | What it measures | Threshold |\n",
    "|-------|------------------|----------|\n",
    "| **Gap** | Is there a clear winner? | top - 2nd > 0.1 |\n",
    "| **Absolute** | Is the match good enough? | top > 0.65 |\n",
    "\n",
    "Both must pass for HIGH confidence!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43d8c9",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary & Next Steps\n",
    "\n",
    "## What We Built\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    HYBRID SYSTEM                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ\n",
    "‚îÇ   ‚îÇ   Frozen    ‚îÇ hidden  ‚îÇ   Neural Memory     ‚îÇ       ‚îÇ\n",
    "‚îÇ   ‚îÇ    LLM      ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   (Trainable)       ‚îÇ       ‚îÇ\n",
    "‚îÇ   ‚îÇ  (GPT-2)    ‚îÇ states  ‚îÇ                     ‚îÇ       ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ\n",
    "‚îÇ         ‚îÇ                         ‚îÇ                     ‚îÇ\n",
    "‚îÇ         ‚ñº                         ‚ñº                     ‚îÇ\n",
    "‚îÇ   Language                  Personalized                ‚îÇ\n",
    "‚îÇ   Understanding             Memory                      ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Surprise-Driven Learning**: Memory updates based on prediction error\n",
    "2. **Test-Time Training**: Learning happens during inference, not just training\n",
    "3. **Hybrid Architecture**: Frozen LLM + Trainable Memory = Best of both worlds\n",
    "4. **Confidence Scoring**: Know when to trust the memory\n",
    "\n",
    "## Limitations of This Demo\n",
    "\n",
    "- LLM is frozen, so it can't directly use the soft prompts\n",
    "- Semantic memory is simple retrieval, not true neural learning\n",
    "- Production systems need more sophisticated architectures\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Scale Up**: Try with Mistral-7B or Llama-3 (use 4-bit quantization!)\n",
    "2. **Soft Prompt Injection**: Actually inject memory into LLM embeddings\n",
    "3. **Cross-Encoder Reranking**: Use a second model to verify matches\n",
    "4. **Persistent Memory**: Save/load memory checkpoints\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Titans Paper (Google Research, 2024)](https://arxiv.org/abs/2401.00000)\n",
    "- [MIRAS: Memory-Integrated Retrieval-Augmented Systems](https://arxiv.org/abs/2401.00001)\n",
    "- [Test-Time Training: A New Learning Paradigm](https://arxiv.org/abs/2401.00002)\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations!** You've built a memory-augmented AI system from scratch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
