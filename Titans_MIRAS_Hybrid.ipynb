{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faad5306",
   "metadata": {},
   "source": [
    "# üß† Titans-MIRAS: Teaching AI to Remember\n",
    "\n",
    "## A Beginner's Guide to Neural Memory Systems\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "In this tutorial, you'll discover how to give an AI \"long-term memory\" ‚Äî the ability to learn and remember facts **during a conversation**, not just during training.\n",
    "\n",
    "By the end, you'll understand:\n",
    "- ü§î **The Problem**: Why ChatGPT \"forgets\" things\n",
    "- üí° **The Solution**: Neural Memory modules that learn in real-time\n",
    "- üîß **The Code**: How to build your own memory-augmented AI\n",
    "- üß™ **The Proof**: A demo where AI remembers facts without being told twice\n",
    "\n",
    "---\n",
    "\n",
    "### The Problem with Today's AI\n",
    "\n",
    "Imagine talking to ChatGPT:\n",
    "\n",
    "```\n",
    "You: My name is Alice and I live in Tokyo.\n",
    "AI:  Nice to meet you, Alice!\n",
    "\n",
    "... (1000 messages later) ...\n",
    "\n",
    "You: Where do I live?\n",
    "AI:  I don't have access to that information.\n",
    "```\n",
    "\n",
    "**Why does this happen?**\n",
    "\n",
    "LLMs (Large Language Models) like GPT-4 have a **fixed context window** ‚Äî a limited \"short-term memory\" of recent messages. Once your conversation exceeds this limit (typically 4K-128K tokens), older information is simply **deleted**.\n",
    "\n",
    "The AI's **weights** (the billions of numbers that define its knowledge) were set during training and **never change** during conversation. It can't truly \"learn\" new facts about you.\n",
    "\n",
    "---\n",
    "\n",
    "### The Titans Solution\n",
    "\n",
    "In 2024, Google Research introduced **Titans** ‚Äî an architecture where a small neural network called **Neural Memory** runs alongside the LLM and **updates its weights in real-time**.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "| Component | Analogy | What it does |\n",
    "|-----------|---------|-------------|\n",
    "| **LLM (Frozen)** | üìö Library | Has vast knowledge but can't add new books |\n",
    "| **Neural Memory** | üìù Notebook | Small, personal, constantly updated |\n",
    "\n",
    "The Neural Memory learns by measuring **\"Surprise\"** ‚Äî if something is unexpected (hard to predict), it's worth remembering!\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This tutorial assumes you know:\n",
    "- Basic Python (variables, functions, loops)\n",
    "- What a neural network is (layers, weights, forward pass)\n",
    "- Basic PyTorch syntax (tensors, `torch.nn`)\n",
    "\n",
    "Don't worry if you're rusty ‚Äî we'll explain everything step by step! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102ae59",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "\n",
    "**Running on Colab?** Execute the cell below to install dependencies and configure the environment.\n",
    "\n",
    "**Running locally?** Skip this cell (dependencies should be installed via `pip install -r requirements.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41911de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GOOGLE COLAB SETUP (Skip if running locally)\n",
    "# ============================================================================\n",
    "# This cell installs all required dependencies for Google Colab\n",
    "# If you're running locally, you can skip this cell.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Check if we're running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"[COLAB] Detected Google Colab environment\")\n",
    "    print(\"[COLAB] Installing required packages...\")\n",
    "    \n",
    "    # Install core dependencies\n",
    "    !pip install -q torch torchvision torchaudio\n",
    "    !pip install -q transformers accelerate\n",
    "    !pip install -q sentence-transformers\n",
    "    !pip install -q matplotlib seaborn requests tqdm scikit-learn\n",
    "    \n",
    "    # Configure matplotlib for Colab\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    \n",
    "    # Check GPU availability\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"[COLAB] GPU detected: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    else:\n",
    "        print(\"[COLAB] No GPU detected. This notebook can run on CPU.\")\n",
    "        print(\"[COLAB] Tip: For better performance, go to Runtime -> Change runtime type -> GPU\")\n",
    "    \n",
    "    print(\"[COLAB] Setup complete!\")\n",
    "else:\n",
    "    print(\"[LOCAL] Not running in Colab. Skipping setup.\")\n",
    "    print(\"[LOCAL] Make sure you've run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6ce72",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Environment Setup\n",
    "\n",
    "Before we start coding, let's make sure your computer is ready.\n",
    "\n",
    "### What We Need\n",
    "- **Python 3.10+** ‚Äî The programming language\n",
    "- **PyTorch** ‚Äî The deep learning framework\n",
    "- **Transformers** ‚Äî Hugging Face's library for pre-trained models\n",
    "- **A GPU** (optional but recommended) ‚Äî Makes everything 10-100x faster\n",
    "\n",
    "Let's check what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: Check Python and System Information\n",
    "# ============================================\n",
    "# This cell verifies that Python is installed correctly\n",
    "# and shows information about your operating system.\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Operating System: {platform.platform()}\")\n",
    "print()\n",
    "\n",
    "# Check if PyTorch is installed\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch installed: version {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not found. Install with: pip install torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae02c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 2: Check GPU Availability\n",
    "# ============================================\n",
    "# GPUs (Graphics Processing Units) can run neural networks\n",
    "# much faster than CPUs. This cell checks if you have one.\n",
    "#\n",
    "# Don't worry if you don't have a GPU ‚Äî everything will\n",
    "# still work, just a bit slower!\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress CUDA capability warnings (e.g., for newer GPUs)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*cuda capability.*\", category=UserWarning)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # CUDA is NVIDIA's GPU computing platform\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu_index = torch.cuda.current_device()\n",
    "    gpu_name = torch.cuda.get_device_name(gpu_index)\n",
    "    gpu_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "    gpu_memory_gb = round(gpu_memory / (1024**3), 2)\n",
    "    \n",
    "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory_gb} GB\")\n",
    "    \n",
    "    # Quick test: can we do math on the GPU?\n",
    "    x = torch.randn(1000, 1000, device=device)\n",
    "    y = torch.matmul(x, x)  # Matrix multiplication\n",
    "    print(f\"   GPU computation test: ‚úÖ Passed\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è  No GPU found. Using CPU (slower but works fine!)\")\n",
    "\n",
    "print(f\"\\nüéØ We'll use: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223aa4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: System Architecture Overview\n",
    "# ============================================\n",
    "# Let's visualize what we're building today!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"What We're Building: Hybrid Memory System\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Draw the LLM box (frozen)\n",
    "llm_box = mpatches.FancyBboxPatch((1, 2), 3, 2.5, boxstyle=\"round,pad=0.1\", \n",
    "                                   facecolor='#E3F2FD', edgecolor='#1976D2', linewidth=3)\n",
    "ax.add_patch(llm_box)\n",
    "ax.text(2.5, 3.25, \"[FROZEN]\\nLLM (GPT-2)\", ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "ax.text(2.5, 2.3, \"124M params\\nNever changes\", ha='center', va='center', fontsize=9, color='gray')\n",
    "\n",
    "# Draw the Memory box (trainable)\n",
    "mem_box = mpatches.FancyBboxPatch((7, 2), 3, 2.5, boxstyle=\"round,pad=0.1\", \n",
    "                                   facecolor='#E8F5E9', edgecolor='#388E3C', linewidth=3)\n",
    "ax.add_patch(mem_box)\n",
    "ax.text(8.5, 3.25, \"[TRAINABLE]\\nNeural Memory\", ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "ax.text(8.5, 2.3, \"~400K params\\nLearns in real-time\", ha='center', va='center', fontsize=9, color='gray')\n",
    "\n",
    "# Draw arrow from LLM to Memory\n",
    "ax.annotate('', xy=(7, 3.25), xytext=(4, 3.25),\n",
    "            arrowprops=dict(arrowstyle='->', color='#FF5722', lw=2))\n",
    "ax.text(5.5, 3.6, \"Hidden States\", ha='center', fontsize=10, color='#FF5722')\n",
    "\n",
    "# Draw input\n",
    "ax.annotate('', xy=(1, 3.25), xytext=(0.2, 3.25),\n",
    "            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "ax.text(0.6, 3.6, \"Text\\nInput\", ha='center', fontsize=9)\n",
    "\n",
    "# Draw output\n",
    "ax.annotate('', xy=(11.5, 3.25), xytext=(10, 3.25),\n",
    "            arrowprops=dict(arrowstyle='->', color='black', lw=2))\n",
    "ax.text(11, 3.6, \"Memory\\nRecall\", ha='center', fontsize=9)\n",
    "\n",
    "# Add size comparison\n",
    "ax.text(6, 0.8, \"Size Comparison: Memory is 0.3% of LLM size!\", \n",
    "        ha='center', fontsize=11, style='italic', color='#666')\n",
    "\n",
    "# Legend\n",
    "ax.text(6, 5.5, \"The Big Idea: One giant frozen brain + tiny personal notebooks\", \n",
    "        ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb19729",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "- **GPU Memory**: More is better. 8GB+ is great for this tutorial.\n",
    "- **CUDA**: This is NVIDIA's framework for GPU computing. If you see it, you're good!\n",
    "- **CPU fallback**: Everything works on CPU too, just 5-10x slower.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd4a65",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Understanding Neural Memory\n",
    "\n",
    "Before we write code, let's understand the **theory** behind Neural Memory.\n",
    "\n",
    "## The Core Idea: Learning Through Surprise\n",
    "\n",
    "Imagine you're reading a book:\n",
    "- \"The sky is blue\" ‚Üí üò¥ Boring, you already knew that\n",
    "- \"The password is X-8-DELTA-9\" ‚Üí üò≤ Surprising! Worth remembering!\n",
    "\n",
    "Neural Memory works the same way. It measures **Surprise** (also called \"prediction error\"):\n",
    "\n",
    "```\n",
    "Surprise = How wrong was my prediction?\n",
    "         = ||What I expected - What actually happened||¬≤\n",
    "```\n",
    "\n",
    "### The Mathematical Formula\n",
    "\n",
    "$$\\text{Surprise}(x, y) = ||f(x) - y||^2$$\n",
    "\n",
    "Where:\n",
    "- $x$ = Input (what the AI is currently \"thinking about\")\n",
    "- $y$ = Target (what we want it to remember)\n",
    "- $f(x)$ = Memory's prediction\n",
    "- $|| \\cdot ||^2$ = Mean Squared Error (MSE)\n",
    "\n",
    "### The Learning Loop\n",
    "\n",
    "```\n",
    "1. üìñ READ:     Get the current context from the LLM\n",
    "2. üò≤ SURPRISE: Calculate how unexpected this is\n",
    "3. üìù LEARN:    If surprising, update memory weights\n",
    "4. üîÆ RECALL:   Use memory to help with the next step\n",
    "```\n",
    "\n",
    "This is called **Test-Time Training (TTT)** ‚Äî the memory learns *during* inference, not just during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0ac53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: The Surprise-Driven Learning Loop\n",
    "# ============================================\n",
    "# Let's visualize how the memory learns through surprise!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ===== LEFT: The Learning Loop Diagram =====\n",
    "ax1 = axes[0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.axis('off')\n",
    "ax1.set_title(\"The Surprise-Driven Learning Loop\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Step boxes at cardinal positions (Top, Right, Bottom, Left)\n",
    "# Box size: width=2.6, height=1.8, centered at (x, y)\n",
    "steps = [\n",
    "    (5, 8.2, \"1. READ\", \"Get hidden state\\nfrom LLM\", '#E3F2FD'),       # TOP\n",
    "    (8.2, 5, \"2. SURPRISE\", \"Compare prediction\\nvs actual\", '#FFEBEE'),  # RIGHT\n",
    "    (5, 1.8, \"3. LEARN\", \"Update weights\\nif surprised\", '#E8F5E9'),    # BOTTOM\n",
    "    (1.8, 5, \"4. RECALL\", \"Generate soft\\nprompt\", '#FFF3E0'),          # LEFT\n",
    "]\n",
    "\n",
    "box_width = 2.6\n",
    "box_height = 1.8\n",
    "\n",
    "for x, y, title, desc, color in steps:\n",
    "    box = mpatches.FancyBboxPatch((x - box_width/2, y - box_height/2), box_width, box_height, \n",
    "                                   boxstyle=\"round,pad=0.1\", \n",
    "                                   facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax1.add_patch(box)\n",
    "    ax1.text(x, y + 0.3, title, ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    ax1.text(x, y - 0.4, desc, ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Draw arrows between steps (clockwise: TOP ‚Üí RIGHT ‚Üí BOTTOM ‚Üí LEFT ‚Üí TOP)\n",
    "arrow_style = dict(arrowstyle='->', color='#666', lw=2.5, connectionstyle='arc3,rad=0.3')\n",
    "\n",
    "# Arrow 1: READ (top) ‚Üí SURPRISE (right)\n",
    "ax1.annotate('', xy=(8.2 - box_width/2, 5 + box_height/2 - 0.2),  # right box top-left\n",
    "             xytext=(5 + box_width/2 - 0.2, 8.2 - box_height/2),   # top box bottom-right\n",
    "             arrowprops=arrow_style)\n",
    "\n",
    "# Arrow 2: SURPRISE (right) ‚Üí LEARN (bottom)\n",
    "ax1.annotate('', xy=(5 + box_width/2 - 0.2, 1.8 + box_height/2),  # bottom box top-right\n",
    "             xytext=(8.2 - box_width/2, 5 - box_height/2 + 0.2),   # right box bottom-left\n",
    "             arrowprops=arrow_style)\n",
    "\n",
    "# Arrow 3: LEARN (bottom) ‚Üí RECALL (left)\n",
    "ax1.annotate('', xy=(1.8 + box_width/2, 5 - box_height/2 + 0.2),  # left box bottom-right\n",
    "             xytext=(5 - box_width/2 + 0.2, 1.8 + box_height/2),   # bottom box top-left\n",
    "             arrowprops=arrow_style)\n",
    "\n",
    "# Arrow 4: RECALL (left) ‚Üí READ (top)\n",
    "ax1.annotate('', xy=(5 - box_width/2 + 0.2, 8.2 - box_height/2),  # top box bottom-left\n",
    "             xytext=(1.8 + box_width/2, 5 + box_height/2 - 0.2),   # left box top-right\n",
    "             arrowprops=arrow_style)\n",
    "\n",
    "# Center text\n",
    "ax1.text(5, 5, \"Neural\\nMemory\", ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# ===== RIGHT: Surprise Visualization =====\n",
    "ax2 = axes[1]\n",
    "ax2.set_title(\"What is 'Surprise'?\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Show prediction vs actual\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "# \"Boring\" example (low surprise)\n",
    "y_pred_boring = np.sin(x) * 0.5 + 2\n",
    "y_actual_boring = np.sin(x) * 0.5 + 2.1\n",
    "ax2.fill_between(x[:50], y_pred_boring[:50], y_actual_boring[:50], alpha=0.3, color='green', label='Low surprise (predictable)')\n",
    "\n",
    "# \"Surprising\" example (high surprise)\n",
    "y_pred_surprise = np.sin(x) * 0.5 + 5\n",
    "y_actual_surprise = np.cos(x) * 1.5 + 5\n",
    "ax2.fill_between(x[50:], y_pred_surprise[50:], y_actual_surprise[50:], alpha=0.3, color='red', label='High surprise (unexpected!)')\n",
    "\n",
    "ax2.plot(x[:50], y_pred_boring[:50], 'b--', label='Memory prediction', linewidth=2)\n",
    "ax2.plot(x[:50], y_actual_boring[:50], 'b-', label='Actual hidden state', linewidth=2)\n",
    "ax2.plot(x[50:], y_pred_surprise[50:], 'b--', linewidth=2)\n",
    "ax2.plot(x[50:], y_actual_surprise[50:], 'b-', linewidth=2)\n",
    "\n",
    "ax2.axvline(x=5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax2.text(2.5, 0.5, 'Boring...\\n\"The cat sat...\"', ha='center', fontsize=11)\n",
    "ax2.text(7.5, 0.5, 'Surprising!\\n\"Code: X-8-DELTA\"', ha='center', fontsize=11)\n",
    "\n",
    "ax2.set_xlabel('Time', fontsize=12)\n",
    "ax2.set_ylabel('Hidden State Value', fontsize=12)\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_ylim(0, 8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add formula\n",
    "ax2.text(5, 7.5, r'$Surprise = ||f(x) - y||^2$', ha='center', fontsize=14, \n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Insight: High surprise ‚Üí Strong learning signal ‚Üí Memory updates more!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: Build the Neural Memory Module\n",
    "# ============================================\n",
    "# This is the heart of our system: a small neural network\n",
    "# that can update its own weights during inference.\n",
    "#\n",
    "# Architecture:\n",
    "#   Input ‚Üí Linear ‚Üí GELU ‚Üí Linear ‚Üí Output\n",
    "#\n",
    "# Key Features:\n",
    "#   - Uses float32 for numerical stability (avoids NaN errors)\n",
    "#   - Has its own optimizer for real-time learning\n",
    "#   - memorize() updates weights, recall() just reads\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    A Neural Memory module that learns in real-time.\n",
    "    \n",
    "    Think of it as a tiny brain that watches the LLM and takes notes.\n",
    "    When something surprising happens, it updates its \"notes\" (weights)\n",
    "    so it can remember better next time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Size of input vectors (must match LLM hidden dimension)\n",
    "    hidden_dim : int\n",
    "        Size of the internal \"compression\" layer\n",
    "    output_dim : int\n",
    "        Size of output vectors (usually same as input_dim)\n",
    "    lr : float\n",
    "        Learning rate - how fast to update weights\n",
    "        Higher = faster learning but less stable\n",
    "        Lower = slower learning but more stable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, \n",
    "                 lr: float = 1e-3, device_str: str = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store dimensions for later reference\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Determine device (GPU or CPU)\n",
    "        if device_str:\n",
    "            self.device = torch.device(device_str)\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Build the neural network\n",
    "        # This is a simple 2-layer network:\n",
    "        #   1. Linear: Compress input to hidden dimension\n",
    "        #   2. GELU: Non-linear activation (lets us learn complex patterns)\n",
    "        #   3. Linear: Expand back to output dimension\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),   # Encoder\n",
    "            nn.GELU(),                          # Activation\n",
    "            nn.Linear(hidden_dim, output_dim),  # Decoder\n",
    "        )\n",
    "        \n",
    "        # Move to device and use float32 for stability\n",
    "        # (float16 can cause NaN errors during training)\n",
    "        self.to(self.device, torch.float32)\n",
    "        \n",
    "        # Create optimizer for real-time learning\n",
    "        # AdamW is a modern optimizer that works well for most cases\n",
    "        self.optim = torch.optim.AdamW(self.parameters(), lr=lr)\n",
    "        \n",
    "        # Loss function measures \"surprise\"\n",
    "        # MSE = Mean Squared Error = average of (prediction - target)¬≤\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    @torch.no_grad()  # This decorator means \"don't track gradients\"\n",
    "    def recall(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Retrieve information from memory WITHOUT learning.\n",
    "        \n",
    "        This is like reading your notes without adding new ones.\n",
    "        Fast and doesn't change the memory.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Query vector (what are we trying to remember?)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor\n",
    "            Memory's response to the query\n",
    "        \"\"\"\n",
    "        # Convert input to correct device and dtype\n",
    "        x = x.to(self.device, torch.float32)\n",
    "        \n",
    "        # Forward pass through the network\n",
    "        output = self.net(x)\n",
    "        \n",
    "        # .detach() disconnects from computation graph\n",
    "        # (we don't need gradients for recall)\n",
    "        return output.detach()\n",
    "    \n",
    "    def memorize(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Learn something new by updating memory weights.\n",
    "        \n",
    "        This is like studying: we look at what we should have known (y),\n",
    "        compare it to what we predicted (f(x)), and adjust our\n",
    "        understanding (weights) to do better next time.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input/query vector\n",
    "        y : torch.Tensor  \n",
    "            Target - what we want to remember\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            The \"surprise\" (loss) value - lower is better\n",
    "        \"\"\"\n",
    "        # Convert inputs to correct device and dtype\n",
    "        x = x.to(self.device, torch.float32)\n",
    "        y = y.to(self.device, torch.float32)\n",
    "        \n",
    "        # Forward pass: what does memory predict?\n",
    "        prediction = self.net(x)\n",
    "        \n",
    "        # Calculate surprise (how wrong were we?)\n",
    "        loss = self.loss_fn(prediction, y)\n",
    "        \n",
    "        # Backward pass: figure out how to improve\n",
    "        self.optim.zero_grad()  # Clear old gradients\n",
    "        loss.backward()          # Calculate new gradients\n",
    "        self.optim.step()        # Update weights\n",
    "        \n",
    "        # Return the surprise value as a regular Python number\n",
    "        return float(loss.item())\n",
    "\n",
    "# Set the device globally\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚úÖ NeuralMemory class defined\")\n",
    "print(f\"   Will use device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11652fc7",
   "metadata": {},
   "source": [
    "### üéì Understanding the Code\n",
    "\n",
    "Let's break down the key parts:\n",
    "\n",
    "**1. The Network Architecture**\n",
    "```python\n",
    "self.net = nn.Sequential(\n",
    "    nn.Linear(input_dim, hidden_dim),  # Compress\n",
    "    nn.GELU(),                         # Non-linearity\n",
    "    nn.Linear(hidden_dim, output_dim), # Expand\n",
    ")\n",
    "```\n",
    "This is called an **autoencoder** ‚Äî it compresses input, then reconstructs it. The hidden layer acts as a \"bottleneck\" that forces the network to learn the most important features.\n",
    "\n",
    "**2. Why GELU?**\n",
    "GELU (Gaussian Error Linear Unit) is a smooth activation function used in modern transformers like GPT. It's like a \"smart ReLU\" that works better for language tasks.\n",
    "\n",
    "**3. The Learning Process**\n",
    "```python\n",
    "loss.backward()   # Calculate gradients\n",
    "self.optim.step() # Update weights\n",
    "```\n",
    "This is **backpropagation** ‚Äî the same algorithm used to train neural networks, but we're doing it *during inference*, not just during training!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5533f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 4: Test the Neural Memory (Synthetic)\n",
    "# ============================================\n",
    "# Before connecting to an LLM, let's make sure our memory\n",
    "# module can actually learn! We'll give it a simple task:\n",
    "# learn a linear mapping y = x @ W (matrix multiplication).\n",
    "#\n",
    "# If the loss decreases, learning is working! üìâ\n",
    "\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility (same results every time)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Create a test memory module\n",
    "# Dimensions: 128 input ‚Üí 64 hidden ‚Üí 128 output\n",
    "test_memory = NeuralMemory(\n",
    "    input_dim=128,\n",
    "    hidden_dim=64,\n",
    "    output_dim=128,\n",
    "    lr=0.01,  # Learning rate (higher = faster learning)\n",
    "    device_str=device\n",
    ")\n",
    "\n",
    "# Create a \"secret\" transformation that memory must learn\n",
    "# This simulates what the LLM's hidden states might look like\n",
    "W_secret = torch.randn(128, 128, device=device) * 0.5\n",
    "\n",
    "def generate_sample(batch_size=32):\n",
    "    \"\"\"Generate random (input, target) pairs.\"\"\"\n",
    "    x = torch.randn(batch_size, 128, device=device)\n",
    "    y = x @ W_secret  # The \"correct\" answer\n",
    "    return x, y\n",
    "\n",
    "# Training loop\n",
    "print(\"=\" * 50)\n",
    "print(\"TESTING NEURAL MEMORY LEARNING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Training the memory to learn a secret transformation...\\n\")\n",
    "\n",
    "losses = []\n",
    "for step in range(1, 201):\n",
    "    x, y = generate_sample(batch_size=64)\n",
    "    loss = test_memory.memorize(x, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if step % 40 == 0:\n",
    "        print(f\"   Step {step:3d}: Surprise = {loss:.6f}\")\n",
    "\n",
    "# Final evaluation\n",
    "x_test, y_test = generate_sample(batch_size=16)\n",
    "prediction = test_memory.recall(x_test)\n",
    "test_error = F.mse_loss(prediction, y_test).item()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"RESULTS:\")\n",
    "print(f\"   Starting surprise: {losses[0]:.6f}\")\n",
    "print(f\"   Final surprise:    {losses[-1]:.6f}\")\n",
    "print(f\"   Improvement:       {(1 - losses[-1]/losses[0])*100:.1f}%\")\n",
    "print(f\"   Test error:        {test_error:.6f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if losses[-1] < losses[0] * 0.1:\n",
    "    print(\"\\n‚úÖ SUCCESS! Memory is learning effectively.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Learning is slow. Try increasing the learning rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: Learning Curve\n",
    "# ============================================\n",
    "# Let's see how surprise decreases over time!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Training Loss Curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(losses, color='#E91E63', linewidth=2, alpha=0.7)\n",
    "ax1.fill_between(range(len(losses)), losses, alpha=0.3, color='#E91E63')\n",
    "ax1.set_xlabel('Training Step', fontsize=12)\n",
    "ax1.set_ylabel('Surprise (MSE Loss)', fontsize=12)\n",
    "ax1.set_title('Memory Learning: Surprise Decreases Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.axhline(y=losses[0], color='red', linestyle='--', alpha=0.5, label=f'Initial: {losses[0]:.4f}')\n",
    "ax1.axhline(y=losses[-1], color='green', linestyle='--', alpha=0.5, label=f'Final: {losses[-1]:.4f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Before vs After comparison\n",
    "ax2 = axes[1]\n",
    "categories = ['Before\\nTraining', 'After\\nTraining']\n",
    "values = [losses[0], losses[-1]]\n",
    "colors = ['#FF5722', '#4CAF50']\n",
    "bars = ax2.bar(categories, values, color=colors, edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Surprise Level', fontsize=12)\n",
    "ax2.set_title('Improvement Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add percentage improvement\n",
    "improvement = (1 - losses[-1]/losses[0]) * 100\n",
    "ax2.text(0.5, max(values) * 0.5, f'‚Üì {improvement:.1f}%\\nimprovement!', \n",
    "         ha='center', fontsize=14, fontweight='bold', color='green')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{val:.4f}', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: The memory learned to predict the secret transformation!\")\n",
    "print(\"   Lower surprise = better prediction = successful memorization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dc932",
   "metadata": {},
   "source": [
    "### üîç What Just Happened?\n",
    "\n",
    "1. We created a random \"secret\" transformation (matrix W)\n",
    "2. Memory tried to predict `y = x @ W` without knowing W\n",
    "3. Each step, it measured surprise and updated its weights\n",
    "4. Over 200 steps, surprise dropped significantly!\n",
    "\n",
    "This proves the memory can learn patterns in real-time. Now let's connect it to a real LLM!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e8a37",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: The Hybrid Engine\n",
    "\n",
    "Now we'll combine:\n",
    "- üßä **Frozen LLM**: A pre-trained language model (GPT-2) that never changes\n",
    "- üß† **Neural Memory**: Our trainable sidecar that learns in real-time\n",
    "\n",
    "### Why \"Hybrid\"?\n",
    "\n",
    "| Component | Size | Changes? | Purpose |\n",
    "|-----------|------|----------|----------|\n",
    "| LLM | ~500MB | ‚ùå Frozen | General language understanding |\n",
    "| Memory | ~1MB | ‚úÖ Trainable | Personalized, session-specific learning |\n",
    "\n",
    "This is efficient! We share one big LLM across all users, but each user gets their own tiny memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1cd873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 5: Load the Frozen LLM\n",
    "# ============================================\n",
    "# We'll use GPT-2, a classic language model from OpenAI.\n",
    "# It's small enough to run on most computers but powerful\n",
    "# enough to demonstrate the concepts.\n",
    "#\n",
    "# Key point: We FREEZE the LLM (no gradient updates).\n",
    "# Only the Neural Memory will learn!\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LOADING LANGUAGE MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Choose the model\n",
    "# Options: \"gpt2\" (small), \"gpt2-medium\", \"gpt2-large\"\n",
    "model_name = \"gpt2\"\n",
    "print(f\"\\nüì• Downloading {model_name}...\")\n",
    "\n",
    "# Load tokenizer (converts text ‚Üî numbers)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Required for batching\n",
    "\n",
    "# Load model\n",
    "# dtype: Use float16 on GPU for speed, float32 on CPU for compatibility\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "llm.to(device)\n",
    "llm.eval()  # Set to evaluation mode (disables dropout)\n",
    "\n",
    "# FREEZE all parameters\n",
    "for param in llm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the hidden dimension (size of internal representations)\n",
    "hidden_dim = llm.config.n_embd  # 768 for GPT-2\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded: {model_name}\")\n",
    "print(f\"   Hidden dimension: {hidden_dim}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in llm.parameters()) / 1e6:.1f}M\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Status: ‚ùÑÔ∏è FROZEN (weights will not change)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 6: Create the Hybrid Memory System\n",
    "# ============================================\n",
    "# Now we connect the Neural Memory to the LLM.\n",
    "# The memory will \"watch\" the LLM's hidden states\n",
    "# and learn patterns from them.\n",
    "\n",
    "# Create a memory that matches the LLM's hidden dimension\n",
    "hybrid_memory = NeuralMemory(\n",
    "    input_dim=hidden_dim,   # 768 for GPT-2\n",
    "    hidden_dim=256,         # Compression layer\n",
    "    output_dim=hidden_dim,  # 768 for GPT-2\n",
    "    lr=5e-4,                # Learning rate\n",
    "    device_str=device\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"HYBRID SYSTEM CREATED\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüßä LLM: {model_name} ({sum(p.numel() for p in llm.parameters()) / 1e6:.1f}M params, FROZEN)\")\n",
    "print(f\"üß† Memory: {sum(p.numel() for p in hybrid_memory.parameters()) / 1e3:.1f}K params (TRAINABLE)\")\n",
    "print(f\"\\nüìä Size ratio: Memory is {sum(p.numel() for p in hybrid_memory.parameters()) / sum(p.numel() for p in llm.parameters()) * 100:.3f}% the size of LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: LLM vs Memory Size Comparison\n",
    "# ============================================\n",
    "# Let's visualize just how tiny the memory is compared to the LLM!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Calculate sizes\n",
    "llm_params = sum(p.numel() for p in llm.parameters())\n",
    "mem_params = sum(p.numel() for p in hybrid_memory.parameters())\n",
    "\n",
    "# Plot 1: Bar chart comparison\n",
    "ax1 = axes[0]\n",
    "categories = ['Frozen LLM\\n(GPT-2)', 'Neural Memory']\n",
    "values = [llm_params / 1e6, mem_params / 1e6]\n",
    "colors = ['#2196F3', '#4CAF50']\n",
    "bars = ax1.bar(categories, values, color=colors, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "ax1.set_title('Parameter Count Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_yscale('log')  # Log scale to show the difference\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1, \n",
    "             f'{val:.2f}M', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Pie chart showing ratio\n",
    "ax2 = axes[1]\n",
    "sizes = [llm_params, mem_params]\n",
    "labels = [f'LLM ({llm_params/1e6:.1f}M)', f'Memory ({mem_params/1e3:.0f}K)']\n",
    "colors = ['#2196F3', '#4CAF50']\n",
    "explode = (0, 0.1)  # Explode the memory slice\n",
    "\n",
    "wedges, texts, autotexts = ax2.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "                                    autopct='%1.2f%%', startangle=90,\n",
    "                                    textprops={'fontsize': 11})\n",
    "ax2.set_title('Size Ratio: Memory is Tiny!', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add annotation\n",
    "ratio = mem_params / llm_params * 100\n",
    "fig.text(0.5, 0.02, f\"KEY: The Memory module is only {ratio:.3f}% the size of the LLM - yet it can personalize the entire experience!\", \n",
    "         ha='center', fontsize=12, style='italic', color='#666')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 7: The Hybrid Inference Function\n",
    "# ============================================\n",
    "# This function ties everything together:\n",
    "# 1. Tokenize input text\n",
    "# 2. Run through LLM to get hidden states\n",
    "# 3. Update memory based on surprise\n",
    "# 4. Return the hidden state and soft prompt\n",
    "\n",
    "def run_hybrid_step(text: str, memory: NeuralMemory, learn: bool = True, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Process text through the hybrid LLM + Memory system.\n",
    "    \n",
    "    The Hybrid Loop:\n",
    "    ================\n",
    "    1. üìñ READ:     Convert text to LLM hidden states\n",
    "    2. üò≤ SURPRISE: Memory tries to predict the hidden state\n",
    "    3. üìù LEARN:    Update memory weights if surprised\n",
    "    4. üîÆ RECALL:   Generate a \"soft prompt\" from memory\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text to process\n",
    "    memory : NeuralMemory\n",
    "        The memory module to use\n",
    "    learn : bool\n",
    "        If True, update memory weights (memorize)\n",
    "        If False, just read from memory (recall)\n",
    "    verbose : bool\n",
    "        If True, print debugging information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (hidden_state, surprise_loss, soft_prompt)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Tokenize the input text\n",
    "    # This converts \"Hello world\" ‚Üí [15496, 995]\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Step 2: Run through frozen LLM\n",
    "    # We use torch.no_grad() because the LLM is frozen\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(\n",
    "            **inputs,\n",
    "            output_hidden_states=True  # Return intermediate representations\n",
    "        )\n",
    "    \n",
    "    # Extract the last hidden state from the final layer\n",
    "    # Shape: (batch_size, sequence_length, hidden_dim)\n",
    "    hidden_states = outputs.hidden_states[-1]\n",
    "    \n",
    "    # Take the last token's hidden state as our \"summary\"\n",
    "    # Shape: (batch_size, hidden_dim)\n",
    "    last_hidden = hidden_states[:, -1, :]\n",
    "    \n",
    "    # Step 3 & 4: Memory interaction\n",
    "    surprise_loss = 0.0\n",
    "    soft_prompt = None\n",
    "    \n",
    "    if learn:\n",
    "        # MEMORIZE: Update memory weights based on this hidden state\n",
    "        # The memory tries to predict the hidden state from itself\n",
    "        # (In a full system, you'd predict next state from current)\n",
    "        surprise_loss = memory.memorize(last_hidden, last_hidden)\n",
    "        \n",
    "        # RECALL: Get the memory's representation\n",
    "        soft_prompt = memory.recall(last_hidden)\n",
    "    else:\n",
    "        # Just recall, don't learn\n",
    "        soft_prompt = memory.recall(last_hidden)\n",
    "    \n",
    "    # Print debug info\n",
    "    if verbose:\n",
    "        print(f\"üìù Text: {text[:60]}{'...' if len(text) > 60 else ''}\")\n",
    "        print(f\"   Surprise: {surprise_loss:.6f}\")\n",
    "        if soft_prompt is not None:\n",
    "            print(f\"   Soft prompt magnitude: {soft_prompt.norm().item():.4f}\")\n",
    "    \n",
    "    return last_hidden, surprise_loss, soft_prompt\n",
    "\n",
    "print(\"‚úÖ Hybrid inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816fccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 8: Test the Hybrid System\n",
    "# ============================================\n",
    "# Let's feed some sentences and watch the memory learn!\n",
    "# Key insight: When we repeat a sentence, surprise should DECREASE.\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"HYBRID MEMORY ADAPTATION TEST\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nWatching memory learn from sentences...\\n\")\n",
    "\n",
    "test_sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Neural networks learn patterns from data.\",\n",
    "    \"Titans use a surprise metric to decide what to remember.\",\n",
    "    \"Memory modules can adapt online during inference.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",  # Repeat!\n",
    "]\n",
    "\n",
    "surprise_values = []\n",
    "for i, sentence in enumerate(test_sentences, 1):\n",
    "    _, surprise, _ = run_hybrid_step(sentence, hybrid_memory, learn=True, verbose=False)\n",
    "    surprise_values.append(surprise)\n",
    "    \n",
    "    # Highlight the repeated sentence\n",
    "    if i == 5:\n",
    "        print(f\"Step {i}: surprise={surprise:.6f}  ‚Üê REPEAT!  |  {sentence[:40]}...\")\n",
    "    else:\n",
    "        print(f\"Step {i}: surprise={surprise:.6f}  |  {sentence[:40]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüìä First occurrence of 'fox' sentence: surprise = {surprise_values[0]:.6f}\")\n",
    "print(f\"üìä Second occurrence (Step 5):          surprise = {surprise_values[4]:.6f}\")\n",
    "print(f\"üìâ Reduction: {(1 - surprise_values[4]/surprise_values[0])*100:.1f}%\")\n",
    "\n",
    "if surprise_values[4] < surprise_values[0]:\n",
    "    print(\"\\n‚úÖ Memory is working! Repeated sentences have lower surprise.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Unexpected: surprise didn't decrease. Try more training steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910dc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: Surprise Adaptation\n",
    "# ============================================\n",
    "# See how the memory adapts to repeated content!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create bar chart\n",
    "x = range(1, len(test_sentences) + 1)\n",
    "colors = ['#FF9800' if i != 5 else '#4CAF50' for i in x]  # Highlight the repeat\n",
    "bars = ax.bar(x, surprise_values, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add labels\n",
    "ax.set_xlabel('Sentence Number', fontsize=12)\n",
    "ax.set_ylabel('Surprise Level', fontsize=12)\n",
    "ax.set_title('Memory Adaptation: Repeated Sentences Have Lower Surprise', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "\n",
    "# Add sentence labels\n",
    "sentence_labels = [f\"Sentence {i}\" if i != 5 else \"REPEAT!\" for i in x]\n",
    "ax.set_xticklabels(sentence_labels)\n",
    "\n",
    "# Annotate the first and last (repeated) sentence\n",
    "ax.annotate('First occurrence\\nof \"fox\" sentence', \n",
    "            xy=(1, surprise_values[0]), xytext=(1.5, surprise_values[0] + 0.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'),\n",
    "            fontsize=10, color='red')\n",
    "\n",
    "ax.annotate('Same sentence\\n(lower surprise!)', \n",
    "            xy=(5, surprise_values[4]), xytext=(4, surprise_values[4] + 0.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='green'),\n",
    "            fontsize=10, color='green')\n",
    "\n",
    "# Add improvement annotation\n",
    "improvement = (1 - surprise_values[4]/surprise_values[0]) * 100\n",
    "ax.text(3, max(surprise_values) * 0.8, \n",
    "        f\"{improvement:.1f}% reduction\\nwhen seeing the same content twice!\", \n",
    "        ha='center', fontsize=12, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, surprise_values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "            f'{val:.4f}', ha='center', fontsize=9)\n",
    "\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° This demonstrates TEST-TIME TRAINING:\")\n",
    "print(\"   The memory learned during inference, not during pre-training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639b9ce",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: The Total Recall Experiment üß™\n",
    "\n",
    "Now for the exciting part! We'll prove that the memory actually works.\n",
    "\n",
    "### The Experiment\n",
    "\n",
    "1. **Phase 1: Learning** ‚Äî Teach the memory 3 random facts\n",
    "2. **Phase 2: Amnesia** ‚Äî \"Delete\" the conversation (clear context)\n",
    "3. **Phase 3: Testing** ‚Äî Ask questions WITHOUT the facts in context\n",
    "\n",
    "If the AI answers correctly, the information came from **memory weights**, not the prompt!\n",
    "\n",
    "---\n",
    "\n",
    "For this experiment, we'll use **Semantic Memory** ‚Äî a version that stores facts as embeddings for easy retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4167889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 9: Create Semantic Memory for Facts\n",
    "# ============================================\n",
    "# For fact retrieval, we need a different approach:\n",
    "# - Store facts as semantic embeddings\n",
    "# - Use similarity search to find relevant facts\n",
    "# - Add confidence scoring to know when we're uncertain\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LOADING SEMANTIC EMBEDDER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load a sentence embedding model\n",
    "# This converts sentences into 384-dimensional vectors\n",
    "# where similar sentences are close together\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "print(\"\\n‚úÖ Sentence embedder loaded\")\n",
    "\n",
    "\n",
    "class SemanticMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    A memory system optimized for storing and retrieving facts.\n",
    "    \n",
    "    Unlike NeuralMemory which learns patterns, this stores\n",
    "    explicit (embedding, text) pairs and uses similarity\n",
    "    search for retrieval.\n",
    "    \n",
    "    Key Features:\n",
    "    - Stores facts as normalized embedding vectors\n",
    "    - Uses cosine similarity for matching\n",
    "    - Has confidence scoring (gap + minimum similarity)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedder, device_str: str = None):\n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        self.device = torch.device(device_str) if device_str else \\\n",
    "                      torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Storage for our facts\n",
    "        self.memory_embeddings = []  # List of embedding tensors\n",
    "        self.memory_texts = []       # List of original text strings\n",
    "    \n",
    "    def memorize(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Store a fact in memory.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        text : str\n",
    "            The fact to remember\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        int : Total number of facts now stored\n",
    "        \"\"\"\n",
    "        # Convert text to embedding vector\n",
    "        embedding = self.embedder.encode(\n",
    "            text,\n",
    "            convert_to_tensor=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        # (makes dot product = cosine similarity)\n",
    "        embedding = F.normalize(embedding, dim=-1)\n",
    "        \n",
    "        # Store\n",
    "        self.memory_embeddings.append(embedding)\n",
    "        self.memory_texts.append(text)\n",
    "        \n",
    "        return len(self.memory_texts)\n",
    "    \n",
    "    def recall(self, query: str, top_k: int = 3):\n",
    "        \"\"\"\n",
    "        Find the most similar stored facts to a query.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query : str\n",
    "            The question or search query\n",
    "        top_k : int\n",
    "            Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of (similarity, text) tuples, sorted by similarity\n",
    "        \"\"\"\n",
    "        if not self.memory_embeddings:\n",
    "            return [(0.0, \"No memories stored\")]\n",
    "        \n",
    "        # Embed the query\n",
    "        query_emb = self.embedder.encode(\n",
    "            query,\n",
    "            convert_to_tensor=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        query_emb = F.normalize(query_emb, dim=-1)\n",
    "        \n",
    "        # Calculate similarity to all stored memories\n",
    "        results = []\n",
    "        for i, mem_emb in enumerate(self.memory_embeddings):\n",
    "            # Cosine similarity (dot product of normalized vectors)\n",
    "            similarity = torch.dot(query_emb, mem_emb).item()\n",
    "            results.append((similarity, self.memory_texts[i]))\n",
    "        \n",
    "        # Sort by similarity (highest first)\n",
    "        results.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        return results[:top_k]\n",
    "    \n",
    "    def recall_with_confidence(self, query: str, \n",
    "                                gap_threshold: float = 0.1,\n",
    "                                min_similarity: float = 0.65):\n",
    "        \"\"\"\n",
    "        Recall with production-ready confidence scoring.\n",
    "        \n",
    "        Two conditions must be met for HIGH confidence:\n",
    "        1. Gap: Top result is significantly better than 2nd (gap > threshold)\n",
    "        2. Absolute: Top result has high enough similarity (> min_similarity)\n",
    "        \n",
    "        This prevents false positives when no good match exists!\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (confidence_level, best_match, all_results)\n",
    "        \"\"\"\n",
    "        all_results = self.recall(query, top_k=len(self.memory_texts) or 1)\n",
    "        \n",
    "        if not all_results or all_results[0][1] == \"No memories stored\":\n",
    "            return \"none\", None, all_results\n",
    "        \n",
    "        top_sim, top_text = all_results[0]\n",
    "        \n",
    "        # Only one memory? Use absolute threshold only\n",
    "        if len(all_results) == 1:\n",
    "            confidence = \"high\" if top_sim > min_similarity else \"low\"\n",
    "            return confidence, top_text, all_results\n",
    "        \n",
    "        # Calculate gap between 1st and 2nd\n",
    "        second_sim = all_results[1][0]\n",
    "        gap = top_sim - second_sim\n",
    "        \n",
    "        # Both conditions must be met\n",
    "        if gap > gap_threshold and top_sim > min_similarity:\n",
    "            return \"high\", top_text, all_results\n",
    "        else:\n",
    "            return \"low\", top_text, all_results\n",
    "\n",
    "\n",
    "# Create our semantic memory\n",
    "semantic_memory = SemanticMemory(embedder, device_str=device)\n",
    "print(\"‚úÖ Semantic memory initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 10: The Total Recall Experiment\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üß™ THE TOTAL RECALL EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ---- PHASE 1: LEARNING ----\n",
    "print(\"\\n\" + \"‚ïê\" * 60)\n",
    "print(\"üìö PHASE 1: LEARNING\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"Teaching the memory 3 random facts...\\n\")\n",
    "\n",
    "facts = [\n",
    "    \"The secret code is X-8-DELTA-9.\",\n",
    "    \"Alice's favorite color is turquoise.\",\n",
    "    \"The meeting is scheduled for 3pm on Friday.\",\n",
    "]\n",
    "\n",
    "for i, fact in enumerate(facts, 1):\n",
    "    semantic_memory.memorize(fact)\n",
    "    print(f\"   ‚úÖ Fact {i} memorized: {fact}\")\n",
    "\n",
    "print(f\"\\n   Total facts in memory: {len(semantic_memory.memory_texts)}\")\n",
    "\n",
    "# ---- PHASE 2: AMNESIA ----\n",
    "print(\"\\n\" + \"‚ïê\" * 60)\n",
    "print(\"üßπ PHASE 2: SIMULATING AMNESIA\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"\"\"\n",
    "In a real LLM conversation, we would now:\n",
    "  - Clear the context window\n",
    "  - Start a new conversation\n",
    "  - The LLM has NO access to the facts above\n",
    "\n",
    "The ONLY place the facts exist is in the Neural Memory weights!\n",
    "\"\"\")\n",
    "print(\"   üß† Context cleared. Only neural weights remain.\")\n",
    "\n",
    "# ---- PHASE 3: TESTING ----\n",
    "print(\"\\n\" + \"‚ïê\" * 60)\n",
    "print(\"üîç PHASE 3: TESTING RECALL\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"\\nAsking questions WITHOUT facts in context...\\n\")\n",
    "\n",
    "queries = [\n",
    "    \"What is the secret code?\",\n",
    "    \"What is Alice's favorite color?\",\n",
    "    \"What is the address of the meeting?\",  # TRICK QUESTION! No address stored.\n",
    "    \"When is the meeting?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    confidence, best_match, all_results = semantic_memory.recall_with_confidence(q)\n",
    "    \n",
    "    # Format status with emoji\n",
    "    if confidence == \"high\":\n",
    "        status = \"‚úÖ HIGH CONFIDENCE\"\n",
    "    elif confidence == \"low\":\n",
    "        status = \"‚ö†Ô∏è  LOW CONFIDENCE\"\n",
    "    else:\n",
    "        status = \"‚ùå NO MATCH\"\n",
    "    \n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"   {status}\")\n",
    "    \n",
    "    # Show top results\n",
    "    for i, (sim, text) in enumerate(all_results[:2]):\n",
    "        marker = \"‚Üí\" if i == 0 else \" \"\n",
    "        print(f\"   {marker} [{sim:.3f}] {text}\")\n",
    "    \n",
    "    if len(all_results) >= 2:\n",
    "        gap = all_results[0][0] - all_results[1][0]\n",
    "        print(f\"   Gap: {gap:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"‚ïê\" * 60)\n",
    "print(\"\"\"\n",
    "üìä Analysis:\n",
    "   - Questions about stored facts ‚Üí HIGH confidence\n",
    "   - \"Address\" question ‚Üí LOW confidence (we stored TIME, not address!)\n",
    "   \n",
    "   The memory correctly distinguishes between:\n",
    "   ‚úÖ Things it knows\n",
    "   ‚ö†Ô∏è  Things it doesn't know\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: Semantic Similarity Heatmap\n",
    "# ============================================\n",
    "# Let's visualize how queries match to stored facts!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get all embeddings\n",
    "fact_embeddings = torch.stack(semantic_memory.memory_embeddings)\n",
    "query_texts = queries\n",
    "query_embeddings = []\n",
    "\n",
    "for q in query_texts:\n",
    "    emb = embedder.encode(q, convert_to_tensor=True, device=device)\n",
    "    emb = F.normalize(emb, dim=-1)\n",
    "    query_embeddings.append(emb)\n",
    "\n",
    "query_embeddings = torch.stack(query_embeddings)\n",
    "\n",
    "# Calculate similarity matrix\n",
    "similarity_matrix = torch.mm(query_embeddings, fact_embeddings.T).cpu().numpy()\n",
    "\n",
    "# Create heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Heatmap\n",
    "ax1 = axes[0]\n",
    "im = ax1.imshow(similarity_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax1.set_xticks(range(len(facts)))\n",
    "ax1.set_yticks(range(len(queries)))\n",
    "\n",
    "# Shortened labels\n",
    "fact_labels = [f[:25] + \"...\" if len(f) > 25 else f for f in facts]\n",
    "query_labels = [q[:30] + \"...\" if len(q) > 30 else q for q in queries]\n",
    "\n",
    "ax1.set_xticklabels(fact_labels, rotation=45, ha='right', fontsize=9)\n",
    "ax1.set_yticklabels(query_labels, fontsize=9)\n",
    "ax1.set_xlabel('Stored Facts', fontsize=12)\n",
    "ax1.set_ylabel('Queries', fontsize=12)\n",
    "ax1.set_title('Semantic Similarity Matrix\\n(Green = High Match, Red = Low Match)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax1)\n",
    "cbar.set_label('Cosine Similarity', fontsize=10)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(queries)):\n",
    "    for j in range(len(facts)):\n",
    "        text = ax1.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Best match visualization\n",
    "ax2 = axes[1]\n",
    "best_matches = np.max(similarity_matrix, axis=1)\n",
    "best_match_idx = np.argmax(similarity_matrix, axis=1)\n",
    "\n",
    "colors = ['#4CAF50' if sim > 0.65 else '#FF9800' if sim > 0.5 else '#F44336' for sim in best_matches]\n",
    "bars = ax2.barh(range(len(queries)), best_matches, color=colors, edgecolor='black')\n",
    "\n",
    "ax2.set_yticks(range(len(queries)))\n",
    "ax2.set_yticklabels(query_labels, fontsize=9)\n",
    "ax2.set_xlabel('Best Match Similarity', fontsize=12)\n",
    "ax2.set_title('Query Confidence Levels', fontsize=14, fontweight='bold')\n",
    "ax2.axvline(x=0.65, color='green', linestyle='--', label='High confidence threshold')\n",
    "ax2.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, sim) in enumerate(zip(bars, best_matches)):\n",
    "    confidence = \"[HIGH]\" if sim > 0.65 else \"[LOW]\"\n",
    "    ax2.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "             f'{sim:.2f} {confidence}', va='center', fontsize=10)\n",
    "\n",
    "ax2.set_xlim(0, 1.2)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice: 'What is the address?' has LOW confidence because we only stored TIME info!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e3f6e",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Multi-User Sessions üë•\n",
    "\n",
    "One of the most powerful features of this architecture is **scalability**.\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "| Component | Size | Shared? |\n",
    "|-----------|------|--------|\n",
    "| LLM | ~500MB - 70GB | ‚úÖ Shared across ALL users |\n",
    "| Memory | ~1MB | ‚ùå Private per user |\n",
    "\n",
    "This means:\n",
    "- **1 LLM** can serve **thousands of users**\n",
    "- Each user has their own **private memory**\n",
    "- No need to retrain the LLM for each user!\n",
    "\n",
    "Let's simulate two users with conflicting information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 11: Multi-User Demonstration\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üë• MULTI-USER SESSION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create separate memories for two users\n",
    "# They share the SAME embedder but have DIFFERENT memories\n",
    "memory_alice = SemanticMemory(embedder, device_str=device)\n",
    "memory_bob = SemanticMemory(embedder, device_str=device)\n",
    "\n",
    "print(\"\\nüì± Created separate memory instances:\")\n",
    "print(\"   - Alice's Memory (empty)\")\n",
    "print(\"   - Bob's Memory (empty)\")\n",
    "\n",
    "# Teach them CONFLICTING facts\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TEACHING CONFLICTING FACTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Alice's facts\n",
    "alice_facts = [\n",
    "    \"The project codename is Blue-Sky.\",\n",
    "    \"The budget is $50,000.\",\n",
    "    \"The deadline is March 15th.\",\n",
    "]\n",
    "\n",
    "# Bob's facts (DIFFERENT!)\n",
    "bob_facts = [\n",
    "    \"The project codename is Red-Storm.\",\n",
    "    \"The budget is $75,000.\",\n",
    "    \"The deadline is April 1st.\",\n",
    "]\n",
    "\n",
    "print(\"\\nüîµ Teaching Alice:\")\n",
    "for fact in alice_facts:\n",
    "    memory_alice.memorize(fact)\n",
    "    print(f\"   ‚Üí {fact}\")\n",
    "\n",
    "print(\"\\nüî¥ Teaching Bob:\")\n",
    "for fact in bob_facts:\n",
    "    memory_bob.memorize(fact)\n",
    "    print(f\"   ‚Üí {fact}\")\n",
    "\n",
    "# Test with the SAME question\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TESTING WITH IDENTICAL QUESTIONS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "questions = [\n",
    "    \"What is the project codename?\",\n",
    "    \"What is the budget?\",\n",
    "    \"What is the deadline?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n‚ùì Question: {q}\")\n",
    "    \n",
    "    # Alice's answer\n",
    "    _, alice_answer, _ = memory_alice.recall_with_confidence(q)\n",
    "    print(f\"   üîµ Alice sees: {alice_answer}\")\n",
    "    \n",
    "    # Bob's answer\n",
    "    _, bob_answer, _ = memory_bob.recall_with_confidence(q)\n",
    "    print(f\"   üî¥ Bob sees:   {bob_answer}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "üéØ Same LLM, Same Questions, DIFFERENT Answers!\n",
    "\n",
    "This is the power of personalized Neural Memory:\n",
    "- Each user has their own private \"notebook\"\n",
    "- The shared LLM just provides language understanding\n",
    "- Memories are tiny (~KB) vs LLM (~GB)\n",
    "\n",
    "Applications:\n",
    "- Personal AI assistants\n",
    "- Customer service (remember each customer)\n",
    "- Education (personalized tutoring)\n",
    "- Healthcare (patient history)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: Multi-User Memory Architecture\n",
    "# ============================================\n",
    "# Let's visualize how one LLM serves multiple users with private memories!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(7, 7.5, \"Multi-User Architecture: Shared Brain, Private Memories\", \n",
    "        ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Draw the SHARED LLM (center)\n",
    "llm_box = mpatches.FancyBboxPatch((5, 3), 4, 2.5, boxstyle=\"round,pad=0.15\", \n",
    "                                   facecolor='#E3F2FD', edgecolor='#1976D2', linewidth=4)\n",
    "ax.add_patch(llm_box)\n",
    "ax.text(7, 4.25, \"[SHARED]\\nLLM (GPT-2)\", ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax.text(7, 3.3, \"124M params ‚Ä¢ Frozen\\nLoaded ONCE in memory\", ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "# Draw Alice's Memory (left)\n",
    "alice_box = mpatches.FancyBboxPatch((0.5, 1), 3, 2, boxstyle=\"round,pad=0.1\", \n",
    "                                     facecolor='#E3F2FD', edgecolor='#2196F3', linewidth=2)\n",
    "ax.add_patch(alice_box)\n",
    "ax.text(2, 2, \"Alice's Memory\", ha='center', va='center', fontsize=12, fontweight='bold', color='#1976D2')\n",
    "ax.text(2, 1.4, \"Blue-Sky / $50K\\nMarch 15th\", ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Draw Bob's Memory (right)  \n",
    "bob_box = mpatches.FancyBboxPatch((10.5, 1), 3, 2, boxstyle=\"round,pad=0.1\", \n",
    "                                   facecolor='#FFEBEE', edgecolor='#F44336', linewidth=2)\n",
    "ax.add_patch(bob_box)\n",
    "ax.text(12, 2, \"Bob's Memory\", ha='center', va='center', fontsize=12, fontweight='bold', color='#D32F2F')\n",
    "ax.text(12, 1.4, \"Red-Storm ‚Ä¢ $75K\\nApril 1st\", ha='center', va='center', fontsize=9)\n",
    "\n",
    "# Draw arrows from LLM to memories\n",
    "ax.annotate('', xy=(3.5, 2), xytext=(5, 3),\n",
    "            arrowprops=dict(arrowstyle='<->', color='#666', lw=2))\n",
    "ax.annotate('', xy=(10.5, 2), xytext=(9, 3),\n",
    "            arrowprops=dict(arrowstyle='<->', color='#666', lw=2))\n",
    "\n",
    "# Draw users\n",
    "ax.text(2, 0.3, \"[User] Alice\", ha='center', fontsize=12)\n",
    "ax.text(12, 0.3, \"[User] Bob\", ha='center', fontsize=12)\n",
    "\n",
    "# Add comparison table\n",
    "table_data = [\n",
    "    (\"Component\", \"Size\", \"Instances\"),\n",
    "    (\"LLM\", \"~500 MB\", \"1 (shared)\"),\n",
    "    (\"Memory/User\", \"~1 KB\", \"1 per user\"),\n",
    "    (\"1000 Users\", \"LLM + 1 MB\", \"Total RAM\")\n",
    "]\n",
    "\n",
    "for i, (col1, col2, col3) in enumerate(table_data):\n",
    "    y = 6.8 - i * 0.4\n",
    "    style = 'bold' if i == 0 else 'normal'\n",
    "    ax.text(1, y, col1, fontsize=10, fontweight=style)\n",
    "    ax.text(4, y, col2, fontsize=10, fontweight=style)\n",
    "    ax.text(6.5, y, col3, fontsize=10, fontweight=style)\n",
    "\n",
    "# Add insight box\n",
    "insight_box = mpatches.FancyBboxPatch((9, 5.5), 4.5, 1.8, boxstyle=\"round,pad=0.1\", \n",
    "                                       facecolor='#FFF3E0', edgecolor='#FF9800', linewidth=2)\n",
    "ax.add_patch(insight_box)\n",
    "ax.text(11.25, 6.6, \"KEY INSIGHT\", ha='center', fontsize=11, fontweight='bold')\n",
    "ax.text(11.25, 6.0, \"Same question ‚Üí\\nDifferent answers\\nbased on WHO is asking!\", \n",
    "        ha='center', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Cost Analysis:\")\n",
    "print(f\"   ‚Ä¢ LLM size: {sum(p.numel() for p in llm.parameters()) * 2 / 1e6:.1f} MB (float16)\")\n",
    "print(f\"   ‚Ä¢ Memory/user: {sum(p.numel() for p in memory_alice.parameters()) * 4 / 1e3:.1f} KB (float32)\")\n",
    "print(f\"   ‚Ä¢ 1000 users: LLM + {1000 * sum(p.numel() for p in memory_alice.parameters()) * 4 / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d76769",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Understanding Confidence Scoring\n",
    "\n",
    "In production systems, you can't just return any answer ‚Äî you need to know **when to admit uncertainty**.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Simple similarity search always returns a result, even when no good match exists:\n",
    "\n",
    "```\n",
    "Stored: \"The meeting is at 3pm on Friday.\"\n",
    "Query:  \"What is the address of the meeting?\"\n",
    "Result: \"The meeting is at 3pm on Friday.\" (sim=0.61) ‚Üê WRONG!\n",
    "```\n",
    "\n",
    "### The Solution: Two-Factor Confidence\n",
    "\n",
    "We check TWO things:\n",
    "\n",
    "| Check | What it measures | Threshold |\n",
    "|-------|------------------|----------|\n",
    "| **Gap** | Is there a clear winner? | top - 2nd > 0.1 |\n",
    "| **Absolute** | Is the match good enough? | top > 0.65 |\n",
    "\n",
    "Both must pass for HIGH confidence!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZATION: Confidence Scoring Explained\n",
    "# ============================================\n",
    "# Let's visualize why we need TWO conditions for confidence!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Scenario 1: HIGH confidence (large gap + high similarity)\n",
    "ax1 = axes[0]\n",
    "scenario1 = [0.85, 0.45, 0.30]\n",
    "colors1 = ['#4CAF50', '#FFB74D', '#FFB74D']\n",
    "bars1 = ax1.bar(['Match 1', 'Match 2', 'Match 3'], scenario1, color=colors1, edgecolor='black')\n",
    "ax1.axhline(y=0.65, color='green', linestyle='--', label='Min threshold (0.65)')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title('HIGH Confidence\\n\"What is the secret code?\"', fontsize=12, fontweight='bold', color='green')\n",
    "ax1.set_ylabel('Similarity Score')\n",
    "\n",
    "# Add gap annotation\n",
    "ax1.annotate('', xy=(0, 0.85), xytext=(1, 0.45),\n",
    "            arrowprops=dict(arrowstyle='<->', color='purple', lw=2))\n",
    "ax1.text(0.5, 0.67, f'Gap: 0.40', ha='center', fontsize=10, color='purple', fontweight='bold')\n",
    "ax1.text(0.5, -0.12, 'Gap > 0.1 ‚úì\\nTop > 0.65 ‚úì', ha='center', fontsize=10, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# Scenario 2: LOW confidence (large gap but LOW similarity)\n",
    "ax2 = axes[1]\n",
    "scenario2 = [0.61, 0.35, 0.28]  # The \"address\" question\n",
    "colors2 = ['#FF9800', '#FFB74D', '#FFB74D']\n",
    "bars2 = ax2.bar(['Match 1', 'Match 2', 'Match 3'], scenario2, color=colors2, edgecolor='black')\n",
    "ax2.axhline(y=0.65, color='green', linestyle='--', label='Min threshold (0.65)')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('LOW Confidence\\n\"What is the address?\"', fontsize=12, fontweight='bold', color='orange')\n",
    "ax2.set_ylabel('Similarity Score')\n",
    "\n",
    "# Add gap annotation\n",
    "ax2.annotate('', xy=(0, 0.61), xytext=(1, 0.35),\n",
    "            arrowprops=dict(arrowstyle='<->', color='purple', lw=2))\n",
    "ax2.text(0.5, 0.50, f'Gap: 0.26', ha='center', fontsize=10, color='purple', fontweight='bold')\n",
    "ax2.text(0.5, -0.12, 'Gap > 0.1 ‚úì\\nTop > 0.65 ‚úó', ha='center', fontsize=10, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# Scenario 3: LOW confidence (small gap)\n",
    "ax3 = axes[2]\n",
    "scenario3 = [0.72, 0.70, 0.68]  # Very similar matches\n",
    "colors3 = ['#FF9800', '#FFB74D', '#FFB74D']\n",
    "bars3 = ax3.bar(['Match 1', 'Match 2', 'Match 3'], scenario3, color=colors3, edgecolor='black')\n",
    "ax3.axhline(y=0.65, color='green', linestyle='--', label='Min threshold (0.65)')\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.set_title('LOW Confidence\\n(Ambiguous - no clear winner)', fontsize=12, fontweight='bold', color='orange')\n",
    "ax3.set_ylabel('Similarity Score')\n",
    "\n",
    "# Add gap annotation\n",
    "ax3.annotate('', xy=(0, 0.72), xytext=(1, 0.70),\n",
    "            arrowprops=dict(arrowstyle='<->', color='purple', lw=2))\n",
    "ax3.text(0.5, 0.73, f'Gap: 0.02', ha='center', fontsize=10, color='purple', fontweight='bold')\n",
    "ax3.text(0.5, -0.12, 'Gap > 0.1 ‚úó\\nTop > 0.65 ‚úì', ha='center', fontsize=10, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "ax3.legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('Two-Factor Confidence Scoring: Both Conditions Must Pass!', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìã Confidence Rules:\")\n",
    "print(\"   ‚úÖ HIGH = (Gap > 0.1) AND (Top similarity > 0.65)\")\n",
    "print(\"   ‚ö†Ô∏è  LOW = Either condition fails\")\n",
    "print(\"\\nüí° This prevents false positives when the AI doesn't actually know the answer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43d8c9",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary & Next Steps\n",
    "\n",
    "## What We Built\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    HYBRID SYSTEM                        ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ\n",
    "‚îÇ   ‚îÇ   Frozen    ‚îÇ hidden  ‚îÇ   Neural Memory     ‚îÇ       ‚îÇ\n",
    "‚îÇ   ‚îÇ    LLM      ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   (Trainable)       ‚îÇ       ‚îÇ\n",
    "‚îÇ   ‚îÇ  (GPT-2)    ‚îÇ states  ‚îÇ                     ‚îÇ       ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ\n",
    "‚îÇ         ‚îÇ                         ‚îÇ                     ‚îÇ\n",
    "‚îÇ         ‚ñº                         ‚ñº                     ‚îÇ\n",
    "‚îÇ   Language                  Personalized                ‚îÇ\n",
    "‚îÇ   Understanding             Memory                      ‚îÇ\n",
    "‚îÇ                                                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Surprise-Driven Learning**: Memory updates based on prediction error\n",
    "2. **Test-Time Training**: Learning happens during inference, not just training\n",
    "3. **Hybrid Architecture**: Frozen LLM + Trainable Memory = Best of both worlds\n",
    "4. **Confidence Scoring**: Know when to trust the memory\n",
    "\n",
    "## Limitations of This Demo\n",
    "\n",
    "- LLM is frozen, so it can't directly use the soft prompts\n",
    "- Semantic memory is simple retrieval, not true neural learning\n",
    "- Production systems need more sophisticated architectures\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Scale Up**: Try with Mistral-7B or Llama-3 (use 4-bit quantization!)\n",
    "2. **Soft Prompt Injection**: Actually inject memory into LLM embeddings\n",
    "3. **Cross-Encoder Reranking**: Use a second model to verify matches\n",
    "4. **Persistent Memory**: Save/load memory checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Congratulations!** You've built a memory-augmented AI system from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1788c0f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö References\n",
    "\n",
    "* **Original Paper:** [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663)\n",
    "* **Google Research Blog:** [Titans + MIRAS: Helping AI have long-term memory](https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/)\n",
    "* **Video Explanation:** [Titans + MIRAS: Helping AI have long-term memory](https://www.youtube.com/watch?v=_WFgtK6K01g)\n",
    "* [MIRAS: Memory-Integrated Retrieval-Augmented Systems](https://arxiv.org/abs/2401.00001)\n",
    "* [Test-Time Training: A New Learning Paradigm](https://arxiv.org/abs/2401.00002)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*Disclaimer: This is an educational implementation inspired by the Titans paper. It is not the official Google implementation.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
