{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d4d4f0",
   "metadata": {},
   "source": [
    "# ðŸ§  Titans + MIRAS: Building AI with Long-Term Memory\n",
    "## A BEGINNER'S GUIDE TO NEXT-GEN AI ARCHITECTURE\n",
    "\n",
    "Welcome, student! ðŸŽ“\n",
    "\n",
    "Today, we are going to embark on an exciting journey into the cutting edge of Artificial Intelligence. We will learn about **Titans** and **MIRAS**, two revolutionary concepts from Google Research that help AI \"remember\" things for a very long time.\n",
    "\n",
    "This isn't just a toy demo. We are going to build the **entire lifecycle** of a Long-Term Memory AI:\n",
    "1.  **Data**: Downloading and preparing a real text corpus (Shakespeare).\n",
    "2.  **Architecture**: Building the Titans Neural Memory from scratch.\n",
    "3.  **Training**: A custom loop using the **MIRAS** surprise metric to teach the AI.\n",
    "4.  **Inference**: A chat interface where the AI remembers context over time.\n",
    "\n",
    "This notebook runs on **NVIDIA DGX Spark** hardware, utilizing mixed precision (BFloat16) for maximum performance.\n",
    "\n",
    "### ðŸŽ¯ The Lifecycle\n",
    "1.  **Setup**: Hardware check.\n",
    "2.  **Data Pipeline**: Ingesting and tokenizing text.\n",
    "3.  **The Model**: Constructing the Titans Architecture.\n",
    "4.  **Education (Training)**: The curriculum learning phase.\n",
    "5.  **Evaluation**: Visualizing how the brain learns.\n",
    "6.  **Deployment**: Chatting with our creation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ–¥ï¸ 1. Hardware Setup: Unleashing the Beast\n",
    "\n",
    "We are running on the **NVIDIA DGX Spark**. This machine is a beast!\n",
    "-   **CPU**: Multi-core ARM64 (Efficient and powerful)\n",
    "-   **GPU**: NVIDIA GB10 (The engine that powers our AI)\n",
    "-   **Memory**: 128 GB Unified Shared Memory (Massive space for our AI to think)\n",
    "-   **OS**: Ubuntu 24.04 with CUDA 13.0\n",
    "\n",
    "Let's verify our environment and set up our tools. We will use **PyTorch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import subprocess\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress CUDA capability warnings (e.g., for newer GPUs)\n",
    "warnings.filterwarnings(\"ignore\", message=\".*cuda capability.*\", category=UserWarning)\n",
    "\n",
    "print(\"ðŸ” Checking Hardware Environment...\")\n",
    "\n",
    "# 1. Check CPU Architecture\n",
    "cpu_arch = platform.machine()\n",
    "print(f\"âœ… CPU Architecture: {cpu_arch} (Expecting aarch64 for ARM64)\")\n",
    "\n",
    "# 2. Check CUDA (GPU Support)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… CUDA Available: Yes\")\n",
    "    print(f\"âœ… CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"âœ… GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"âœ… GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    # Using BFloat16 for GB10 chips (High range, decent precision)\n",
    "    dtype = torch.bfloat16 \n",
    "    print(f\"ðŸš€ Optimization: Using {dtype} for next-gen performance.\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: CUDA not detected. Running on CPU (Slower).\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "# 3. Create Artifacts Directory\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "print(\"ðŸ“‚ Artifacts directory created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53ae63",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š 2. Data Pipeline: From Shakespeare to Tensors\n",
    "\n",
    "To teach an AI, we need a curriculum. We will use the standard **Tiny Shakespeare** dataset. It's complex enough to require memory (characters, plot points) but small enough to train quickly for this tutorial.\n",
    "\n",
    "### Step 1: Download & Visualization\n",
    "We will fetch the data from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "file_path = 'artifacts/input.txt'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"â¬‡ï¸ Downloading Shakespeare dataset...\")\n",
    "    data = requests.get(data_url).text\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(data)\n",
    "else:\n",
    "    print(\"âœ… Dataset found locally.\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "print(f\"ðŸ“ Dataset Length: {len(data)} characters\")\n",
    "print(f\"ðŸ“– Sample:\\n{data[:200]}...\")\n",
    "\n",
    "# Character Inspector\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"ðŸ”¤ Unique Characters (Vocabulary Size): {vocab_size}\")\n",
    "print(f\"Dictionary: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb21094",
   "metadata": {},
   "source": [
    "### Step 2: Tokenization\n",
    "We need to convert these characters `['a', 'b', 'c']` into integers `[1, 2, 3]` that the GPU can process. This is called **Tokenization**.\n",
    "\n",
    "Since we are beginners, we use a **Character-Level Tokenizer**.\n",
    "-   **Encoder**: \"Hello\" -> `[20, 43, 56, 56, 59]`\n",
    "-   **Decoder**: `[20, 43, 56, 56, 59]` -> \"Hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d968a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "def encode(s):\n",
    "    # Robust encoding: simply skip characters that aren't in Shakespeare's vocabulary\n",
    "    # This prevents the '+' error and handles user input safely later.\n",
    "    return [stoi[c] for c in s if c in stoi]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Test it\n",
    "test_str = \"Titans MIRAS\" # Removed '+' just to be clean\n",
    "encoded = encode(test_str)\n",
    "decoded = decode(encoded)\n",
    "print(f\"Original: {test_str}\")\n",
    "print(f\"Encoded:  {encoded}\")\n",
    "print(f\"Decoded:  {decoded}\")\n",
    "\n",
    "# Prepare Data Tensors\n",
    "# We use torch.long (int64) for indices.\n",
    "data_tensor = torch.tensor(encode(data), dtype=torch.long)\n",
    "\n",
    "# Train/Val Split (90% training, 10% validation)\n",
    "n = int(0.9 * len(data_tensor))\n",
    "train_data = data_tensor[:n]\n",
    "val_data = data_tensor[n:]\n",
    "\n",
    "# Move data to GPU shared memory if possible (for speed)\n",
    "# Note: For massive datasets, we load on demand. For 1MB Shakespeare, we can just load it.\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "\n",
    "print(f\"ðŸ‹ï¸ Training Set: {len(train_data)} tokens\")\n",
    "print(f\"ðŸ§ª Validation Set: {len(val_data)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e72a9c",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ§  3. The Titans Architecture\n",
    "\n",
    "This is where magic happens. A standard Transformer has \"Attention\". Titans adds a **Neural Memory Module**.\n",
    "\n",
    "### The Components\n",
    "1.  **Embeddings**: Converts ID `42` to a vector.\n",
    "2.  **Short-Term Context (Attention)**: Looks at the last ~64 tokens (the \"Working Memory\").\n",
    "3.  **Long-Term Memory (Neural Module)**: An MLP that learns to compress *thousands* of past tokens into its weights.\n",
    "4.  **MIRAS Gating**: Decides how much to trust the Long-Term vs Short-Term memory.\n",
    "\n",
    "We will build the **Neural Memory** first, then the full block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be536c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    The 'Hippocampus' of our AI. \n",
    "    It is an MLP that learns to map (Key -> Value) pairs from the past.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, memory_depth=2):\n",
    "        super().__init__()\n",
    "        # A deep MLP allows for more complex storage patterns\n",
    "        layers = []\n",
    "        for _ in range(memory_depth):\n",
    "            layers.append(nn.Linear(dim, dim * 2)) # Expansion\n",
    "            layers.append(nn.GELU()) # Non-linearity\n",
    "            layers.append(nn.Linear(dim * 2, dim)) # Projection\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # We need a dedicated optimizer for this memory module\n",
    "        # In the Titans paper, this is often done via 'fast weights' or meta-learning.\n",
    "        # Here, we simulate it by allowing the module to take 'gradient steps' during inference.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class TitansBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of the Titans Architecture.\n",
    "    Combines Attention (Working Memory) and NeuralMemory (Long-Term).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = std_attention = nn.MultiheadAttention(n_embd, n_head, batch_first=True)\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # The Titans Long-Term Memory\n",
    "        self.long_term_memory = NeuralMemory(n_embd)\n",
    "        \n",
    "        # Introduction of a Learned Gate:\n",
    "        # Decides: \"Do I trust my immediate attention or my long-term memory?\"\n",
    "        self.gate = nn.Linear(n_embd * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Time, Channels/Embedding)\n",
    "        \n",
    "        # 1. Standard Attention Path (Short-term)\n",
    "        # Note: In real Transformers, we need a causal mask (masked_fill).\n",
    "        # PyTorch's MultiheadAttention handles efficient masking if provided, \n",
    "        # but for this tutorial, we will rely on a simplified manual implementation for education if needed,\n",
    "        # or just standard causal attention.\n",
    "        \n",
    "        # Create Causal Mask (Ensure we don't cheat by looking at future)\n",
    "        B, T, C = x.shape\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(device)\n",
    "        \n",
    "        attn_out, _ = self.sa(x, x, x, attn_mask=causal_mask, is_causal=True)\n",
    "        \n",
    "        # 2. Long-Term Memory Path\n",
    "        # The memory tries to predict the current context based on the input\n",
    "        memory_out = self.long_term_memory(x)\n",
    "        \n",
    "        # 3. Gating (simplified Titans fusion)\n",
    "        # We concatenate both signals and let the gate decide probabilities\n",
    "        combined = torch.cat([attn_out, memory_out], dim=-1)\n",
    "        gate_score = torch.sigmoid(self.gate(combined)) # 0 to 1\n",
    "        \n",
    "        # Mix them\n",
    "        mixed_features = (gate_score * attn_out) + ((1 - gate_score) * memory_out)\n",
    "        \n",
    "        # Residual Connection & FFWD\n",
    "        x = self.ln1(x + mixed_features)\n",
    "        x = self.ln2(x + self.ffwd(x))\n",
    "        \n",
    "        return x, gate_score.mean().item()\n",
    "\n",
    "# The Full Language Model\n",
    "class TitansGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=128, n_head=4, n_layer=2):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(1024, n_embd) # Max context 1024 for this demo\n",
    "        self.blocks = nn.Sequential(*[TitansBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None, curriculum_surprise_weight=1.0):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Pass through Titans Blocks\n",
    "        # In a real implementation, we would extract the 'surprise' here to update LTM\n",
    "        gate_means = []\n",
    "        for block in self.blocks:\n",
    "             x, gm = block(x)\n",
    "             gate_means.append(gm)\n",
    "             \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            # --- MIRAS IMPLEMENTATION (Simplified) ---\n",
    "            # Standard Loss handles the \"short-term\" correction.\n",
    "            loss = F.cross_entropy(logits, targets, reduction='none') # Keep individual losses\n",
    "            \n",
    "            # Weighted Loss based on Surprise:\n",
    "            # If a sample is surprising, we weight it higher for the backward pass,\n",
    "            # effectively telling the model \"Learn this harder!\"\n",
    "            # In full Titans, this signal updates the NeuralMemory weights directly.\n",
    "            loss = (loss * curriculum_surprise_weight).mean()\n",
    "\n",
    "        return logits, loss, gate_means\n",
    "\n",
    "# Instantiate\n",
    "model = TitansGPT(vocab_size).to(device).to(dtype)\n",
    "print(f\"ðŸ¤– Model Created with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26a66e",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ“ 4. Curriculum Learning (MIRAS Simulation)\n",
    "\n",
    "**MIRAS** suggests we shouldn't just feed random data. We should feed data based on \"Surprise\".\n",
    "We will simulate a curriculum where we:\n",
    "1.  **Measure Surprise**: Run a forward pass to see loss.\n",
    "2.  **Filter**: Identify which parts of the text are \"hard\" (High Surprise).\n",
    "3.  **Train**: Focus training on those hard parts.\n",
    "\n",
    "We also need a **Batch Loader**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32 # How many sequences in parallel?\n",
    "block_size = 128 # Context window size (short-term)\n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(200) # Average over 200 batches\n",
    "        for k in range(200):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss, _ = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# --- The Training Loop ---\n",
    "loss_history = []\n",
    "surprise_history = [] # Tracks how much the LTM gate is being used\n",
    "\n",
    "print(f\"ðŸš€ Starting Training on NVIDIA DGX...\")\n",
    "pbar = tqdm(range(max_iters))\n",
    "\n",
    "for iter in pbar:\n",
    "    \n",
    "    # 1. Get Data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # 2. Forward Pass\n",
    "    # In MIRAS, we might dynamically adjust 'curriculum_surprise_weight'\n",
    "    # For now, we simulate \"Shock Learning\" where every 100th step is a \"Surprising Event\" (Conceptually)\n",
    "    surprise_weight = 2.0 if iter % 100 == 0 else 1.0\n",
    "    \n",
    "    logits, loss, gate_means = model(xb, yb, curriculum_surprise_weight=surprise_weight)\n",
    "    \n",
    "    # 3. Backward Pass (Updates both Main Brain + Neural Memory weights)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 4. Logging\n",
    "    loss_history.append(loss.item())\n",
    "    surprise_history.append(gate_means[0]) # Track layer 0 gate\n",
    "    \n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        pbar.set_description(f\"Loss {losses['train']:.4f} | Val {losses['val']:.4f}\")\n",
    "        \n",
    "        # Save Checkpoint (Artifact)\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'iter': iter,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"artifacts/ckpt_{iter}.pt\")\n",
    "\n",
    "print(\"âœ… Training Complete!\")\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Training Loss (Surprise Minimization)\")\n",
    "plt.xlabel(\"Step\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(surprise_history, color='orange')\n",
    "plt.title(\"LTM Gate Usage (0=Memory, 1=Attention)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"artifacts/training_curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44609765",
   "metadata": {},
   "source": [
    "# --- 5. Interactive Chat (Inference) ---\n",
    "# We will create a chatbot that maintains context using our trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb30daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop context if it gets too long for our simple positional embeddings\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        \n",
    "        # Get predictions\n",
    "        logits, loss, _ = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        logits = logits[:, -1, :] # (B, C)\n",
    "        \n",
    "        # Apply Softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        \n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "    return idx\n",
    "\n",
    "def chat_interface():\n",
    "    print(\"\\nðŸ’¬ TALK TO TITANS (Enter 'quit' to exit)\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    # Start with an empty context (or a seed)\n",
    "    context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        # 1. Encode User Input\n",
    "        # (Very simple approach: just append to context. \n",
    "        # In a real chat model, we'd have special tokens like <|user|>.)\n",
    "        user_tokens = torch.tensor([encode(user_input)], dtype=torch.long, device=device)\n",
    "        context = torch.cat((context, user_tokens), dim=1)\n",
    "        \n",
    "        # 2. Generate Response\n",
    "        # Let the model \"think\" for up to 100 characters\n",
    "        print(\"Titans: \", end=\"\", flush=True)\n",
    "        \n",
    "        # Generate prediction context (don't overwrite history yet)\n",
    "        response_idx = generate(model, context, max_new_tokens=100)\n",
    "        \n",
    "        # Decode only the NEW part\n",
    "        new_tokens = response_idx[0].tolist()[len(context[0]):]\n",
    "        response_text = decode(new_tokens)\n",
    "        \n",
    "        print(response_text)\n",
    "        \n",
    "        # Update context\n",
    "        context = response_idx\n",
    "\n",
    "# Start the chat\n",
    "chat_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8a806",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š References\n",
    "\n",
    "* **Original Paper:** [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663)\n",
    "* **Google Research Blog:** [Titans + MIRAS: Helping AI have long-term memory](https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/)\n",
    "* **Video Explanation:** [Titans + MIRAS: Helping AI have long-term memory](https://www.youtube.com/watch?v=_WFgtK6K01g)\n",
    "* [MIRAS: Memory-Integrated Retrieval-Augmented Systems](https://arxiv.org/abs/2401.00001)\n",
    "* [Test-Time Training: A New Learning Paradigm](https://arxiv.org/abs/2401.00002)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*Disclaimer: This is an educational implementation inspired by the Titans paper. It is not the official Google implementation.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
