{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d4d4f0",
   "metadata": {},
   "source": [
    "# [BRAIN] Titans + MIRAS: Building AI with Long-Term Memory\n",
    "## A BEGINNER'S GUIDE TO NEXT-GEN AI ARCHITECTURE\n",
    "\n",
    "Welcome, student! ðŸŽ“\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤” What Will You Learn Today?\n",
    "\n",
    "Today, we are going to embark on an exciting journey into the cutting edge of Artificial Intelligence. We will learn about **Titans** and **MIRAS**, two revolutionary concepts from Google Research that help AI \"remember\" things for a very long time.\n",
    "\n",
    "### ðŸ§© The Problem We're Solving\n",
    "\n",
    "Imagine you're reading a long novel. You can easily remember:\n",
    "- What happened in the **last few pages** (Short-Term Memory)\n",
    "- Important plot points from **earlier chapters** (Long-Term Memory)\n",
    "\n",
    "Current AI models (like standard Transformers) are like readers who can only remember the last few pages! **Titans** solves this by giving AI both types of memory.\n",
    "\n",
    "### ðŸŽ¯ What We'll Build\n",
    "\n",
    "| Step | What We Do | Why It Matters |\n",
    "|------|------------|----------------|\n",
    "| 1ï¸âƒ£ **Setup** | Check our hardware | Ensure GPU is ready |\n",
    "| 2ï¸âƒ£ **Data** | Download Shakespeare | AI needs text to learn from |\n",
    "| 3ï¸âƒ£ **Tokenization** | Convert text -> numbers | Computers only understand numbers |\n",
    "| 4ï¸âƒ£ **Architecture** | Build Titans model | The \"brain\" structure |\n",
    "| 5ï¸âƒ£ **Training** | Teach the model | Learning from examples |\n",
    "| 6ï¸âƒ£ **Chat** | Talk to our AI | See it in action! |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Quick Concepts (Read This First!)\n",
    "\n",
    "### What is a Neural Network?\n",
    "Think of it as a **function** that takes input and produces output. Just like `f(x) = 2x + 1`, but with millions of learnable parameters instead of just `2` and `1`.\n",
    "\n",
    "### What is a Transformer?\n",
    "A type of neural network that processes text by looking at **relationships between words**. ChatGPT, Claude, and most modern AI use Transformers.\n",
    "\n",
    "### What is Memory in AI?\n",
    "- **Short-Term Memory (Attention)**: \"I remember the last 100 words\"\n",
    "- **Long-Term Memory (Neural Memory)**: \"I remember important patterns from the entire conversation\"\n",
    "\n",
    "### What is MIRAS?\n",
    "A training strategy that says: **\"Focus more on surprising/difficult examples\"**. Like a student spending more time on hard problems!\n",
    "\n",
    "---\n",
    "\n",
    "Let's begin! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0759ebe",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "\n",
    "**Running on Colab?** Execute the cell below to install dependencies and configure the environment.\n",
    "\n",
    "**Running locally?** Skip this cell (dependencies should be installed via `pip install -r requirements.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f642651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GOOGLE COLAB SETUP (Skip if running locally)\n",
    "# ============================================================================\n",
    "# This cell installs all required dependencies for Google Colab\n",
    "# If you're running locally, you can skip this cell.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Check if we're running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"[COLAB] Detected Google Colab environment\")\n",
    "    print(\"[COLAB] Installing required packages...\")\n",
    "    \n",
    "    # Install core dependencies\n",
    "    !pip install -q torch torchvision torchaudio\n",
    "    !pip install -q matplotlib requests tqdm\n",
    "    \n",
    "    # Configure matplotlib for Colab\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    \n",
    "    # Check GPU availability\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"[COLAB] GPU detected: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    else:\n",
    "        print(\"[COLAB] No GPU detected. Training will be slower on CPU.\")\n",
    "        print(\"[COLAB] Tip: Go to Runtime -> Change runtime type -> GPU\")\n",
    "    \n",
    "    print(\"[COLAB] Setup complete!\")\n",
    "else:\n",
    "    print(\"[LOCAL] Not running in Colab. Skipping setup.\")\n",
    "    print(\"[LOCAL] Make sure you've run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fb3c4",
   "metadata": {},
   "source": [
    "# ðŸ“‹ Table of Contents\n",
    "\n",
    "| Part | Topic | Description |\n",
    "|------|-------|-------------|\n",
    "| **Part 1** | ðŸ–¥ï¸ Environment Setup | Check hardware, import libraries, GPU benchmark |\n",
    "| **Part 2** | [INFO] Data Pipeline | Download data, visualize, tokenize, create batches |\n",
    "| **Part 3** | [BRAIN] Titans Architecture | Embeddings, Attention, Neural Memory, Gating, Full Model |\n",
    "| **Part 4** | ðŸŽ“ Training with MIRAS | Hyperparameters, training loop, loss curves |\n",
    "| **Part 5** | [CHART] Analysis | Training visualization, model evolution, learned patterns |\n",
    "| **Part 6** | [CHAT] Inference | Text generation, temperature, interactive chat |\n",
    "\n",
    "**â±ï¸ Estimated Time:** 30-45 minutes to run all cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e5594",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ–¥ï¸ Part 1: Environment Setup\n",
    "\n",
    "## Why Do We Need a GPU?\n",
    "\n",
    "Training AI is like doing millions of math problems simultaneously. \n",
    "\n",
    "| Hardware | Analogy | Speed |\n",
    "|----------|---------|-------|\n",
    "| **CPU** | One very smart person | 1x |\n",
    "| **GPU** | Thousands of workers in parallel | 100x+ |\n",
    "\n",
    "We'll check if your GPU is ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1.1: Import Libraries\n",
    "# ============================================================\n",
    "# These are our \"tools\" - pre-written code packages that help us\n",
    "\n",
    "import torch                        # The main AI framework (like TensorFlow, but easier)\n",
    "import torch.nn as nn               # Neural Network building blocks\n",
    "import torch.nn.functional as F     # Useful functions (softmax, relu, etc.)\n",
    "import requests                     # For downloading files from the internet\n",
    "import os                           # For file/folder operations\n",
    "import matplotlib.pyplot as plt     # For creating visualizations\n",
    "import numpy as np                  # For numerical operations\n",
    "import platform                     # To check our computer's specs\n",
    "import time                         # For timing operations\n",
    "from tqdm import tqdm               # Progress bars (makes waiting fun!)\n",
    "import warnings\n",
    "\n",
    "# Suppress some technical warnings that might confuse beginners\n",
    "warnings.filterwarnings(\"ignore\", message=\".*cuda capability.*\", category=UserWarning)\n",
    "\n",
    "# Configure matplotlib to handle Unicode characters properly\n",
    "# This prevents \"missing glyph\" warnings for special characters\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"[OK] All libraries imported successfully!\")\n",
    "print(\"\\n[INFO] Libraries we're using:\")\n",
    "print(f\"   - PyTorch version: {torch.__version__}\")\n",
    "print(f\"   - NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeaf2b3",
   "metadata": {},
   "source": [
    "### ðŸ” Hardware Detection\n",
    "\n",
    "Now let's check what hardware we have available. The code below will:\n",
    "1. Check your CPU type\n",
    "2. Look for a GPU (Graphics Processing Unit)\n",
    "3. Set up the best settings for your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ada19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1.2: Check Hardware (CPU & GPU)\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ” Checking Hardware Environment...\\n\")\n",
    "\n",
    "# Check CPU Architecture\n",
    "cpu_arch = platform.machine()\n",
    "print(f\"ðŸ–¥ï¸  CPU Architecture: {cpu_arch}\")\n",
    "\n",
    "# Check for GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"[OK] GPU Available: Yes!\")\n",
    "    print(f\"   â€¢ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   â€¢ GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"   â€¢ GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Get GPU memory info\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"   â€¢ GPU Memory: {total_memory:.1f} GB\")\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "    dtype = torch.bfloat16  # Best for modern GPUs\n",
    "    print(f\"\\nðŸš€ Using GPU with BFloat16 precision for fast training!\")\n",
    "else:\n",
    "    print(\"[!]  No GPU detected. Using CPU (this will be slower).\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "# Create folder for saving our work\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "print(\"\\nðŸ“‚ Created 'artifacts' folder for saving checkpoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464fe7f0",
   "metadata": {},
   "source": [
    "### [CHART] Visualizing CPU vs GPU Speed\n",
    "\n",
    "Let's see why GPUs are so much faster for AI work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e208ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1.3: Visualize CPU vs GPU (Quick Benchmark)\n",
    "# ============================================================\n",
    "\n",
    "# Let's do a simple matrix multiplication benchmark\n",
    "matrix_size = 1000\n",
    "\n",
    "# Time on CPU\n",
    "cpu_matrix = torch.randn(matrix_size, matrix_size)\n",
    "start = time.time()\n",
    "_ = cpu_matrix @ cpu_matrix  # Matrix multiplication\n",
    "cpu_time = time.time() - start\n",
    "\n",
    "# Time on GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    gpu_matrix = torch.randn(matrix_size, matrix_size, device='cuda')\n",
    "    torch.cuda.synchronize()  # Wait for GPU to be ready\n",
    "    start = time.time()\n",
    "    _ = gpu_matrix @ gpu_matrix\n",
    "    torch.cuda.synchronize()  # Wait for operation to complete\n",
    "    gpu_time = time.time() - start\n",
    "else:\n",
    "    gpu_time = cpu_time  # If no GPU, same as CPU\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart comparing times\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(['CPU', 'GPU'], [cpu_time * 1000, gpu_time * 1000], \n",
    "               color=['#FF6B6B', '#4ECDC4'], edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Time (milliseconds)', fontsize=12)\n",
    "ax1.set_title(f'Matrix Multiplication Speed\\n({matrix_size}x{matrix_size} matrices)', fontsize=14)\n",
    "ax1.set_ylim(0, max(cpu_time, gpu_time) * 1000 * 1.2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, [cpu_time * 1000, gpu_time * 1000]):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{val:.2f}ms', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Speedup visualization\n",
    "ax2 = axes[1]\n",
    "speedup = cpu_time / gpu_time if gpu_time > 0 else 1\n",
    "ax2.barh(['Speedup'], [speedup], color='#45B7D1', edgecolor='black', linewidth=2, height=0.5)\n",
    "ax2.set_xlabel('Times Faster', fontsize=12)\n",
    "ax2.set_title('GPU Speedup over CPU', fontsize=14)\n",
    "ax2.set_xlim(0, max(speedup * 1.2, 2))\n",
    "ax2.axvline(x=1, color='gray', linestyle='--', alpha=0.7, label='No speedup')\n",
    "ax2.text(speedup + 0.1, 0, f'{speedup:.1f}x', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/cpu_vs_gpu.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n[TIP] Key Insight: GPU is {speedup:.1f}x faster for matrix operations!\")\n",
    "print(\"   This is why we use GPUs for training neural networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa53ae63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# [INFO] Part 2: Data Pipeline - From Text to Numbers\n",
    "\n",
    "## ðŸ¤” Why Do We Need Data?\n",
    "\n",
    "AI learns by example, just like humans! \n",
    "- **Humans**: Learn to speak by hearing thousands of sentences\n",
    "- **AI**: Learns to write by reading thousands of texts\n",
    "\n",
    "We'll use **Shakespeare's plays** as our training data. Why Shakespeare?\n",
    "- [OK] Free and publicly available\n",
    "- [OK] Complex language patterns\n",
    "- [OK] Small enough to train quickly (~1MB)\n",
    "\n",
    "## ðŸ”„ The Data Pipeline\n",
    "\n",
    "```\n",
    "Raw Text -> Tokenization -> Numbers -> Tensors -> GPU\n",
    "\"Hello\"  ->  ['H','e','l','l','o']  ->  [7,4,11,11,14]  ->  tensor([...])  ->  cuda:0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1926b",
   "metadata": {},
   "source": [
    "### ðŸ“¥ Step 2.1: Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2.1: Download Shakespeare Dataset\n",
    "# ============================================================\n",
    "\n",
    "# URL where the dataset is hosted\n",
    "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "file_path = 'artifacts/input.txt'\n",
    "\n",
    "# Download if not already present\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"â¬‡ï¸  Downloading Shakespeare dataset...\")\n",
    "    response = requests.get(data_url)\n",
    "    data = response.text\n",
    "    \n",
    "    # Save to file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(data)\n",
    "    print(\"[OK] Download complete!\")\n",
    "else:\n",
    "    print(\"[OK] Dataset found locally. Loading...\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "# Show dataset statistics\n",
    "print(f\"\\n[CHART] Dataset Statistics:\")\n",
    "print(f\"   â€¢ Total characters: {len(data):,}\")\n",
    "print(f\"   â€¢ Total lines: {len(data.splitlines()):,}\")\n",
    "print(f\"   â€¢ File size: {len(data)/1024:.1f} KB\")\n",
    "\n",
    "# Show a preview\n",
    "print(f\"\\nðŸ“– First 300 characters:\")\n",
    "print(\"=\" * 50)\n",
    "print(data[:300])\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9dd1b",
   "metadata": {},
   "source": [
    "### [CHART] Step 2.2: Visualize the Dataset\n",
    "\n",
    "Let's explore what's in our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2.2: Visualize Character Distribution\n",
    "# ============================================================\n",
    "\n",
    "# Find all unique characters\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"[TEXT] Vocabulary Analysis:\")\n",
    "print(f\"   â€¢ Unique characters: {vocab_size}\")\n",
    "print(f\"   â€¢ Characters: {''.join(chars)}\")\n",
    "\n",
    "# Count character frequencies\n",
    "from collections import Counter\n",
    "char_counts = Counter(data)\n",
    "\n",
    "# Get top 20 most common characters\n",
    "top_chars = char_counts.most_common(20)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Top 20 characters bar chart\n",
    "ax1 = axes[0]\n",
    "chars_labels = [c if c != '\\n' else '\\\\n' for c, _ in top_chars]\n",
    "chars_labels = [c if c != ' ' else 'space' for c in chars_labels]\n",
    "counts = [count for _, count in top_chars]\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(chars_labels)))\n",
    "bars = ax1.bar(chars_labels, counts, color=colors, edgecolor='black')\n",
    "ax1.set_xlabel('Character', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Top 20 Most Common Characters', fontsize=14)\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Plot 2: Character type distribution (pie chart)\n",
    "ax2 = axes[1]\n",
    "letters = sum(1 for c in data if c.isalpha())\n",
    "spaces = data.count(' ')\n",
    "newlines = data.count('\\n')\n",
    "punctuation = sum(1 for c in data if not c.isalnum() and c not in ' \\n')\n",
    "digits = sum(1 for c in data if c.isdigit())\n",
    "\n",
    "sizes = [letters, spaces, newlines, punctuation]\n",
    "labels = [f'Letters\\n({letters:,})', f'Spaces\\n({spaces:,})', \n",
    "          f'Newlines\\n({newlines:,})', f'Punctuation\\n({punctuation:,})']\n",
    "colors_pie = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "explode = (0.05, 0, 0, 0)\n",
    "\n",
    "ax2.pie(sizes, labels=labels, colors=colors_pie, explode=explode,\n",
    "        autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10})\n",
    "ax2.set_title('Character Type Distribution', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/character_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n[TIP] Insight: Spaces and letters make up most of the text!\")\n",
    "print(f\"   This is typical for natural language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb21094",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ”¢ Step 2.3: Understanding Tokenization\n",
    "\n",
    "## What is Tokenization?\n",
    "\n",
    "Computers can't read letters - they only understand numbers! **Tokenization** converts text into numbers.\n",
    "\n",
    "```\n",
    "\"Hello\" -> ['H', 'e', 'l', 'l', 'o'] -> [7, 4, 11, 11, 14]\n",
    "```\n",
    "\n",
    "### Types of Tokenizers:\n",
    "\n",
    "| Type | What it does | Example | Used by |\n",
    "|------|-------------|---------|---------|\n",
    "| **Character-level** | Each letter = 1 token | \"Hi\" -> [7, 8] | Simple models |\n",
    "| **Word-level** | Each word = 1 token | \"Hello world\" -> [42, 99] | Older models |\n",
    "| **Subword** | Common patterns = 1 token | \"playing\" -> [\"play\", \"ing\"] | GPT, BERT |\n",
    "\n",
    "We'll use **character-level** tokenization because it's easiest to understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d968a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2.3: Build Our Tokenizer\n",
    "# ============================================================\n",
    "\n",
    "# Create character-to-number mapping (stoi = \"string to integer\")\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create number-to-character mapping (itos = \"integer to string\")  \n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Let's see what our mappings look like\n",
    "print(\"ðŸ“– Character -> Number Mapping (first 10):\")\n",
    "for i, (char, num) in enumerate(stoi.items()):\n",
    "    if i >= 10:\n",
    "        print(\"   ...\")\n",
    "        break\n",
    "    display_char = repr(char) if char in '\\n\\t' else f\"'{char}'\"\n",
    "    print(f\"   {display_char:8} -> {num}\")\n",
    "\n",
    "# Define our encoder and decoder functions\n",
    "def encode(text):\n",
    "    \"\"\"Convert text to list of numbers\"\"\"\n",
    "    # Skip characters not in our vocabulary (handles edge cases)\n",
    "    return [stoi[c] for c in text if c in stoi]\n",
    "\n",
    "def decode(numbers):\n",
    "    \"\"\"Convert list of numbers back to text\"\"\"\n",
    "    return ''.join([itos[i] for i in numbers])\n",
    "\n",
    "# Test our tokenizer!\n",
    "print(\"\\nðŸ§ª Testing the Tokenizer:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_examples = [\"Hello\", \"To be or not to be\", \"ROMEO:\"]\n",
    "for text in test_examples:\n",
    "    encoded = encode(text)\n",
    "    decoded = decode(encoded)\n",
    "    print(f\"Original:  '{text}'\")\n",
    "    print(f\"Encoded:   {encoded}\")\n",
    "    print(f\"Decoded:   '{decoded}'\")\n",
    "    print(f\"Match: {'[OK]' if text == decoded else 'âŒ'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c37c73",
   "metadata": {},
   "source": [
    "### [CHART] Step 2.4: Visualize Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9795fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2.4: Visualize Tokenization Process\n",
    "# ============================================================\n",
    "\n",
    "# Visualize how tokenization works\n",
    "example_text = \"ROMEO: O, she\"\n",
    "encoded_example = encode(example_text)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Positions with more vertical space\n",
    "char_box_y = 1.5      # Character boxes (top row)\n",
    "token_box_y = -0.8    # Token boxes (bottom row)\n",
    "box_height = 0.8\n",
    "box_width = 0.9\n",
    "spacing = 1.1\n",
    "\n",
    "# Create a visual representation\n",
    "for i, (char, token) in enumerate(zip(example_text, encoded_example)):\n",
    "    x_pos = i * spacing\n",
    "    \n",
    "    # Draw character box (top)\n",
    "    rect = plt.Rectangle((x_pos, char_box_y), box_width, box_height, \n",
    "                          facecolor='#E8F4FD', edgecolor='#2196F3', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Draw token box (bottom)\n",
    "    rect2 = plt.Rectangle((x_pos, token_box_y), box_width, box_height, \n",
    "                           facecolor='#E8F8E8', edgecolor='#4CAF50', linewidth=2)\n",
    "    ax.add_patch(rect2)\n",
    "    \n",
    "    # Add character text (centered in char box)\n",
    "    display_char = char if char != ' ' else '_'\n",
    "    ax.text(x_pos + box_width/2, char_box_y + box_height/2, display_char, \n",
    "            ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Add token number (centered in token box)\n",
    "    ax.text(x_pos + box_width/2, token_box_y + box_height/2, str(token), \n",
    "            ha='center', va='center', fontsize=14, fontweight='bold', color='#2E7D32')\n",
    "    \n",
    "    # Draw arrow from bottom of char box to top of token box\n",
    "    arrow_x = x_pos + box_width/2\n",
    "    arrow_start_y = char_box_y  # Bottom of character box\n",
    "    arrow_end_y = token_box_y + box_height  # Top of token box\n",
    "    \n",
    "    ax.annotate('', xy=(arrow_x, arrow_end_y + 0.05), xytext=(arrow_x, arrow_start_y - 0.05),\n",
    "                arrowprops=dict(arrowstyle='->', color='#FF5722', lw=2, \n",
    "                               shrinkA=0, shrinkB=0))\n",
    "\n",
    "# Add labels\n",
    "label_x = -0.4\n",
    "ax.text(label_x, char_box_y + box_height/2, 'Text:', ha='right', va='center', \n",
    "        fontsize=14, fontweight='bold')\n",
    "ax.text(label_x, token_box_y + box_height/2, 'Tokens:', ha='right', va='center', \n",
    "        fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-1, len(example_text) * spacing + 0.5)\n",
    "ax.set_ylim(-1.2, 2.8)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "ax.set_title('Tokenization: Converting Characters to Numbers', fontsize=16, pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/tokenization_visual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n[TIP] Each character becomes a unique number!\")\n",
    "print(f\"   This is how computers 'read' text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d233bbb",
   "metadata": {},
   "source": [
    "### [TOOL] Step 2.5: Create Training Data\n",
    "\n",
    "Now we convert our entire Shakespeare text into tensors (multi-dimensional arrays) that PyTorch can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ad6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2.5: Convert Data to Tensors\n",
    "# ============================================================\n",
    "\n",
    "# Encode the entire dataset\n",
    "data_tensor = torch.tensor(encode(data), dtype=torch.long)\n",
    "print(f\"[CHART] Data Tensor Shape: {data_tensor.shape}\")\n",
    "print(f\"   This means we have {len(data_tensor):,} tokens!\")\n",
    "\n",
    "# Split into training (90%) and validation (10%) sets\n",
    "# Training: What the model learns from\n",
    "# Validation: What we test the model on (it never sees this during training!)\n",
    "split_point = int(0.9 * len(data_tensor))\n",
    "\n",
    "train_data = data_tensor[:split_point]\n",
    "val_data = data_tensor[split_point:]\n",
    "\n",
    "# Move to GPU for speed\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "\n",
    "print(f\"\\n[GRAPH] Data Split:\")\n",
    "print(f\"   â€¢ Training set:   {len(train_data):,} tokens (90%)\")\n",
    "print(f\"   â€¢ Validation set: {len(val_data):,} tokens (10%)\")\n",
    "\n",
    "# Visualize the split\n",
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "\n",
    "# Create split visualization\n",
    "total = len(data_tensor)\n",
    "train_width = split_point / total\n",
    "val_width = 1 - train_width\n",
    "\n",
    "ax.barh(['Dataset'], [train_width], color='#4CAF50', label=f'Training ({len(train_data):,})')\n",
    "ax.barh(['Dataset'], [val_width], left=[train_width], color='#FF9800', \n",
    "        label=f'Validation ({len(val_data):,})')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel('Proportion of Data')\n",
    "ax.set_title('Train/Validation Split', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add percentage labels\n",
    "ax.text(train_width/2, 0, '90%', ha='center', va='center', fontsize=14, \n",
    "        fontweight='bold', color='white')\n",
    "ax.text(train_width + val_width/2, 0, '10%', ha='center', va='center', fontsize=14, \n",
    "        fontweight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/data_split.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n[TIP] Why split the data?\")\n",
    "print(f\"   Training set: Model learns patterns from this\")\n",
    "print(f\"   Validation set: We check if model learned well (unseen data!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e72a9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# [BRAIN] Part 3: The Titans Architecture\n",
    "\n",
    "## Understanding the Big Picture\n",
    "\n",
    "Think of **Titans** as an AI brain with TWO types of memory:\n",
    "\n",
    "| Memory Type | Human Analogy | What It Does | Range |\n",
    "|-------------|---------------|--------------|-------|\n",
    "| **Short-Term (Attention)** | \"I remember what you just said\" | Looks at recent tokens | ~128 tokens |\n",
    "| **Long-Term (Neural Memory)** | \"I remember your name from earlier\" | Stores patterns in weights | Unlimited! |\n",
    "\n",
    "## [BUILD] Architecture Overview\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚           TITANS ARCHITECTURE           â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                        â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚                                       â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ ATTENTION  â”‚                        â”‚ NEURAL MEMORY â”‚\n",
    "              â”‚(Short-Term)â”‚                        â”‚  (Long-Term)  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                    â”‚                                       â”‚\n",
    "                    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\n",
    "                    â””â”€â”€â”€â”€â–ºâ”‚   GATING LAYER    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚ (Decides which to â”‚\n",
    "                          â”‚   trust more)     â”‚\n",
    "                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”\n",
    "                              â”‚  OUTPUT â”‚\n",
    "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Let's build each component step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf8e18",
   "metadata": {},
   "source": [
    "### ðŸ§± Step 3.1: Understanding Embeddings\n",
    "\n",
    "Before we process text, we need to convert token IDs into **vectors** (lists of numbers).\n",
    "\n",
    "Why? Because:\n",
    "- Token ID `42` is just a label - it doesn't carry meaning\n",
    "- A vector like `[0.2, -0.5, 0.8, ...]` can capture relationships between words\n",
    "- Similar words get similar vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc54b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3.1: Visualize What Embeddings Do\n",
    "# ============================================================\n",
    "\n",
    "# Create a simple embedding layer\n",
    "n_embd = 128  # Each character becomes a 128-dimensional vector\n",
    "embedding = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "# Get embeddings for some example characters\n",
    "example_chars = ['A', 'B', 'a', 'b', ' ', '.']\n",
    "example_ids = [stoi[c] for c in example_chars]\n",
    "example_embeddings = embedding(torch.tensor(example_ids))\n",
    "\n",
    "print(\"[CHART] Embedding Example:\")\n",
    "print(f\"   Vocabulary size: {vocab_size} characters\")\n",
    "print(f\"   Embedding dimension: {n_embd}\")\n",
    "print(f\"\\n   Each character gets a {n_embd}-dimensional vector!\")\n",
    "\n",
    "# Visualize embeddings as heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of embeddings\n",
    "ax1 = axes[0]\n",
    "im = ax1.imshow(example_embeddings.detach().numpy()[:, :32], cmap='RdBu', aspect='auto')\n",
    "ax1.set_yticks(range(len(example_chars)))\n",
    "ax1.set_yticklabels([repr(c) if c in ' \\n' else f\"'{c}'\" for c in example_chars])\n",
    "ax1.set_xlabel('Embedding Dimension (showing first 32)', fontsize=11)\n",
    "ax1.set_ylabel('Character', fontsize=11)\n",
    "ax1.set_title('Character Embeddings (First 32 Dimensions)', fontsize=13)\n",
    "plt.colorbar(im, ax=ax1, label='Value')\n",
    "\n",
    "# Similarity matrix\n",
    "ax2 = axes[1]\n",
    "# Calculate cosine similarity between embeddings\n",
    "emb_normalized = F.normalize(example_embeddings, dim=1)\n",
    "similarity = (emb_normalized @ emb_normalized.T).detach().numpy()\n",
    "\n",
    "im2 = ax2.imshow(similarity, cmap='Greens', vmin=0, vmax=1)\n",
    "ax2.set_xticks(range(len(example_chars)))\n",
    "ax2.set_yticks(range(len(example_chars)))\n",
    "ax2.set_xticklabels([f\"'{c}'\" for c in example_chars])\n",
    "ax2.set_yticklabels([f\"'{c}'\" for c in example_chars])\n",
    "ax2.set_title('Character Similarity (Before Training)', fontsize=13)\n",
    "plt.colorbar(im2, ax=ax2, label='Similarity')\n",
    "\n",
    "# Add similarity values\n",
    "for i in range(len(example_chars)):\n",
    "    for j in range(len(example_chars)):\n",
    "        ax2.text(j, i, f'{similarity[i,j]:.2f}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/embeddings_visual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[TIP] Key Insight:\")\n",
    "print(\"   Before training, similar characters (A/a, B/b) may not have similar embeddings.\")\n",
    "print(\"   After training, the model learns meaningful relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf1d5b",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Step 3.2: Understanding Attention (Short-Term Memory)\n",
    "\n",
    "**Attention** is how Transformers \"focus\" on relevant parts of the input.\n",
    "\n",
    "Think of reading a sentence:\n",
    "> \"The cat sat on the **mat** because **it** was tired.\"\n",
    "\n",
    "To understand what \"it\" refers to, you **attend** back to \"cat\" (not \"mat\")!\n",
    "\n",
    "```\n",
    "Query:   \"What does 'it' refer to?\"\n",
    "Keys:    [The, cat, sat, on, the, mat, because, it, was, tired]\n",
    "Values:  [meaning of each word]\n",
    "         \n",
    "         Attention finds: \"cat\" is most relevant!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3.2: Visualize How Attention Works\n",
    "# ============================================================\n",
    "\n",
    "# Let's create a simple attention visualization\n",
    "sentence = \"The cat sat\"\n",
    "tokens = list(sentence)\n",
    "n_tokens = len(tokens)\n",
    "\n",
    "# Simulate attention weights (in reality, these are learned)\n",
    "# For the word \"sat\", what does it pay attention to?\n",
    "np.random.seed(42)\n",
    "attention_weights = np.random.dirichlet(np.ones(n_tokens), size=n_tokens)\n",
    "\n",
    "# Make it more realistic: things attend more to nearby tokens\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        if j > i:  # Can't attend to future (causal mask)\n",
    "            attention_weights[i, j] = 0\n",
    "    # Normalize\n",
    "    if attention_weights[i].sum() > 0:\n",
    "        attention_weights[i] /= attention_weights[i].sum()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Attention heatmap\n",
    "ax1 = axes[0]\n",
    "im = ax1.imshow(attention_weights, cmap='Blues', vmin=0, vmax=1)\n",
    "ax1.set_xticks(range(n_tokens))\n",
    "ax1.set_yticks(range(n_tokens))\n",
    "ax1.set_xticklabels([f\"'{t}'\" if t != ' ' else \"'_'\" for t in tokens])\n",
    "ax1.set_yticklabels([f\"'{t}'\" if t != ' ' else \"'_'\" for t in tokens])\n",
    "ax1.set_xlabel(\"Keys (What we look at)\", fontsize=12)\n",
    "ax1.set_ylabel(\"Queries (Current position)\", fontsize=12)\n",
    "ax1.set_title(\"Attention Weights (Causal Mask)\", fontsize=14)\n",
    "plt.colorbar(im, ax=ax1, label='Attention Weight')\n",
    "\n",
    "# Add values to heatmap\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        color = 'white' if attention_weights[i,j] > 0.4 else 'black'\n",
    "        ax1.text(j, i, f'{attention_weights[i,j]:.2f}', ha='center', va='center', \n",
    "                fontsize=9, color=color)\n",
    "\n",
    "# Causal mask explanation\n",
    "ax2 = axes[1]\n",
    "causal_mask = np.triu(np.ones((n_tokens, n_tokens)), k=1)\n",
    "ax2.imshow(causal_mask, cmap='RdYlGn_r', vmin=0, vmax=1)\n",
    "ax2.set_xticks(range(n_tokens))\n",
    "ax2.set_yticks(range(n_tokens))\n",
    "ax2.set_xticklabels([f\"'{t}'\" if t != ' ' else \"'_'\" for t in tokens])\n",
    "ax2.set_yticklabels([f\"'{t}'\" if t != ' ' else \"'_'\" for t in tokens])\n",
    "ax2.set_xlabel(\"Position (Future)\", fontsize=12)\n",
    "ax2.set_ylabel(\"Position (Current)\", fontsize=12)\n",
    "ax2.set_title(\"Causal Mask (Green=Can See, Red=Blocked)\", fontsize=14)\n",
    "\n",
    "# Add labels\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        text = \"X\" if causal_mask[i,j] else \"O\"\n",
    "        color = 'red' if causal_mask[i,j] else 'green'\n",
    "        ax2.text(j, i, text, ha='center', va='center', fontsize=14, \n",
    "                fontweight='bold', color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/attention_visual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[TIP] Key Concepts:\")\n",
    "print(\"   â€¢ Each position can only 'see' itself and previous positions\")\n",
    "print(\"   â€¢ This prevents 'cheating' by looking at future tokens\")\n",
    "print(\"   â€¢ Attention weights show how much each position influences another\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f20a4",
   "metadata": {},
   "source": [
    "### [BRAIN] Step 3.3: Understanding Neural Memory (Long-Term Memory)\n",
    "\n",
    "This is the **innovation of Titans**! \n",
    "\n",
    "Standard Transformers only have attention (short-term memory). Titans adds a **Neural Memory Module** - an MLP (Multi-Layer Perceptron) that learns to store long-term patterns.\n",
    "\n",
    "```\n",
    "               Neural Memory (MLP)\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   Input â”€â”€â”€â–º â”‚ Linear -> GELU     â”‚\n",
    "              â”‚ Linear -> GELU     â”‚ â”€â”€â”€â–º Output  \n",
    "              â”‚ (stores patterns) â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              \n",
    "   Think of it as: A small brain that remembers\n",
    "   important patterns from LONG ago!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d92025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3.3: Build the Neural Memory Module\n",
    "# ============================================================\n",
    "\n",
    "class NeuralMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    The 'Long-Term Memory' of our AI.\n",
    "    \n",
    "    Think of it as a small neural network that learns to store\n",
    "    important patterns from the past in its weights.\n",
    "    \n",
    "    Architecture:\n",
    "        Input (128) -> Linear (256) -> GELU -> Linear (128) -> Output\n",
    "                      â†‘                        â†‘\n",
    "                  Expansion            Back to original size\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, memory_depth=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build the memory network layer by layer\n",
    "        layers = []\n",
    "        for i in range(memory_depth):\n",
    "            # Expand dimensions (allows for richer representations)\n",
    "            layers.append(nn.Linear(dim, dim * 2))\n",
    "            # Non-linearity (allows learning complex patterns)\n",
    "            layers.append(nn.GELU())\n",
    "            # Project back to original dimension\n",
    "            layers.append(nn.Linear(dim * 2, dim))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Process input through the memory network\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "# Test the Neural Memory\n",
    "test_memory = NeuralMemory(n_embd)\n",
    "test_input = torch.randn(1, 10, n_embd)  # (batch=1, seq_len=10, dim=128)\n",
    "test_output = test_memory(test_input)\n",
    "\n",
    "print(\"[BRAIN] Neural Memory Module Created!\")\n",
    "print(f\"   â€¢ Input shape:  {test_input.shape}\")\n",
    "print(f\"   â€¢ Output shape: {test_output.shape}\")\n",
    "print(f\"   â€¢ Parameters: {sum(p.numel() for p in test_memory.parameters()):,}\")\n",
    "\n",
    "# Visualize the Neural Memory architecture\n",
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "\n",
    "# Draw the network architecture - wider boxes, shorter height, more spacing\n",
    "layers_info = [\n",
    "    ('Input\\n(128)', 0, '#E3F2FD'),\n",
    "    ('Linear\\n(128->256)', 1.4, '#BBDEFB'),\n",
    "    ('GELU\\n(activation)', 2.8, '#90CAF9'),\n",
    "    ('Linear\\n(256->128)', 4.2, '#BBDEFB'),\n",
    "    ('Linear\\n(128->256)', 5.6, '#BBDEFB'),\n",
    "    ('GELU\\n(activation)', 7.0, '#90CAF9'),\n",
    "    ('Linear\\n(256->128)', 8.4, '#BBDEFB'),\n",
    "    ('Output\\n(128)', 9.8, '#C8E6C9'),\n",
    "]\n",
    "\n",
    "box_width = 1.0\n",
    "box_height = 0.6\n",
    "y_center = 0.5\n",
    "\n",
    "for i, (name, x, color) in enumerate(layers_info):\n",
    "    # Draw box\n",
    "    rect = plt.Rectangle((x, y_center - box_height/2), box_width, box_height, \n",
    "                          facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + box_width/2, y_center, name, ha='center', va='center', \n",
    "            fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw arrows between boxes\n",
    "    if i < len(layers_info) - 1:\n",
    "        arrow_start_x = x + box_width\n",
    "        arrow_end_x = layers_info[i+1][1]\n",
    "        ax.annotate('', xy=(arrow_end_x - 0.05, y_center), \n",
    "                   xytext=(arrow_start_x + 0.05, y_center),\n",
    "                   arrowprops=dict(arrowstyle='->', color='#FF5722', lw=2.5))\n",
    "\n",
    "ax.set_xlim(-0.3, 11.2)\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "ax.axis('off')\n",
    "ax.set_title('Neural Memory Architecture (2-layer MLP)', fontsize=14, pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/neural_memory.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[TIP] How it works:\")\n",
    "print(\"   1. Input goes through expansion (128 -> 256)\")\n",
    "print(\"   2. GELU adds non-linearity (allows complex patterns)\")\n",
    "print(\"   3. Compression back to original size (256 -> 128)\")\n",
    "print(\"   4. Repeat for deeper memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3ff23",
   "metadata": {},
   "source": [
    "### [GATE] Step 3.4: The Gating Mechanism\n",
    "\n",
    "Now comes the clever part: **How do we combine short-term and long-term memory?**\n",
    "\n",
    "Titans uses a **learned gate** that decides:\n",
    "- \"For this token, should I trust my short-term memory (attention) more?\"\n",
    "- \"Or should I rely on my long-term patterns (neural memory)?\"\n",
    "\n",
    "```\n",
    "Gate = sigmoid(learned_weights @ [attention_output, memory_output])\n",
    "\n",
    "If gate ~ 1.0 -> Trust attention more\n",
    "If gate ~ 0.0 -> Trust neural memory more\n",
    "If gate ~ 0.5 -> Mix both equally\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3.4: Visualize the Gating Mechanism\n",
    "# ============================================================\n",
    "\n",
    "# Visualize how gating works\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Sigmoid function (the gate)\n",
    "ax1 = axes[0]\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = 1 / (1 + np.exp(-x))  # Sigmoid\n",
    "ax1.plot(x, y, 'b-', linewidth=3)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.7)\n",
    "ax1.fill_between(x, y, alpha=0.3)\n",
    "ax1.set_xlabel('Gate Input', fontsize=12)\n",
    "ax1.set_ylabel('Gate Output (0 to 1)', fontsize=12)\n",
    "ax1.set_title('Sigmoid: The Gating Function', fontsize=14)\n",
    "ax1.set_ylim(-0.1, 1.1)\n",
    "ax1.annotate('Trust Memory', xy=(-5, 0.1), fontsize=11, color='purple')\n",
    "ax1.annotate('Trust Attention', xy=(3, 0.9), fontsize=11, color='green')\n",
    "\n",
    "# Plot 2: Example of gating at different values\n",
    "ax2 = axes[1]\n",
    "gate_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "attn_contrib = gate_values\n",
    "mem_contrib = [1 - g for g in gate_values]\n",
    "\n",
    "x_pos = np.arange(len(gate_values))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x_pos - width/2, attn_contrib, width, label='Attention', color='#4CAF50')\n",
    "bars2 = ax2.bar(x_pos + width/2, mem_contrib, width, label='Memory', color='#9C27B0')\n",
    "\n",
    "ax2.set_xlabel('Gate Value', fontsize=12)\n",
    "ax2.set_ylabel('Contribution', fontsize=12)\n",
    "ax2.set_title('How Gate Controls Mixing', fontsize=14)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([f'g={g}' for g in gate_values])\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "# Plot 3: Visual mixing diagram\n",
    "ax3 = axes[2]\n",
    "ax3.axis('off')\n",
    "\n",
    "# Draw boxes\n",
    "attention_box = plt.Rectangle((0.1, 0.6), 0.3, 0.25, \n",
    "                               facecolor='#C8E6C9', edgecolor='#4CAF50', linewidth=3)\n",
    "memory_box = plt.Rectangle((0.1, 0.2), 0.3, 0.25, \n",
    "                            facecolor='#E1BEE7', edgecolor='#9C27B0', linewidth=3)\n",
    "gate_box = plt.Rectangle((0.5, 0.35), 0.15, 0.25, \n",
    "                          facecolor='#FFF9C4', edgecolor='#FFC107', linewidth=3)\n",
    "output_box = plt.Rectangle((0.75, 0.35), 0.2, 0.25, \n",
    "                            facecolor='#BBDEFB', edgecolor='#2196F3', linewidth=3)\n",
    "\n",
    "for box in [attention_box, memory_box, gate_box, output_box]:\n",
    "    ax3.add_patch(box)\n",
    "\n",
    "# Add text\n",
    "ax3.text(0.25, 0.725, 'Attention\\n(Short-term)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "ax3.text(0.25, 0.325, 'Memory\\n(Long-term)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "ax3.text(0.575, 0.475, 'Gate\\n(0-1)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "ax3.text(0.85, 0.475, 'Mixed\\nOutput', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Draw arrows\n",
    "ax3.annotate('', xy=(0.5, 0.55), xytext=(0.4, 0.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='#4CAF50', lw=2))\n",
    "ax3.annotate('', xy=(0.5, 0.4), xytext=(0.4, 0.35),\n",
    "            arrowprops=dict(arrowstyle='->', color='#9C27B0', lw=2))\n",
    "ax3.annotate('', xy=(0.75, 0.475), xytext=(0.65, 0.475),\n",
    "            arrowprops=dict(arrowstyle='->', color='#FFC107', lw=2))\n",
    "\n",
    "ax3.set_xlim(0, 1)\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.set_title('Titans Gating: Combining Memories', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/gating_visual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[TIP] The gate learns WHEN to use which memory!\")\n",
    "print(\"   â€¢ Recent, simple patterns -> Use attention\")\n",
    "print(\"   â€¢ Long-term, complex patterns -> Use neural memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af648b9",
   "metadata": {},
   "source": [
    "### [BUILD] Step 3.5: Building the Complete Titans Block\n",
    "\n",
    "Now let's combine all components into a single **TitansBlock**:\n",
    "\n",
    "1. **Attention** (short-term memory)\n",
    "2. **Neural Memory** (long-term memory)  \n",
    "3. **Gating** (learns when to use which)\n",
    "4. **Feed-Forward Network** (additional processing)\n",
    "5. **Layer Normalization** (stabilizes training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be536c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3.5: Build the Titans Block\n",
    "# ============================================================\n",
    "\n",
    "class TitansBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Titans layer that combines:\n",
    "    - Attention (short-term memory)\n",
    "    - Neural Memory (long-term memory)\n",
    "    - Learned gating (decides which to trust)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ===== SHORT-TERM: Multi-Head Attention =====\n",
    "        # PyTorch's built-in attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(n_embd, n_head, batch_first=True)\n",
    "        \n",
    "        # ===== LONG-TERM: Neural Memory =====\n",
    "        self.long_term_memory = NeuralMemory(n_embd)\n",
    "        \n",
    "        # ===== GATING: Learns when to use which memory =====\n",
    "        # Takes concatenated [attention, memory] and outputs a score\n",
    "        self.gate = nn.Linear(n_embd * 2, 1)\n",
    "        \n",
    "        # ===== FEED-FORWARD: Additional processing =====\n",
    "        self.ffwd = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Expand\n",
    "            nn.GELU(),                       # Non-linearity\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Project back\n",
    "            nn.Dropout(0.1),                 # Regularization\n",
    "        )\n",
    "        \n",
    "        # ===== NORMALIZATION: Stabilizes training =====\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process input through the Titans block.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, sequence_length, embedding_dim)\n",
    "            \n",
    "        Returns:\n",
    "            output: Processed tensor of same shape\n",
    "            gate_mean: Average gate value (for monitoring)\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Step 1: Create causal mask (can't look at future tokens!)\n",
    "        causal_mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(device)\n",
    "        \n",
    "        # Step 2: Get attention output (short-term memory)\n",
    "        attn_out, _ = self.attention(x, x, x, attn_mask=causal_mask, is_causal=True)\n",
    "        \n",
    "        # Step 3: Get memory output (long-term memory)\n",
    "        memory_out = self.long_term_memory(x)\n",
    "        \n",
    "        # Step 4: Compute gate (learned mixing)\n",
    "        combined = torch.cat([attn_out, memory_out], dim=-1)\n",
    "        gate_score = torch.sigmoid(self.gate(combined))  # Between 0 and 1\n",
    "        \n",
    "        # Step 5: Mix the two memory types\n",
    "        # gate_score * attention + (1 - gate_score) * memory\n",
    "        mixed = (gate_score * attn_out) + ((1 - gate_score) * memory_out)\n",
    "        \n",
    "        # Step 6: Residual connection + Layer Norm\n",
    "        x = self.ln1(x + mixed)\n",
    "        \n",
    "        # Step 7: Feed-forward + Residual + Layer Norm\n",
    "        x = self.ln2(x + self.ffwd(x))\n",
    "        \n",
    "        return x, gate_score.mean().item()\n",
    "\n",
    "# Test the Titans Block\n",
    "test_block = TitansBlock(n_embd, n_head=4).to(device).to(dtype)\n",
    "test_input = torch.randn(2, 16, n_embd, device=device, dtype=dtype)\n",
    "test_output, gate_val = test_block(test_input)\n",
    "\n",
    "print(\"[BUILD] Titans Block Created!\")\n",
    "print(f\"   â€¢ Input shape:  {test_input.shape}\")\n",
    "print(f\"   â€¢ Output shape: {test_output.shape}\")\n",
    "print(f\"   â€¢ Gate value:   {gate_val:.3f} (0=memory, 1=attention)\")\n",
    "print(f\"   â€¢ Parameters:   {sum(p.numel() for p in test_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033e2fe",
   "metadata": {},
   "source": [
    "### ðŸ¤– Step 3.6: Building the Complete TitansGPT Model\n",
    "\n",
    "Now we stack multiple Titans blocks together to create our full language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c235792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3.6: Build the Complete TitansGPT Model\n",
    "# ============================================================\n",
    "\n",
    "class TitansGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Titans Language Model.\n",
    "    \n",
    "    Architecture:\n",
    "        Token IDs -> Embedding -> Position Embedding -> Titans Blocks -> Output\n",
    "        \n",
    "    The model predicts the NEXT character given the previous characters.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_embd=128, n_head=4, n_layer=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.n_embd = n_embd\n",
    "        self.n_layer = n_layer\n",
    "        \n",
    "        # ===== EMBEDDINGS =====\n",
    "        # Token embedding: converts token ID to vector\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        \n",
    "        # Position embedding: tells the model WHERE each token is\n",
    "        self.position_embedding = nn.Embedding(1024, n_embd)  # Max 1024 tokens\n",
    "        \n",
    "        # ===== TITANS BLOCKS =====\n",
    "        # Stack multiple Titans blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TitansBlock(n_embd, n_head) for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        # ===== OUTPUT =====\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        # Language modeling head: converts embeddings to vocabulary probabilities\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None, curriculum_surprise_weight=1.0):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Input token IDs, shape (batch, sequence_length)\n",
    "            targets: Target token IDs for computing loss\n",
    "            curriculum_surprise_weight: MIRAS weight for surprise-based learning\n",
    "            \n",
    "        Returns:\n",
    "            logits: Prediction scores for each token\n",
    "            loss: Training loss (if targets provided)\n",
    "            gate_means: Average gate values from each block\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Step 1: Get token embeddings\n",
    "        tok_emb = self.token_embedding(idx)  # (B, T, n_embd)\n",
    "        \n",
    "        # Step 2: Get position embeddings\n",
    "        positions = torch.arange(T, device=device)\n",
    "        pos_emb = self.position_embedding(positions)  # (T, n_embd)\n",
    "        \n",
    "        # Step 3: Combine token + position embeddings\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        # Step 4: Pass through Titans blocks\n",
    "        gate_means = []\n",
    "        for block in self.blocks:\n",
    "            x, gate_mean = block(x)\n",
    "            gate_means.append(gate_mean)\n",
    "        \n",
    "        # Step 5: Final layer norm\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Step 6: Project to vocabulary\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        # Step 7: Compute loss (if training)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B * T, C)\n",
    "            targets_flat = targets.view(B * T)\n",
    "            \n",
    "            # === MIRAS: Surprise-Weighted Learning ===\n",
    "            # Compute per-token loss\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat, reduction='none')\n",
    "            \n",
    "            # Weight by surprise (harder examples get higher weight)\n",
    "            loss = (loss * curriculum_surprise_weight).mean()\n",
    "\n",
    "        return logits, loss, gate_means\n",
    "\n",
    "# ===== CREATE THE MODEL =====\n",
    "model = TitansGPT(vocab_size).to(device).to(dtype)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"ðŸ¤– TitansGPT Model Created!\")\n",
    "print(f\"   â€¢ Vocabulary size: {vocab_size}\")\n",
    "print(f\"   â€¢ Embedding dim:   {model.n_embd}\")\n",
    "print(f\"   â€¢ Number of layers: {model.n_layer}\")\n",
    "print(f\"   â€¢ Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"   â€¢ Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1d990",
   "metadata": {},
   "source": [
    "### [CHART] Step 3.7: Visualize the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fa284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3.7: Visualize Model Architecture & Parameter Distribution\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 4))\n",
    "\n",
    "# Plot 1: Parameter distribution by component\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Count parameters by component\n",
    "param_counts = {\n",
    "    'Token Embedding': model.token_embedding.weight.numel(),\n",
    "    'Position Embedding': model.position_embedding.weight.numel(),\n",
    "    'Titans Blocks': sum(p.numel() for block in model.blocks for p in block.parameters()),\n",
    "    'Final LayerNorm': sum(p.numel() for p in model.ln_f.parameters()),\n",
    "    'LM Head': sum(p.numel() for p in model.lm_head.parameters()),\n",
    "}\n",
    "\n",
    "components = list(param_counts.keys())\n",
    "counts = [param_counts[c] for c in components]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "\n",
    "bars = ax1.barh(components, counts, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_xlabel('Number of Parameters', fontsize=12)\n",
    "ax1.set_title('Parameter Distribution by Component', fontsize=14)\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax1.text(bar.get_width() + 1000, bar.get_y() + bar.get_height()/2, \n",
    "             f'{count:,}', va='center', fontsize=10)\n",
    "\n",
    "ax1.set_xlim(0, max(counts) * 1.25)\n",
    "\n",
    "# Plot 2: Model Architecture Diagram\n",
    "ax2 = axes[1]\n",
    "ax2.axis('off')\n",
    "\n",
    "# Draw architecture - WIDE boxes, SHORT height, LARGE spacing for arrows\n",
    "# Format: (name, x_position, color, detail)\n",
    "components_viz = [\n",
    "    ('Input\\nTokens', 0, '#E8F4FD', 'Token IDs'),\n",
    "    ('Token\\nEmbedding', 2.2, '#FFE4E1', f'{vocab_size}->{n_embd}'),\n",
    "    ('+', 4.1, 'white', ''),\n",
    "    ('Position\\nEmbedding', 4.7, '#E8F4FD', f'pos->{n_embd}'),\n",
    "    ('Titans\\nBlock x2', 6.9, '#E8F8E8', 'Attn + Memory'),\n",
    "    ('Layer\\nNorm', 9.1, '#FFF8DC', 'Normalize'),\n",
    "    ('LM Head', 11.3, '#E6E6FA', f'{n_embd}->{vocab_size}'),\n",
    "]\n",
    "\n",
    "y_center = 0.5\n",
    "box_height = 0.35  # SHORT boxes\n",
    "box_width = 1.6    # WIDE boxes\n",
    "\n",
    "for i, (name, x, color, detail) in enumerate(components_viz):\n",
    "    if name == '+':\n",
    "        ax2.text(x, y_center, '+', fontsize=28, ha='center', va='center', fontweight='bold')\n",
    "    else:\n",
    "        rect = plt.Rectangle((x, y_center - box_height/2), \n",
    "                             box_width, box_height, \n",
    "                             facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(x + box_width/2, y_center + 0.05, name, ha='center', va='center', \n",
    "                fontsize=9, fontweight='bold')\n",
    "        ax2.text(x + box_width/2, y_center - 0.1, detail, ha='center', va='center', \n",
    "                fontsize=8, color='gray')\n",
    "    \n",
    "    # Draw arrows between components (skip + symbol)\n",
    "    if i < len(components_viz) - 1 and name != '+' and components_viz[i+1][0] != '+':\n",
    "        next_x = components_viz[i+1][1]\n",
    "        arrow_start = x + box_width + 0.05\n",
    "        arrow_end = next_x - 0.05\n",
    "        # Draw visible arrow with sufficient length\n",
    "        ax2.annotate('', xy=(arrow_end, y_center),\n",
    "                    xytext=(arrow_start, y_center),\n",
    "                    arrowprops=dict(arrowstyle='->', color='#FF5722', lw=2.5, \n",
    "                                   mutation_scale=15))\n",
    "\n",
    "ax2.set_xlim(-0.5, 13.5)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_title('TitansGPT Architecture Flow', fontsize=14, pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/model_architecture.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[TIP] Model Summary:\")\n",
    "print(f\"   Total: {total_params:,} parameters\")\n",
    "print(f\"   That's {total_params/1e6:.2f} million numbers the model learns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26a66e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ“ Part 4: Training with MIRAS\n",
    "\n",
    "## What is Training?\n",
    "\n",
    "Training is how neural networks **learn**. It's a loop:\n",
    "\n",
    "```\n",
    "1. FORWARD PASS:  Input -> Model -> Prediction\n",
    "2. LOSS:          Compare prediction vs actual -> How wrong were we?\n",
    "3. BACKWARD PASS: Calculate gradients (which direction to adjust weights)\n",
    "4. UPDATE:        Adjust weights slightly in the right direction\n",
    "5. REPEAT:        Do this millions of times!\n",
    "```\n",
    "\n",
    "## ðŸŽ¯ What is MIRAS?\n",
    "\n",
    "**MIRAS** (Memory-Integrated Retrieval-Augmented Surprise) is a training strategy:\n",
    "\n",
    "> **\"Focus more on SURPRISING examples\"**\n",
    "\n",
    "Think of it like studying:\n",
    "- Easy problems -> Quick review, low weight\n",
    "- Hard problems -> Study harder, high weight\n",
    "\n",
    "```\n",
    "Surprise = How wrong was the prediction?\n",
    "\n",
    "High surprise -> \"This is new/hard, learn it more!\"\n",
    "Low surprise  -> \"I already know this, quick review\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ceecb",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Step 4.1: Setting Up the Data Loader\n",
    "\n",
    "We need a function that gives us **batches** of training data. \n",
    "\n",
    "Why batches?\n",
    "- Processing one example at a time is slow\n",
    "- Processing ALL data at once needs too much memory\n",
    "- Batches are the sweet spot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4.1: Training Hyperparameters & Data Loader\n",
    "# ============================================================\n",
    "\n",
    "# === HYPERPARAMETERS ===\n",
    "# These are the \"knobs\" we can tune to affect training\n",
    "\n",
    "batch_size = 32      # How many sequences to process at once\n",
    "block_size = 128     # Length of each sequence (context window)\n",
    "max_iters = 1000     # Total training steps\n",
    "eval_interval = 100  # How often to evaluate\n",
    "learning_rate = 3e-4 # How big each weight update is (0.0003)\n",
    "\n",
    "print(\"âš™ï¸ Training Hyperparameters:\")\n",
    "print(f\"   â€¢ Batch size:     {batch_size} sequences per step\")\n",
    "print(f\"   â€¢ Block size:     {block_size} tokens per sequence\")\n",
    "print(f\"   â€¢ Max iterations: {max_iters}\")\n",
    "print(f\"   â€¢ Learning rate:  {learning_rate}\")\n",
    "\n",
    "# === OPTIMIZER ===\n",
    "# AdamW is a popular optimizer that adapts learning rate per parameter\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# === DATA LOADER FUNCTION ===\n",
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    Get a random batch of training or validation data.\n",
    "    \n",
    "    Args:\n",
    "        split: 'train' or 'val'\n",
    "        \n",
    "    Returns:\n",
    "        x: Input tokens (batch_size, block_size)\n",
    "        y: Target tokens (batch_size, block_size) - shifted by 1!\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Pick random starting positions\n",
    "    random_starts = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Create input (x) and target (y) sequences\n",
    "    # Target is input shifted by 1 position (predict next character!)\n",
    "    x = torch.stack([data[i:i + block_size] for i in random_starts])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in random_starts])\n",
    "    \n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# Test the data loader\n",
    "sample_x, sample_y = get_batch('train')\n",
    "print(f\"\\nðŸ“¦ Sample Batch:\")\n",
    "print(f\"   â€¢ Input shape:  {sample_x.shape}\")\n",
    "print(f\"   â€¢ Target shape: {sample_y.shape}\")\n",
    "print(f\"\\n   Example (first 20 tokens of first sequence):\")\n",
    "print(f\"   Input:  '{decode(sample_x[0, :20].tolist())}'\")\n",
    "print(f\"   Target: '{decode(sample_y[0, :20].tolist())}'\")\n",
    "print(f\"\\n   Notice: Target is shifted by 1 character!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4287f3da",
   "metadata": {},
   "source": [
    "### [CHART] Step 4.2: Visualize How Batching Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4.2: Visualize Input/Target Relationship\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 6))\n",
    "\n",
    "# Show input-target relationship\n",
    "example_text = \"ROMEO: To be\"\n",
    "example_input = example_text[:-1]   # \"ROMEO: To b\"\n",
    "example_target = example_text[1:]   # \"OMEO: To be\"\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.axis('off')\n",
    "ax1.set_title('How Training Works: Predict the Next Character', fontsize=14)\n",
    "\n",
    "# Draw input sequence\n",
    "y_input = 0.7\n",
    "y_target = 0.3\n",
    "for i, char in enumerate(example_input):\n",
    "    rect = plt.Rectangle((i * 0.08 + 0.05, y_input - 0.08), 0.07, 0.16, \n",
    "                          facecolor='#E3F2FD', edgecolor='#2196F3', linewidth=2)\n",
    "    ax1.add_patch(rect)\n",
    "    display_char = char if char != ' ' else '_'\n",
    "    ax1.text(i * 0.08 + 0.085, y_input, display_char, ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Draw target sequence\n",
    "for i, char in enumerate(example_target):\n",
    "    rect = plt.Rectangle((i * 0.08 + 0.05, y_target - 0.08), 0.07, 0.16, \n",
    "                          facecolor='#E8F5E9', edgecolor='#4CAF50', linewidth=2)\n",
    "    ax1.add_patch(rect)\n",
    "    display_char = char if char != ' ' else '_'\n",
    "    ax1.text(i * 0.08 + 0.085, y_target, display_char, ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Draw arrows\n",
    "for i in range(len(example_input)):\n",
    "    ax1.annotate('', xy=(i * 0.08 + 0.085, y_target + 0.1), \n",
    "                xytext=(i * 0.08 + 0.085, y_input - 0.1),\n",
    "                arrowprops=dict(arrowstyle='->', color='#FF5722', lw=1.5))\n",
    "\n",
    "ax1.text(0.02, y_input, 'Input:', ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "ax1.text(0.02, y_target, 'Target:', ha='right', va='center', fontsize=12, fontweight='bold')\n",
    "ax1.text(0.5, 0.05, 'Each input position predicts the NEXT character', \n",
    "         ha='center', fontsize=11, style='italic')\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Visualize batch concept\n",
    "ax2 = axes[1]\n",
    "ax2.axis('off')\n",
    "ax2.set_title(f'Batch: {batch_size} Sequences Processed in Parallel', fontsize=14)\n",
    "\n",
    "# Draw batch visualization (show first 4 sequences)\n",
    "for seq_idx in range(4):\n",
    "    y = 0.8 - seq_idx * 0.2\n",
    "    ax2.text(0.02, y, f'Seq {seq_idx+1}:', ha='right', va='center', fontsize=10)\n",
    "    \n",
    "    # Get a sample sequence\n",
    "    seq_text = decode(sample_x[seq_idx, :15].tolist())\n",
    "    for i, char in enumerate(seq_text):\n",
    "        color = plt.cm.Set3(seq_idx / 4)\n",
    "        rect = plt.Rectangle((i * 0.05 + 0.05, y - 0.06), 0.045, 0.12, \n",
    "                              facecolor=color, edgecolor='gray', linewidth=1)\n",
    "        ax2.add_patch(rect)\n",
    "        display_char = char if char not in ' \\n' else '.'\n",
    "        ax2.text(i * 0.05 + 0.0725, y, display_char, ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    ax2.text(0.82, y, '...', fontsize=12)\n",
    "    ax2.text(0.88, y, f'({block_size} tokens)', fontsize=9, color='gray')\n",
    "\n",
    "ax2.text(0.5, 0.02, f'All {batch_size} sequences are processed simultaneously on GPU!', \n",
    "         ha='center', fontsize=11, style='italic', color='#1976D2')\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/batching_visual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[TIP] Key Insight:\")\n",
    "print(f\"   Each training step processes {batch_size} x {block_size} = {batch_size * block_size:,} tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ea2fb",
   "metadata": {},
   "source": [
    "### [GRAPH] Step 4.3: The Training Loop\n",
    "\n",
    "Now let's train our model! This is where the magic happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4.3: Evaluation Function\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()  # Don't compute gradients (saves memory)\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Evaluate model on both training and validation sets.\n",
    "    \n",
    "    Returns average loss over many batches for reliable estimates.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(200)  # Collect 200 samples\n",
    "        for k in range(200):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss, _ = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    \n",
    "    model.train()  # Set model back to training mode\n",
    "    return out\n",
    "\n",
    "# Test evaluation\n",
    "print(\"ðŸ§ª Testing evaluation function...\")\n",
    "initial_losses = estimate_loss()\n",
    "print(f\"   â€¢ Initial train loss: {initial_losses['train']:.4f}\")\n",
    "print(f\"   â€¢ Initial val loss:   {initial_losses['val']:.4f}\")\n",
    "print(f\"\\n   (High loss is expected before training!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac6b15d",
   "metadata": {},
   "source": [
    "### ðŸš€ Step 4.4: Run Training with MIRAS\n",
    "\n",
    "Now we train! Watch the loss decrease over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4.4: The Main Training Loop (MIRAS-style)\n",
    "# ============================================================\n",
    "\n",
    "# Storage for training metrics\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "surprise_history = []  # Track gate usage\n",
    "train_losses_at_eval = []\n",
    "val_losses_at_eval = []\n",
    "eval_iters = []\n",
    "\n",
    "print(f\"ðŸš€ Starting Training on {device}...\")\n",
    "print(f\"   This will take a few minutes. Watch the loss decrease!\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "pbar = tqdm(range(max_iters), desc=\"Training\")\n",
    "\n",
    "for iter in pbar:\n",
    "    \n",
    "    # ===== STEP 1: Get a batch of data =====\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # ===== STEP 2: MIRAS Surprise Weighting =====\n",
    "    # Every 100 steps, we \"surprise\" the model with higher weight\n",
    "    # This simulates MIRAS's focus on surprising examples\n",
    "    if iter % 100 == 0:\n",
    "        surprise_weight = 2.0  # \"Pay extra attention!\"\n",
    "    else:\n",
    "        surprise_weight = 1.0  # Normal training\n",
    "    \n",
    "    # ===== STEP 3: Forward Pass =====\n",
    "    logits, loss, gate_means = model(xb, yb, curriculum_surprise_weight=surprise_weight)\n",
    "    \n",
    "    # ===== STEP 4: Backward Pass =====\n",
    "    optimizer.zero_grad(set_to_none=True)  # Clear old gradients\n",
    "    loss.backward()                         # Compute new gradients\n",
    "    optimizer.step()                        # Update weights\n",
    "    \n",
    "    # ===== STEP 5: Logging =====\n",
    "    loss_history.append(loss.item())\n",
    "    surprise_history.append(gate_means[0])  # Track layer 0 gate\n",
    "    \n",
    "    # ===== STEP 6: Periodic Evaluation =====\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        train_losses_at_eval.append(losses['train'])\n",
    "        val_losses_at_eval.append(losses['val'])\n",
    "        eval_iters.append(iter)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_description(\n",
    "            f\"Train: {losses['train']:.4f} | Val: {losses['val']:.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'iter': iter,\n",
    "            'train_loss': losses['train'],\n",
    "            'val_loss': losses['val'],\n",
    "        }\n",
    "        torch.save(checkpoint, f\"artifacts/ckpt_{iter}.pt\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n[OK] Training Complete!\")\n",
    "print(f\"   Total time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"   Final train loss: {train_losses_at_eval[-1]:.4f}\")\n",
    "print(f\"   Final val loss:   {val_losses_at_eval[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5960573",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# [CHART] Part 5: Analyzing Training Results\n",
    "\n",
    "Let's visualize what happened during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d485ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5.1: Comprehensive Training Visualization\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Training Loss Over Time\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(loss_history, alpha=0.3, color='blue', label='Per-step loss')\n",
    "# Smooth the loss for better visualization\n",
    "window = 50\n",
    "if len(loss_history) > window:\n",
    "    smoothed = np.convolve(loss_history, np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(range(window-1, len(loss_history)), smoothed, color='blue', \n",
    "             linewidth=2, label=f'Smoothed (window={window})')\n",
    "ax1.set_xlabel('Training Step', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Over Time', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Train vs Validation Loss\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(eval_iters, train_losses_at_eval, 'b-o', label='Training', linewidth=2, markersize=8)\n",
    "ax2.plot(eval_iters, val_losses_at_eval, 'r-s', label='Validation', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Training Step', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.set_title('Training vs Validation Loss', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for overfitting check\n",
    "if len(val_losses_at_eval) > 1:\n",
    "    if val_losses_at_eval[-1] > val_losses_at_eval[-2]:\n",
    "        ax2.annotate('[!] Possible overfitting', \n",
    "                    xy=(eval_iters[-1], val_losses_at_eval[-1]),\n",
    "                    xytext=(eval_iters[-1]-200, val_losses_at_eval[-1]+0.2),\n",
    "                    fontsize=10, color='red',\n",
    "                    arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Plot 3: Gate Usage Over Time\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(surprise_history, color='orange', alpha=0.5)\n",
    "if len(surprise_history) > window:\n",
    "    smoothed_gate = np.convolve(surprise_history, np.ones(window)/window, mode='valid')\n",
    "    ax3.plot(range(window-1, len(surprise_history)), smoothed_gate, \n",
    "             color='darkorange', linewidth=2, label=f'Smoothed')\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Equal mixing')\n",
    "ax3.set_xlabel('Training Step', fontsize=12)\n",
    "ax3.set_ylabel('Gate Value', fontsize=12)\n",
    "ax3.set_title('Titans Gate: Memory vs Attention Balance', fontsize=14)\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "ax3.text(0.02, 0.98, '<- More Memory', transform=ax3.transAxes, \n",
    "         fontsize=10, va='top', color='purple')\n",
    "ax3.text(0.02, 0.02, '<- More Attention', transform=ax3.transAxes, \n",
    "         fontsize=10, va='bottom', color='green')\n",
    "\n",
    "# Plot 4: Loss Improvement\n",
    "ax4 = axes[1, 1]\n",
    "improvements = [train_losses_at_eval[0] - l for l in train_losses_at_eval]\n",
    "ax4.bar(eval_iters, improvements, color='#4CAF50', edgecolor='black', width=80)\n",
    "ax4.set_xlabel('Training Step', fontsize=12)\n",
    "ax4.set_ylabel('Loss Improvement', fontsize=12)\n",
    "ax4.set_title('Cumulative Loss Reduction', fontsize=14)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add total improvement annotation\n",
    "total_improvement = train_losses_at_eval[0] - train_losses_at_eval[-1]\n",
    "ax4.text(0.95, 0.95, f'Total improvement: {total_improvement:.3f}', \n",
    "         transform=ax4.transAxes, fontsize=11, ha='right', va='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/training_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[TIP] Training Analysis:\")\n",
    "print(f\"   â€¢ Loss decreased from {train_losses_at_eval[0]:.4f} to {train_losses_at_eval[-1]:.4f}\")\n",
    "print(f\"   â€¢ Total improvement: {total_improvement:.4f} ({100*total_improvement/train_losses_at_eval[0]:.1f}% reduction)\")\n",
    "print(f\"   â€¢ Average gate value: {np.mean(surprise_history):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f14802d",
   "metadata": {},
   "source": [
    "### [GRAPH] Step 5.2: Sample Generation During Training\n",
    "\n",
    "Let's see how the model's output quality improved during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5.2: Compare Model Outputs at Different Training Stages\n",
    "# ============================================================\n",
    "\n",
    "# Define generate function (used here and in Part 6)\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt_tokens, max_new_tokens, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained TitansGPT model\n",
    "        prompt_tokens: Starting token IDs, shape (1, seq_len)\n",
    "        max_new_tokens: How many new characters to generate\n",
    "        temperature: Controls randomness (lower = more predictable)\n",
    "        \n",
    "    Returns:\n",
    "        Full sequence including prompt and generated tokens\n",
    "    \"\"\"\n",
    "    idx = prompt_tokens\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop to last block_size tokens (our model's context window)\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        \n",
    "        # Get model predictions\n",
    "        logits, _, _ = model(idx_cond)\n",
    "        \n",
    "        # Focus on the last position only\n",
    "        logits = logits[:, -1, :]  # (batch, vocab_size)\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append to sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "# Load checkpoints and compare outputs\n",
    "print(\"[CHART] Model Evolution: Comparing outputs at different training stages\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checkpoints_to_compare = [0, 500, 900]  # Beginning, middle, end\n",
    "prompt = \"ROMEO:\"\n",
    "prompt_tokens = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "\n",
    "for ckpt_iter in checkpoints_to_compare:\n",
    "    ckpt_path = f\"artifacts/ckpt_{ckpt_iter}.pt\"\n",
    "    \n",
    "    if os.path.exists(ckpt_path):\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        model.eval()\n",
    "        \n",
    "        # Generate sample\n",
    "        generated = generate(model, prompt_tokens.clone(), max_new_tokens=100, temperature=0.8)\n",
    "        output_text = decode(generated[0].tolist())\n",
    "        \n",
    "        train_loss = checkpoint.get('train_loss', None)\n",
    "        loss_str = f\"{train_loss:.4f}\" if isinstance(train_loss, (int, float)) else \"N/A\"\n",
    "        print(f\"\\nðŸ“ Step {ckpt_iter} (Loss: {loss_str}):\")\n",
    "        print(f\"   {output_text}\")\n",
    "        print(\"-\"*70)\n",
    "    else:\n",
    "        print(f\"\\n[!] Checkpoint {ckpt_path} not found\")\n",
    "\n",
    "# Reload the final checkpoint\n",
    "final_ckpt = f\"artifacts/ckpt_{max(eval_iters)}.pt\"\n",
    "if os.path.exists(final_ckpt):\n",
    "    checkpoint = torch.load(final_ckpt, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.train()\n",
    "    \n",
    "print(\"\\n[OK] Final model restored!\")\n",
    "print(\"\\n[TIP] Notice how the text becomes more coherent as training progresses!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70bc2fe",
   "metadata": {},
   "source": [
    "### [BRAIN] Step 5.3: Understanding What the Model Learned\n",
    "\n",
    "Let's peek inside the model to see what it learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5.3: Analyze What the Model Learned\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Token Embedding Similarity (after training)\n",
    "ax1 = axes[0, 0]\n",
    "sample_chars = ['A', 'a', 'B', 'b', 'O', 'o', ' ', '.', '!', '\\n']\n",
    "sample_ids = [stoi[c] for c in sample_chars if c in stoi]\n",
    "sample_labels = [repr(c) if c in ' \\n' else f\"'{c}'\" for c in sample_chars if c in stoi]\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model.token_embedding.weight[sample_ids].float().cpu()\n",
    "    emb_norm = F.normalize(embeddings, dim=1)\n",
    "    similarity = (emb_norm @ emb_norm.T).numpy()\n",
    "\n",
    "im = ax1.imshow(similarity, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "ax1.set_xticks(range(len(sample_labels)))\n",
    "ax1.set_yticks(range(len(sample_labels)))\n",
    "ax1.set_xticklabels(sample_labels, fontsize=9)\n",
    "ax1.set_yticklabels(sample_labels, fontsize=9)\n",
    "ax1.set_title('Character Similarity (After Training)', fontsize=12)\n",
    "plt.colorbar(im, ax=ax1, label='Cosine Similarity')\n",
    "\n",
    "# Add values\n",
    "for i in range(len(sample_labels)):\n",
    "    for j in range(len(sample_labels)):\n",
    "        color = 'white' if abs(similarity[i,j]) > 0.5 else 'black'\n",
    "        ax1.text(j, i, f'{similarity[i,j]:.1f}', ha='center', va='center', fontsize=8, color=color)\n",
    "\n",
    "# Plot 2: Most similar character pairs\n",
    "ax2 = axes[0, 1]\n",
    "# Find most similar pairs (excluding self-similarity)\n",
    "all_chars = list(stoi.keys())[:40]  # First 40 characters\n",
    "all_ids = [stoi[c] for c in all_chars]\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_emb = model.token_embedding.weight[all_ids].float().cpu()\n",
    "    all_norm = F.normalize(all_emb, dim=1)\n",
    "    all_sim = (all_norm @ all_norm.T).numpy()\n",
    "\n",
    "# Get top similar pairs\n",
    "pairs = []\n",
    "for i in range(len(all_chars)):\n",
    "    for j in range(i+1, len(all_chars)):\n",
    "        pairs.append((all_sim[i,j], all_chars[i], all_chars[j]))\n",
    "\n",
    "pairs.sort(reverse=True)\n",
    "top_pairs = pairs[:10]\n",
    "\n",
    "pair_labels = [f\"'{p[1]}' & '{p[2]}'\" if p[1] not in '\\n\\t ' and p[2] not in '\\n\\t ' \n",
    "               else f\"{repr(p[1])} & {repr(p[2])}\" for p in top_pairs]\n",
    "pair_sims = [p[0] for p in top_pairs]\n",
    "\n",
    "colors = plt.cm.Greens(np.linspace(0.3, 0.9, len(pair_labels)))\n",
    "ax2.barh(range(len(pair_labels)), pair_sims, color=colors)\n",
    "ax2.set_yticks(range(len(pair_labels)))\n",
    "ax2.set_yticklabels(pair_labels, fontsize=10)\n",
    "ax2.set_xlabel('Similarity Score', fontsize=11)\n",
    "ax2.set_title('Top 10 Most Similar Character Pairs', fontsize=12)\n",
    "ax2.set_xlim(0, 1)\n",
    "\n",
    "# Plot 3: Next character prediction heatmap\n",
    "ax3 = axes[1, 0]\n",
    "test_contexts = ['ROMEO', 'the ', 'is a', 'What']\n",
    "next_char_probs = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for ctx in test_contexts:\n",
    "        tokens = torch.tensor([encode(ctx)], dtype=torch.long, device=device)\n",
    "        logits, _, _ = model(tokens)\n",
    "        probs = F.softmax(logits[0, -1, :].float(), dim=0).cpu().numpy()\n",
    "        # Get top 5 predictions\n",
    "        top_indices = probs.argsort()[-5:][::-1]\n",
    "        top_probs = probs[top_indices]\n",
    "        top_chars = [itos[i] for i in top_indices]\n",
    "        next_char_probs.append((ctx, list(zip(top_chars, top_probs))))\n",
    "\n",
    "# Display predictions\n",
    "ax3.axis('off')\n",
    "ax3.set_title('Next Character Predictions', fontsize=12)\n",
    "y_start = 0.9\n",
    "for ctx, predictions in next_char_probs:\n",
    "    pred_str = ', '.join([f\"'{c}'({p:.2f})\" if c not in '\\n\\t ' else f\"{repr(c)}({p:.2f})\" \n",
    "                          for c, p in predictions])\n",
    "    ax3.text(0.02, y_start, f\"After '{ctx}':\", fontsize=11, fontweight='bold', \n",
    "             transform=ax3.transAxes)\n",
    "    ax3.text(0.02, y_start - 0.06, f\"  -> {pred_str}\", fontsize=10, \n",
    "             transform=ax3.transAxes)\n",
    "    y_start -= 0.18\n",
    "\n",
    "# Plot 4: Summary stats\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "ax4.set_title('Training Summary', fontsize=12)\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "[CHART] MODEL STATISTICS\n",
    "--------------------------\n",
    "Total Parameters: {sum(p.numel() for p in model.parameters()):,}\n",
    "Training Steps:   {max_iters}\n",
    "Final Train Loss: {train_losses_at_eval[-1]:.4f}\n",
    "Final Val Loss:   {val_losses_at_eval[-1]:.4f}\n",
    "\n",
    "[GRAPH] IMPROVEMENT\n",
    "--------------------------\n",
    "Loss Reduction:   {train_losses_at_eval[0] - train_losses_at_eval[-1]:.4f}\n",
    "Improvement:      {100*(train_losses_at_eval[0] - train_losses_at_eval[-1])/train_losses_at_eval[0]:.1f}%\n",
    "\n",
    "[BRAIN] TITANS FEATURES\n",
    "--------------------------\n",
    "Avg Gate Value:   {np.mean(surprise_history):.3f}\n",
    "Memory Usage:     {100*(1-np.mean(surprise_history)):.1f}%\n",
    "Attention Usage:  {100*np.mean(surprise_history):.1f}%\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.1, 0.9, summary_text, fontsize=11, family='monospace',\n",
    "         verticalalignment='top', transform=ax4.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/learning_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n[TIP] Key Insights:\")\n",
    "print(\"   â€¢ The model learned character similarities (A~a, upper/lowercase)\")\n",
    "print(\"   â€¢ It can predict likely next characters based on context\")\n",
    "print(\"   â€¢ The Titans gate learns to balance memory types!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44609765",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# [CHAT] Part 6: Text Generation & Inference\n",
    "\n",
    "Now for the fun part - let's see our model generate text!\n",
    "\n",
    "## How Does Generation Work?\n",
    "\n",
    "```\n",
    "1. Give model a \"prompt\" (starting text)\n",
    "2. Model predicts probabilities for next character\n",
    "3. Sample from probabilities (pick a character)\n",
    "4. Add new character to prompt\n",
    "5. Repeat steps 2-4!\n",
    "```\n",
    "\n",
    "This is called **autoregressive generation** - the model uses its own outputs as inputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d766a7b3",
   "metadata": {},
   "source": [
    "### ðŸŽ² Step 6.1: The Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb30daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6.1: Text Generation Function\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, prompt_tokens, max_new_tokens, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text from the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained TitansGPT model\n",
    "        prompt_tokens: Starting token IDs, shape (1, seq_len)\n",
    "        max_new_tokens: How many new characters to generate\n",
    "        temperature: Controls randomness (lower = more predictable)\n",
    "        \n",
    "    Returns:\n",
    "        Full sequence including prompt and generated tokens\n",
    "    \"\"\"\n",
    "    idx = prompt_tokens\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop to last block_size tokens (our model's context window)\n",
    "        idx_cond = idx[:, -block_size:]\n",
    "        \n",
    "        # Get model predictions\n",
    "        logits, _, _ = model(idx_cond)\n",
    "        \n",
    "        # Focus on the last position only\n",
    "        logits = logits[:, -1, :]  # (batch, vocab_size)\n",
    "        \n",
    "        # Apply temperature\n",
    "        # Higher temperature = more random, lower = more focused\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append to sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "# Test generation with different prompts\n",
    "print(\"ðŸŽ­ Text Generation Examples:\\n\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not\",\n",
    "    \"The king\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    prompt_tokens = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "    \n",
    "    generated = generate(model, prompt_tokens, max_new_tokens=100, temperature=0.8)\n",
    "    generated_text = decode(generated[0].tolist())\n",
    "    \n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b74c1",
   "metadata": {},
   "source": [
    "### [TEMP] Step 6.2: Understanding Temperature\n",
    "\n",
    "**Temperature** controls how \"creative\" the model is:\n",
    "- `temperature = 0.1`: Very predictable, always picks top choice\n",
    "- `temperature = 1.0`: Balanced, follows probability distribution\n",
    "- `temperature = 2.0`: Very random, might pick unlikely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd48c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6.2: Visualize Temperature Effect\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Show how temperature affects probability distribution\n",
    "example_logits = torch.tensor([2.0, 1.5, 1.0, 0.5, 0.0, -0.5])  # Raw model output\n",
    "labels = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "\n",
    "temperatures = [0.3, 1.0, 2.0]\n",
    "titles = ['Low Temp (0.3)\\n\"Focused\"', 'Normal (1.0)\\n\"Balanced\"', 'High Temp (2.0)\\n\"Creative\"']\n",
    "\n",
    "for ax, temp, title in zip(axes, temperatures, titles):\n",
    "    scaled_logits = example_logits / temp\n",
    "    probs = F.softmax(scaled_logits, dim=0).numpy()\n",
    "    \n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(probs)))\n",
    "    bars = ax.bar(labels, probs, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    ax.set_ylabel('Probability' if ax == axes[0] else '')\n",
    "    ax.set_xlabel('Next Token Options')\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add probability labels\n",
    "    for bar, prob in zip(bars, probs):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "               f'{prob:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('How Temperature Affects Token Selection', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/temperature_visual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Generate with different temperatures\n",
    "print(\"\\n[TEMP] Same Prompt, Different Temperatures:\\n\")\n",
    "prompt = \"ROMEO:\"\n",
    "prompt_tokens = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "\n",
    "for temp in [0.3, 0.8, 1.5]:\n",
    "    generated = generate(model, prompt_tokens, max_new_tokens=80, temperature=temp)\n",
    "    print(f\"Temperature = {temp}:\")\n",
    "    print(f\"  {decode(generated[0].tolist())}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166c609",
   "metadata": {},
   "source": [
    "### [CHAT] Step 6.3: Multi-Turn Conversation Demo\n",
    "\n",
    "Let's simulate a conversation where the model maintains context across multiple turns. This demonstrates how the Titans architecture can remember and build upon previous context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf19ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6.3: Multi-Turn Conversation Demo\n",
    "# ============================================================\n",
    "\n",
    "def demonstrate_conversation(prompts, tokens_per_turn=100):\n",
    "    \"\"\"\n",
    "    Demonstrate multi-turn conversation with the model.\n",
    "    The model maintains context across turns.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompts to send to the model\n",
    "        tokens_per_turn: How many tokens to generate per turn\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"[CHAT] MULTI-TURN CONVERSATION DEMO\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Watching how the model maintains context across turns...\\n\")\n",
    "    \n",
    "    # Initialize context\n",
    "    context = torch.tensor([[stoi.get('\\n', 0)]], dtype=torch.long, device=device)\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"[NOTE] Turn {i} - Input: \\\"{prompt}\\\"\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Encode and add prompt to context\n",
    "        prompt_tokens = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "        context = torch.cat((context, prompt_tokens), dim=1)\n",
    "        \n",
    "        # Generate response\n",
    "        response = generate(model, context, max_new_tokens=tokens_per_turn, temperature=0.8)\n",
    "        \n",
    "        # Get only new tokens\n",
    "        new_tokens = response[0, context.shape[1]:].tolist()\n",
    "        response_text = decode(new_tokens)\n",
    "        \n",
    "        print(f\"ðŸ¤– Titans: {response_text}\")\n",
    "        print(f\"\\n   [Context length: {response.shape[1]} tokens]\")\n",
    "        print(\"=\" * 70 + \"\\n\")\n",
    "        \n",
    "        # Update context for next turn\n",
    "        context = response\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Demo 1: Shakespeare-style dialogue\n",
    "print(\"ðŸŽ­ DEMO 1: Shakespeare Dialogue Continuation\")\n",
    "shakespeare_prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"JULIET:\",\n",
    "    \"ROMEO: But soft,\",\n",
    "]\n",
    "_ = demonstrate_conversation(shakespeare_prompts)\n",
    "\n",
    "# Demo 2: Building a scene\n",
    "print(\"\\n\" + \"ðŸ° DEMO 2: Scene Building\")\n",
    "scene_prompts = [\n",
    "    \"Enter KING and QUEEN.\",\n",
    "    \"KING: My lord,\",\n",
    "    \"QUEEN: What news\",\n",
    "]\n",
    "_ = demonstrate_conversation(scene_prompts)\n",
    "\n",
    "# Demo 3: Completing famous lines\n",
    "print(\"\\n\" + \"ðŸ“œ DEMO 3: Famous Line Completions\")\n",
    "famous_prompts = [\n",
    "    \"To be or not to be\",\n",
    "    \"All the world's a stage\",\n",
    "    \"What light through yonder\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“œ COMPLETING FAMOUS SHAKESPEARE LINES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for prompt in famous_prompts:\n",
    "    prompt_tokens = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
    "    generated = generate(model, prompt_tokens, max_new_tokens=80, temperature=0.7)\n",
    "    output = decode(generated[0].tolist())\n",
    "    \n",
    "    print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "    print(f\"Completion: {output}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n[OK] Conversation demos complete!\")\n",
    "print(\"\\n[TIP] Notice how the model:\")\n",
    "print(\"   â€¢ Follows Shakespeare's writing style\")\n",
    "print(\"   â€¢ Maintains character dialogue format\")  \n",
    "print(\"   â€¢ Builds coherent (though not always perfect) text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8a806",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully built a **Titans + MIRAS** language model from scratch! \n",
    "\n",
    "## [NOTE] What You Learned\n",
    "\n",
    "| Topic | What You Learned |\n",
    "|-------|------------------|\n",
    "| **Data Pipeline** | How to tokenize text and create training batches |\n",
    "| **Embeddings** | Converting tokens to learnable vectors |\n",
    "| **Attention** | Short-term memory that focuses on recent context |\n",
    "| **Neural Memory** | Long-term memory that stores patterns in weights |\n",
    "| **Gating** | Learning when to trust which memory type |\n",
    "| **Training Loop** | Forward pass -> Loss -> Backward pass -> Update |\n",
    "| **MIRAS** | Focusing on surprising examples during training |\n",
    "| **Generation** | Autoregressive text generation with temperature |\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **Experiment with hyperparameters**: Try different `n_embd`, `n_layer`, `learning_rate`\n",
    "2. **Train longer**: Increase `max_iters` for better results\n",
    "3. **Try different data**: Use your own text corpus\n",
    "4. **Scale up**: Use a larger model with more layers\n",
    "5. **Read the paper**: Understand the full Titans architecture\n",
    "\n",
    "## [INFO] References\n",
    "\n",
    "* **Original Paper:** [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2501.00663)\n",
    "* **Google Research Blog:** [Titans + MIRAS: Helping AI have long-term memory](https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/)\n",
    "* **Video Explanation:** [YouTube - Titans Explained](https://www.youtube.com/watch?v=_WFgtK6K01g)\n",
    "* **Andrej Karpathy's nanoGPT:** [GitHub](https://github.com/karpathy/nanoGPT) - Inspiration for this tutorial\n",
    "\n",
    "---\n",
    "\n",
    "*This is an educational implementation inspired by the Titans paper. It is not the official Google implementation.*\n",
    "\n",
    "**Created with â¤ï¸ for AI/ML beginners**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
