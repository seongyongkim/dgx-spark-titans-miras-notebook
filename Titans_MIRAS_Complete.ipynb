{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65c195c",
   "metadata": {},
   "source": [
    "# Titans-MIRAS Hybrid Memory System\n",
    "A complete tutorial combining environment setup, memory architecture, hybrid engine, and demo chat app.\n",
    "\n",
    "## Contents\n",
    "1. **Environment Setup** — GPU and 4-bit quantization verification\n",
    "2. **Memory Architecture** — Neural Memory module with Surprise-driven learning\n",
    "3. **Hybrid Engine** — Integration with frozen LLM (GPT-2)\n",
    "4. **Demo Chat App** — Production-ready memory recall with confidence scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8cf695",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Environment Setup\n",
    "Verify GPU and 4-bit quantization readiness for the Titans-MIRAS hybrid memory demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b7bd6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 | packaged by conda-forge | (main, Oct 22 2025, 23:16:53) [GCC 14.3.0]\n",
      "Platform: Linux-6.14.0-1013-nvidia-aarch64-with-glibc2.39\n",
      "PyTorch available: True\n",
      "Torch version: 2.10.0+cu130\n"
     ]
    }
   ],
   "source": [
    "# Check Python version and system info\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch available:\", True)\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "except Exception as e:\n",
    "    print(\"PyTorch import failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af0c1aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GB10\n",
      "Compute capability: (12, 1)\n",
      "Total GPU memory (GB): 119.7\n",
      "GPU matmul ok: torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability and details\n",
    "import torch\n",
    "cuda_ok = torch.cuda.is_available()\n",
    "print(\"CUDA available:\", cuda_ok)\n",
    "if cuda_ok:\n",
    "    device = torch.device(\"cuda\")\n",
    "    idx = torch.cuda.current_device()\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(idx))\n",
    "    cap = torch.cuda.get_device_capability(idx)\n",
    "    print(\"Compute capability:\", cap)\n",
    "    total_mem = torch.cuda.get_device_properties(idx).total_memory\n",
    "    print(\"Total GPU memory (GB):\", round(total_mem/1024**3, 2))\n",
    "    x = torch.randn(1024, 1024, device=device)\n",
    "    y = torch.matmul(x, x)\n",
    "    print(\"GPU matmul ok:\", y.shape)\n",
    "else:\n",
    "    print(\"Running on CPU; 4-bit may be limited.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ccc914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with 4-bit quantization: gpt2\n",
      "Generation: Hello Titans-MIRAS!\n",
      "\n",
      "The Titans are back!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 4-bit quantization support (bitsandbytes)\n",
    "import warnings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "try:\n",
    "    model_id = \"gpt2\"  # small proxy for environment validation\n",
    "    quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "                                      bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    tok.pad_token = tok.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"Loaded model with 4-bit quantization:\", model_id)\n",
    "    import math\n",
    "    inputs = tok(\"Hello Titans-MIRAS!\", return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=8)\n",
    "    print(\"Generation:\", tok.decode(out[0]))\n",
    "except Exception as e:\n",
    "    print(\"4-bit test failed (this can be expected on CPU-only environments):\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832615f7",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Memory Architecture (MAC)\n",
    "A Neural Memory module with Surprise-driven `memorize()` update, inspired by Titans + MIRAS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b14cf5",
   "metadata": {},
   "source": [
    "## Theory: Memory as Context (MAC)\n",
    "Memory emits a small vector that is concatenated or prepended to the LLM input (soft prompt). The module learns to predict task-relevant features from recent hidden states. Surprise is measured by prediction error (MSE). High surprise triggers stronger learning; low surprise decays.\n",
    "\n",
    "Key pieces:\n",
    "- Input: recent hidden/context vector x\n",
    "- Output: soft prompt vector p\n",
    "- Surprise: L(x, y) = ||f(x) − y||²\n",
    "- Online update: θ ← θ − η ∇θ L\n",
    "- Recall: use f(x) as a soft prompt for the next LLM step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "131851eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralMemory device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralMemory(nn.Module):\n",
    "    \"\"\"Neural Memory module for Titans-MIRAS hybrid system.\n",
    "    \n",
    "    Uses float32 for stable training, automatically casts float16 inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, lr: float = 1e-3, device_str: str = None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.device = torch.device(device_str) if device_str else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Always use float32 for stable training\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "        self.to(self.device, torch.float32)\n",
    "        self.optim = torch.optim.AdamW(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def recall(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.to(self.device, torch.float32)\n",
    "        p = self.net(x)\n",
    "        return p.detach()\n",
    "\n",
    "    def memorize(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        # Cast float16 inputs from LLM to float32 for stable training\n",
    "        x = x.to(self.device, torch.float32)\n",
    "        y = y.to(self.device, torch.float32)\n",
    "        pred = self.net(x)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return float(loss.item())\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"NeuralMemory device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0c6ff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  40  loss 18.093212\n",
      "step  80  loss 11.844204\n",
      "step 120  loss 9.229830\n",
      "step 160  loss 7.861908\n",
      "step 200  loss 6.429991\n",
      "test mse: 5.756186\n"
     ]
    }
   ],
   "source": [
    "# Synthetic test: learn a simple linear mapping online\n",
    "import math, random\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "in_dim, hid_dim, out_dim = 128, 64, 128\n",
    "mem = NeuralMemory(in_dim, hid_dim, out_dim, lr=1e-2, device_str=device)\n",
    "\n",
    "# ground truth mapping (unknown to memory)\n",
    "W_true = torch.randn(in_dim, out_dim, device=device) * 0.5\n",
    "\n",
    "def sample_xy(batch=32):\n",
    "    x = torch.randn(batch, in_dim, device=device)\n",
    "    y = x @ W_true\n",
    "    return x, y\n",
    "\n",
    "steps = 200\n",
    "log_every = 40\n",
    "losses = []\n",
    "for t in range(1, steps + 1):\n",
    "    x, y = sample_xy(batch=64)\n",
    "    loss = mem.memorize(x, y)\n",
    "    losses.append(loss)\n",
    "    if t % log_every == 0:\n",
    "        print(f\"step {t:3d}  loss {loss:.6f}\")\n",
    "\n",
    "# verify recall quality on fresh batch\n",
    "x_test, y_test = sample_xy(batch=16)\n",
    "p = mem.recall(x_test)\n",
    "mse = F.mse_loss(p, y_test).item()\n",
    "print(\"test mse:\", round(mse, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec05f7",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Hybrid Engine Integration\n",
    "Wire up a frozen LLM (GPT-2) with the `NeuralMemory` module. The engine implements the read→surprise→learn→recall loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34c2ff5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a1b3bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gpt2, hidden_dim=768\n"
     ]
    }
   ],
   "source": [
    "# Load frozen LLM (GPT-2 for speed) and tokenizer\n",
    "model_id = \"gpt2\"  # or \"gpt2-medium\" for better quality\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.float16 if device==\"cuda\" else torch.float32)\n",
    "llm.to(device)\n",
    "llm.eval()  # frozen\n",
    "\n",
    "hidden_dim = llm.config.n_embd\n",
    "print(f\"Loaded {model_id}, hidden_dim={hidden_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e722645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid memory initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize memory for hybrid engine (maps hidden_dim -> hidden_dim soft prompt)\n",
    "hybrid_memory = NeuralMemory(input_dim=hidden_dim, hidden_dim=256, output_dim=hidden_dim, lr=5e-4, device_str=device)\n",
    "print(\"Hybrid memory initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e72e29",
   "metadata": {},
   "source": [
    "## The Hybrid Loop\n",
    "1. **Read**: Tokenize input text and run the LLM to get hidden states\n",
    "2. **Surprise**: Compute prediction error (MSE) between memory's prediction and the actual hidden state\n",
    "3. **Learn**: Update memory weights via backprop with the Surprise loss\n",
    "4. **Recall**: Memory generates a soft prompt vector to condition the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38467182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The Titans architecture enables long-term memory by...\n",
      "Surprise loss: 95.512871\n",
      "Soft prompt norm: 61.4240\n"
     ]
    }
   ],
   "source": [
    "def run_step_with_memory(text: str, use_memory: bool = True, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    1. Tokenize input\n",
    "    2. Get LLM hidden states (frozen)\n",
    "    3. If use_memory: memory.memorize(prev_hidden, current_hidden) to learn surprise\n",
    "    4. memory.recall(current_hidden) produces soft prompt for next step\n",
    "    5. Return generated text and surprise loss\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Extract last hidden state from final layer\n",
    "    hidden_states = outputs.hidden_states[-1]  # shape: (batch, seq_len, hidden_dim)\n",
    "    last_hidden = hidden_states[:, -1, :]  # shape: (batch, hidden_dim)\n",
    "    \n",
    "    surprise_loss = 0.0\n",
    "    soft_prompt = None\n",
    "    \n",
    "    if use_memory:\n",
    "        # For simplicity: predict last_hidden from itself (circular dependency demo)\n",
    "        # In a real system, you'd predict *next* hidden from current context\n",
    "        surprise_loss = hybrid_memory.memorize(last_hidden, last_hidden)\n",
    "        soft_prompt = hybrid_memory.recall(last_hidden)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Text: {text[:60]}...\")\n",
    "        print(f\"Surprise loss: {surprise_loss:.6f}\")\n",
    "        if soft_prompt is not None:\n",
    "            print(f\"Soft prompt norm: {soft_prompt.norm().item():.4f}\")\n",
    "    \n",
    "    return last_hidden, surprise_loss, soft_prompt\n",
    "\n",
    "# Test the step\n",
    "text = \"The Titans architecture enables long-term memory by\"\n",
    "h, loss, sp = run_step_with_memory(text, use_memory=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdd653e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-step Memory Adaptation ===\n",
      "Step 1: loss=78.840233  |  The quick brown fox jumps over the lazy dog.\n",
      "Step 2: loss=137.497589  |  Neural networks learn patterns from data.\n",
      "Step 3: loss=122.783745  |  Titans use a surprise metric to decide what to rem\n",
      "Step 4: loss=125.055893  |  Memory modules can adapt online during inference.\n",
      "Step 5: loss=72.701721  |  The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "First loss: 78.840233, Last loss: 72.701721\n"
     ]
    }
   ],
   "source": [
    "# Multi-step adaptation demo: feed varied sentences and watch surprise decrease\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Neural networks learn patterns from data.\",\n",
    "    \"Titans use a surprise metric to decide what to remember.\",\n",
    "    \"Memory modules can adapt online during inference.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",  # repeat\n",
    "]\n",
    "\n",
    "print(\"=== Multi-step Memory Adaptation ===\")\n",
    "losses = []\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    _, loss, _ = run_step_with_memory(sent, use_memory=True, verbose=False)\n",
    "    losses.append(loss)\n",
    "    print(f\"Step {i}: loss={loss:.6f}  |  {sent[:50]}\")\n",
    "\n",
    "print(f\"\\nFirst loss: {losses[0]:.6f}, Last loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e6ad0",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Demo Chat App\n",
    "Production-ready memory recall with semantic embeddings and confidence scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0a63dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Sentence embedder loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load sentence embedding model for semantic similarity\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "print(\"Sentence embedder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "486b2d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic memory initialized\n"
     ]
    }
   ],
   "source": [
    "# NeuralMemory with semantic embeddings and production-ready recall\n",
    "class SemanticMemory(nn.Module):\n",
    "    def __init__(self, embedder, device_str: str = None):\n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        self.device = torch.device(device_str) if device_str else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Episodic memory: stores (embedding, text) pairs\n",
    "        self.memory_embeddings = []  # Semantic embeddings\n",
    "        self.memory_texts = []       # Original text\n",
    "\n",
    "    def memorize(self, text: str):\n",
    "        \"\"\"Store a fact in memory.\"\"\"\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True, device=self.device)\n",
    "        embedding = nn.functional.normalize(embedding, dim=-1)\n",
    "        self.memory_embeddings.append(embedding)\n",
    "        self.memory_texts.append(text)\n",
    "        return len(self.memory_embeddings)\n",
    "\n",
    "    def recall(self, query: str, top_k: int = 3):\n",
    "        \"\"\"Find most similar memories to the query.\"\"\"\n",
    "        if not self.memory_embeddings:\n",
    "            return [(0.0, \"No memories stored\")]\n",
    "        \n",
    "        query_emb = self.embedder.encode(query, convert_to_tensor=True, device=self.device)\n",
    "        query_emb = nn.functional.normalize(query_emb, dim=-1)\n",
    "        \n",
    "        # Compute similarities to all stored memories\n",
    "        similarities = []\n",
    "        for i, mem_emb in enumerate(self.memory_embeddings):\n",
    "            sim = torch.dot(query_emb, mem_emb).item()\n",
    "            similarities.append((sim, self.memory_texts[i]))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def recall_with_confidence(self, query: str, gap_threshold: float = 0.1, min_similarity: float = 0.65):\n",
    "        \"\"\"\n",
    "        Production-ready recall using BOTH:\n",
    "        1. Relative gap detection (is there a clear winner?)\n",
    "        2. Minimum similarity threshold (is the match actually relevant?)\n",
    "        \n",
    "        Returns (confidence, best_match, all_results)\n",
    "        \n",
    "        Confidence levels:\n",
    "        - \"high\": Top match has high similarity AND is clearly better than 2nd\n",
    "        - \"low\": Top match exists but either too low similarity OR no clear gap\n",
    "        - \"none\": No memories stored\n",
    "        \"\"\"\n",
    "        results = self.recall(query, top_k=len(self.memory_texts) if self.memory_texts else 1)\n",
    "        \n",
    "        if not results or results[0][1] == \"No memories stored\":\n",
    "            return \"none\", None, results\n",
    "        \n",
    "        top_sim, top_text = results[0]\n",
    "        \n",
    "        if len(results) == 1:\n",
    "            confidence = \"high\" if top_sim > min_similarity else \"low\"\n",
    "            return confidence, top_text, results\n",
    "        \n",
    "        second_sim = results[1][0]\n",
    "        gap = top_sim - second_sim\n",
    "        \n",
    "        # HIGH confidence requires BOTH:\n",
    "        # 1. Clear winner (large gap from 2nd place)\n",
    "        # 2. Strong absolute match (above minimum similarity)\n",
    "        if gap > gap_threshold and top_sim > min_similarity:\n",
    "            return \"high\", top_text, results\n",
    "        else:\n",
    "            return \"low\", top_text, results\n",
    "\n",
    "semantic_memory = SemanticMemory(embedder, device_str=device)\n",
    "print(\"Semantic memory initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0246ab",
   "metadata": {},
   "source": [
    "## Phase 1: Memorize Facts\n",
    "Feed the memory distinct facts. The memory stores semantic embeddings for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98cc1f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Memorizing Facts ===\n",
      "Fact 1: stored  |  The secret code is X-8-DELTA-9.\n",
      "Fact 2: stored  |  Alice's favorite color is turquoise.\n",
      "Fact 3: stored  |  The meeting is scheduled for 3pm on Friday.\n"
     ]
    }
   ],
   "source": [
    "facts = [\n",
    "    \"The secret code is X-8-DELTA-9.\",\n",
    "    \"Alice's favorite color is turquoise.\",\n",
    "    \"The meeting is scheduled for 3pm on Friday.\",\n",
    "]\n",
    "\n",
    "print(\"=== Memorizing Facts ===\")\n",
    "for i, fact in enumerate(facts, 1):\n",
    "    count = semantic_memory.memorize(fact)\n",
    "    print(f\"Fact {i}: stored  |  {fact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86facc8f",
   "metadata": {},
   "source": [
    "## Phase 2: Query and Recall from Memory\n",
    "Query the system about the facts using production-ready confidence scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ac49cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Querying with Production-Ready Memory Recall ===\n",
      "(Uses relative gap detection + minimum similarity threshold)\n",
      "\n",
      "Q: What is the secret code?\n",
      "   ✓ HIGH CONFIDENCE\n",
      "   → [0.7169] The secret code is X-8-DELTA-9.\n",
      "     [0.0857] The meeting is scheduled for 3pm on Friday.\n",
      "     [0.0379] Alice's favorite color is turquoise.\n",
      "   Gap (1st - 2nd): 0.6312\n",
      "\n",
      "Q: What is Alice's favorite color?\n",
      "   ✓ HIGH CONFIDENCE\n",
      "   → [0.8441] Alice's favorite color is turquoise.\n",
      "     [0.1104] The secret code is X-8-DELTA-9.\n",
      "     [0.0404] The meeting is scheduled for 3pm on Friday.\n",
      "   Gap (1st - 2nd): 0.7337\n",
      "\n",
      "Q: What is the address of the meeting?\n",
      "   ⚠ LOW CONFIDENCE (no clear match)\n",
      "   → [0.6056] The meeting is scheduled for 3pm on Friday.\n",
      "     [0.1861] The secret code is X-8-DELTA-9.\n",
      "     [0.0095] Alice's favorite color is turquoise.\n",
      "   Gap (1st - 2nd): 0.4195\n",
      "\n",
      "Q: When is the meeting?\n",
      "   ✓ HIGH CONFIDENCE\n",
      "   → [0.7688] The meeting is scheduled for 3pm on Friday.\n",
      "     [0.1464] The secret code is X-8-DELTA-9.\n",
      "     [0.0202] Alice's favorite color is turquoise.\n",
      "   Gap (1st - 2nd): 0.6224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is the secret code?\",\n",
    "    \"What is Alice's favorite color?\",\n",
    "    \"What is the address of the meeting?\",  # No address was memorized!\n",
    "    \"When is the meeting?\",\n",
    "]\n",
    "\n",
    "print(\"=== Querying with Production-Ready Memory Recall ===\")\n",
    "print(\"(Uses relative gap detection + minimum similarity threshold)\\n\")\n",
    "\n",
    "for q in queries:\n",
    "    confidence, best_match, all_results = semantic_memory.recall_with_confidence(q, gap_threshold=0.1)\n",
    "    \n",
    "    # Show status based on confidence\n",
    "    if confidence == \"high\":\n",
    "        status = \"✓ HIGH CONFIDENCE\"\n",
    "    elif confidence == \"low\":\n",
    "        status = \"⚠ LOW CONFIDENCE (no clear match)\"\n",
    "    else:\n",
    "        status = \"✗ NO MEMORIES\"\n",
    "    \n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"   {status}\")\n",
    "    \n",
    "    # Show top results with similarities\n",
    "    for i, (sim, text) in enumerate(all_results[:3]):\n",
    "        marker = \"→\" if i == 0 else \" \"\n",
    "        print(f\"   {marker} [{sim:.4f}] {text}\")\n",
    "    \n",
    "    # Show gap analysis\n",
    "    if len(all_results) >= 2:\n",
    "        gap = all_results[0][0] - all_results[1][0]\n",
    "        print(f\"   Gap (1st - 2nd): {gap:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef2f35",
   "metadata": {},
   "source": [
    "---\n",
    "# Production Threshold Strategies\n",
    "\n",
    "This demo uses **Gap Detection + Minimum Similarity** — combining two strategies:\n",
    "\n",
    "| Strategy | How it Works | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Fixed Threshold** | `sim > 0.7` | Simple | Fragile, domain-specific |\n",
    "| **Relative Gap** | `top - 2nd > 0.1` | Adaptive | Needs 2+ memories |\n",
    "| **Gap + Min Sim** | Both conditions | Robust | Two parameters |\n",
    "| **Softmax Confidence** | `softmax(sims)[0] > 0.7` | Probabilistic | Temperature tuning |\n",
    "| **Reranker** | Cross-encoder rescores top-k | Most accurate | Slower, extra model |\n",
    "| **LLM Decides** | Pass top-k to LLM | Most flexible | Higher latency/cost |\n",
    "\n",
    "## Key Insight\n",
    "The \"address\" query has a **large gap** but **low absolute similarity** (0.61 < 0.65) because the matching fact talks about *time*, not *address*. Both conditions must be met for high confidence.\n",
    "\n",
    "## Next Steps\n",
    "- Experiment with larger models (Mistral-7B with 4-bit quantization)\n",
    "- Implement soft prompt injection into the LLM's embedding layer\n",
    "- Add a cross-encoder reranker for higher accuracy\n",
    "- Save/load memory checkpoints for persistent long-term memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
