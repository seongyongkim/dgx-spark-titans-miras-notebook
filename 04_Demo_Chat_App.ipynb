{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273936cb",
   "metadata": {},
   "source": [
    "# 04 — Demo Chat App\n",
    "This notebook demonstrates the full Titans-MIRAS hybrid memory system: we feed 3 distinct facts, clear the LLM context window, and query. The neural memory (trained purely on Surprise) should retrieve memorized facts without any context in the LLM's window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c2153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Load sentence embedding model for semantic similarity\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "print(\"Sentence embedder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeuralMemory with semantic embeddings and production-ready recall\n",
    "class NeuralMemory(nn.Module):\n",
    "    def __init__(self, embedder, device_str: str = None):\n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        self.device = torch.device(device_str) if device_str else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Episodic memory: stores (embedding, text) pairs\n",
    "        self.memory_embeddings = []  # Semantic embeddings\n",
    "        self.memory_texts = []       # Original text\n",
    "\n",
    "    def memorize(self, text: str):\n",
    "        \"\"\"Store a fact in memory.\"\"\"\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True, device=self.device)\n",
    "        embedding = nn.functional.normalize(embedding, dim=-1)\n",
    "        self.memory_embeddings.append(embedding)\n",
    "        self.memory_texts.append(text)\n",
    "        return len(self.memory_embeddings)\n",
    "\n",
    "    def recall(self, query: str, top_k: int = 3):\n",
    "        \"\"\"Find most similar memories to the query.\"\"\"\n",
    "        if not self.memory_embeddings:\n",
    "            return [(0.0, \"No memories stored\")]\n",
    "        \n",
    "        query_emb = self.embedder.encode(query, convert_to_tensor=True, device=self.device)\n",
    "        query_emb = nn.functional.normalize(query_emb, dim=-1)\n",
    "        \n",
    "        # Compute similarities to all stored memories\n",
    "        similarities = []\n",
    "        for i, mem_emb in enumerate(self.memory_embeddings):\n",
    "            sim = torch.dot(query_emb, mem_emb).item()\n",
    "            similarities.append((sim, self.memory_texts[i]))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[0], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def recall_with_confidence(self, query: str, gap_threshold: float = 0.1, min_similarity: float = 0.65):\n",
    "        \"\"\"\n",
    "        Production-ready recall using BOTH:\n",
    "        1. Relative gap detection (is there a clear winner?)\n",
    "        2. Minimum similarity threshold (is the match actually relevant?)\n",
    "        \n",
    "        Returns (confidence, best_match, all_results)\n",
    "        \n",
    "        Confidence levels:\n",
    "        - \"high\": Top match has high similarity AND is clearly better than 2nd\n",
    "        - \"low\": Top match exists but either too low similarity OR no clear gap\n",
    "        - \"none\": No memories stored\n",
    "        \"\"\"\n",
    "        results = self.recall(query, top_k=len(self.memory_texts) if self.memory_texts else 1)\n",
    "        \n",
    "        if not results or results[0][1] == \"No memories stored\":\n",
    "            return \"none\", None, results\n",
    "        \n",
    "        top_sim, top_text = results[0]\n",
    "        \n",
    "        if len(results) == 1:\n",
    "            confidence = \"high\" if top_sim > min_similarity else \"low\"\n",
    "            return confidence, top_text, results\n",
    "        \n",
    "        second_sim = results[1][0]\n",
    "        gap = top_sim - second_sim\n",
    "        \n",
    "        # HIGH confidence requires BOTH:\n",
    "        # 1. Clear winner (large gap from 2nd place)\n",
    "        # 2. Strong absolute match (above minimum similarity)\n",
    "        if gap > gap_threshold and top_sim > min_similarity:\n",
    "            return \"high\", top_text, results\n",
    "        else:\n",
    "            return \"low\", top_text, results\n",
    "\n",
    "memory = NeuralMemory(embedder, device_str=device)\n",
    "print(\"Memory initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5c663",
   "metadata": {},
   "source": [
    "## Phase 1: Memorize Facts\n",
    "Feed the memory 3 distinct facts and let it adapt online. The LLM context still remembers them at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cee2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = [\n",
    "    \"The secret code is X-8-DELTA-9.\",\n",
    "    \"Alice's favorite color is turquoise.\",\n",
    "    \"The meeting is scheduled for 3pm on Friday.\",\n",
    "]\n",
    "\n",
    "print(\"=== Memorizing Facts ===\")\n",
    "for i, fact in enumerate(facts, 1):\n",
    "    count = memory.memorize(fact)\n",
    "    print(f\"Fact {i}: stored  |  {fact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2123fd",
   "metadata": {},
   "source": [
    "## Phase 3: Query and Recall from Memory\n",
    "Query the system about the facts. The LLM doesn't have them in context, but the memory module can inject a soft prompt that influences generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26653772",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is the secret code?\",\n",
    "    \"What is Alice's favorite color?\",\n",
    "    \"What is the address of the meeting?\",  # No address was memorized!\n",
    "    \"When is the meeting?\",\n",
    "]\n",
    "\n",
    "print(\"=== Querying with Production-Ready Memory Recall ===\")\n",
    "print(\"(Uses relative gap detection: high confidence = clear winner among stored facts)\\n\")\n",
    "\n",
    "for q in queries:\n",
    "    confidence, best_match, all_results = memory.recall_with_confidence(q, gap_threshold=0.1)\n",
    "    \n",
    "    # Show status based on confidence\n",
    "    if confidence == \"high\":\n",
    "        status = \"✓ HIGH CONFIDENCE\"\n",
    "    elif confidence == \"low\":\n",
    "        status = \"⚠ LOW CONFIDENCE (no clear match)\"\n",
    "    else:\n",
    "        status = \"✗ NO MEMORIES\"\n",
    "    \n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"   {status}\")\n",
    "    \n",
    "    # Show top results with similarities\n",
    "    for i, (sim, text) in enumerate(all_results[:3]):\n",
    "        marker = \"→\" if i == 0 else \" \"\n",
    "        print(f\"   {marker} [{sim:.4f}] {text}\")\n",
    "    \n",
    "    # Show gap analysis\n",
    "    if len(all_results) >= 2:\n",
    "        gap = all_results[0][0] - all_results[1][0]\n",
    "        print(f\"   Gap (1st - 2nd): {gap:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3b111",
   "metadata": {},
   "source": [
    "# Production Threshold Strategies\n",
    "\n",
    "This demo uses **Relative Gap Detection** — instead of a fixed threshold, we check if the top match is significantly better than the 2nd best:\n",
    "\n",
    "| Strategy | How it Works | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Fixed Threshold** | `sim > 0.7` | Simple | Fragile, domain-specific |\n",
    "| **Relative Gap** | `top - 2nd > 0.1` | Adaptive, no tuning | Needs 2+ memories |\n",
    "| **Softmax Confidence** | `softmax(sims)[0] > 0.7` | Probabilistic | Temperature tuning |\n",
    "| **Reranker** | Cross-encoder rescores top-k | Most accurate | Slower, extra model |\n",
    "| **LLM Decides** | Pass top-k to LLM | Most flexible | Higher latency/cost |\n",
    "\n",
    "## Key Insight\n",
    "The \"address\" query has **low gap** (small difference between top match and others) because no address was stored — all facts are equally irrelevant. Questions with memorized answers show **high gap** (clear winner).\n",
    "\n",
    "## Next Steps\n",
    "- Experiment with larger models (Mistral-7B with 4-bit quantization)\n",
    "- Implement soft prompt injection into the LLM's embedding layer\n",
    "- Add a cross-encoder reranker for higher accuracy\n",
    "- Save/load memory checkpoints for persistent long-term memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
