{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050cadf1",
   "metadata": {},
   "source": [
    "# 03 — Hybrid Engine Integration\n",
    "This notebook wires up a frozen LLM (GPT-2) with the `NeuralMemory` module. The engine implements the read→surprise→learn→recall loop, where memory weights adapt online during inference based on the Surprise signal from the LLM's hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load frozen LLM (GPT-2 for speed) and tokenizer\n",
    "model_id = \"gpt2\"  # or \"gpt2-medium\" for better quality\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32)\n",
    "llm.to(device)\n",
    "llm.eval()  # frozen\n",
    "\n",
    "hidden_dim = llm.config.n_embd\n",
    "print(f\"Loaded {model_id}, hidden_dim={hidden_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f1dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import NeuralMemory from Notebook 2\n",
    "class NeuralMemory(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, lr: float = 1e-3, device_str: str = None):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.device = torch.device(device_str) if device_str else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Always use float32 for stable training (inputs will be cast automatically)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "        self.to(self.device, torch.float32)\n",
    "        self.optim = torch.optim.AdamW(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def recall(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.to(self.device, torch.float32)\n",
    "        p = self.net(x)\n",
    "        return p.detach()\n",
    "\n",
    "    def memorize(self, x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        # Cast float16 inputs from LLM to float32 for stable training\n",
    "        x = x.to(self.device, torch.float32)\n",
    "        y = y.to(self.device, torch.float32)\n",
    "        pred = self.net(x)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        return float(loss.item())\n",
    "\n",
    "# Initialize memory (maps hidden_dim -> hidden_dim soft prompt)\n",
    "memory = NeuralMemory(input_dim=hidden_dim, hidden_dim=256, output_dim=hidden_dim, lr=5e-4, device_str=device)\n",
    "print(\"Memory initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eae0be2",
   "metadata": {},
   "source": [
    "## The Hybrid Loop\n",
    "1. **Read**: Tokenize input text and run the LLM to get hidden states\n",
    "2. **Surprise**: Compute prediction error (MSE) between memory's prediction and the actual hidden state\n",
    "3. **Learn**: Update memory weights via backprop with the Surprise loss\n",
    "4. **Recall**: Memory generates a soft prompt vector to condition the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d73f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_step_with_memory(text: str, use_memory: bool = True, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    1. Tokenize input\n",
    "    2. Get LLM hidden states (frozen)\n",
    "    3. If use_memory: memory.memorize(prev_hidden, current_hidden) to learn surprise\n",
    "    4. memory.recall(current_hidden) produces soft prompt for next step\n",
    "    5. Return generated text and surprise loss\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llm(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Extract last hidden state from final layer\n",
    "    hidden_states = outputs.hidden_states[-1]  # shape: (batch, seq_len, hidden_dim)\n",
    "    last_hidden = hidden_states[:, -1, :]  # shape: (batch, hidden_dim)\n",
    "    \n",
    "    surprise_loss = 0.0\n",
    "    soft_prompt = None\n",
    "    \n",
    "    if use_memory:\n",
    "        # For simplicity: predict last_hidden from itself (circular dependency demo)\n",
    "        # In a real system, you'd predict *next* hidden from current context\n",
    "        surprise_loss = memory.memorize(last_hidden, last_hidden)\n",
    "        soft_prompt = memory.recall(last_hidden)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Text: {text[:60]}...\")\n",
    "        print(f\"Surprise loss: {surprise_loss:.6f}\")\n",
    "        if soft_prompt is not None:\n",
    "            print(f\"Soft prompt norm: {soft_prompt.norm().item():.4f}\")\n",
    "    \n",
    "    return last_hidden, surprise_loss, soft_prompt\n",
    "\n",
    "# Test the step\n",
    "text = \"The Titans architecture enables long-term memory by\"\n",
    "h, loss, sp = run_step_with_memory(text, use_memory=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2138860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-step adaptation demo: feed varied sentences and watch surprise decrease\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Neural networks learn patterns from data.\",\n",
    "    \"Titans use a surprise metric to decide what to remember.\",\n",
    "    \"Memory modules can adapt online during inference.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",  # repeat\n",
    "]\n",
    "\n",
    "print(\"=== Multi-step Memory Adaptation ===\")\n",
    "losses = []\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    _, loss, _ = run_step_with_memory(sent, use_memory=True, verbose=False)\n",
    "    losses.append(loss)\n",
    "    print(f\"Step {i}: loss={loss:.6f}  |  {sent[:50]}\")\n",
    "\n",
    "print(f\"\\nFirst loss: {losses[0]:.6f}, Last loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204dbbae",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "In Notebook 4, we'll build an interactive chat demo that memorizes distinct facts, clears the LLM context window, and retrieves memorized information purely from the neural memory weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
